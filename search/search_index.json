{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TerraKit's documentation!","text":"<p>TerraKit is a Python package for finding, retrieving and processing geospatial information from a range of data connectors. TerraKit makes creating ML-ready EO datasets easy.</p> <p></p>"},{"location":"#getting-started","title":"Getting started","text":"<p>To install TerraKit, use pip</p> <p><pre><code>pip install terrakit\n</code></pre> or uv: <pre><code>uv pip install terrakit\n</code></pre></p> <p>Check TerraKit is working as expected by running:</p> <pre><code>python -c \"import terrakit; data_source='nasa_earthdata'; dc = terrakit.DataConnector(connector_type=data_source)\"\n</code></pre> <p>Take a look at the example notebooks for more help getting started with TerraKit. </p>"},{"location":"#terrakit-cli","title":"TerraKit CLI","text":"<p>We can also run TerraKit using the CLI. Take a look at the TerraKit CLI Notebook for some examples of how to use this.</p>"},{"location":"#terrakit-pipelines","title":"TerraKit Pipelines","text":"<p>TerraKit provides tools for For more information, checkout out the Pipelines section.</p>"},{"location":"adding_connectors/","title":"Add a New Data Connector","text":"<p>To add a new data connector, use the connector_template.py as a starting point. The new connector should implement the <code>list_collection</code>, <code>find_data</code> and <code>get_data</code> functions and extend the <code>Connector</code> class from the <code>terrakit.download.connector</code> module. Finally update terrakit.py to enable the new connector to be selected.</p> <p>To also include new tests for the new connector, please make use of test_connector_template.py.</p> <p>Make sure to also update the documentation. Each data connector has a separate markdown file making it easy to add new docs.</p>"},{"location":"adding_connectors/#data-connector-template-class-documentation","title":"Data Connector Template class Documentation","text":""},{"location":"adding_connectors/#terrakit.download.data_connectors.connector_template.ConnectorTemplate","title":"<code>ConnectorTemplate</code>","text":"<p>               Bases: <code>Connector</code></p> <p>Attributes:</p> Name Type Description <code>connector_type</code> <code>str</code> <p>Name of connector</p> <code>collections</code> <code>list</code> <p>A list of available collections.</p> <code>collections_details</code> <code>list</code> <p>Detailed information about the collections.</p> Source code in <code>terrakit/download/data_connectors/connector_template.py</code> <pre><code>class ConnectorTemplate(Connector):\n    \"\"\"\n    Attributes:\n        connector_type (str): Name of connector\n        collections (list): A list of available collections.\n        collections_details (list): Detailed information about the collections.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize SentinelHub with collections and configuration.\n        \"\"\"\n        self.connector_type: str = \"&lt;new_connector&gt;\"\n        self.collections: list[Any] = load_and_list_collections(\n            connector_type=self.connector_type\n        )\n        self.collections_details: list[Any] = load_and_list_collections(\n            as_json=True, connector_type=self.connector_type\n        )\n\n    def list_collections(self) -&gt; list[Any]:\n        \"\"\"\n        Lists the available collections.\n\n        Returns:\n            list: A list of collection names.\n        \"\"\"\n        logger.info(\"Listing available collections\")\n        return self.collections\n\n    def find_data(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n        Args:\n            data_collection_name (str): The name of the Sentinel Hub data collection to search.\n            date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n            date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n            area_polygon (Polygon, optional): A polygon defining the area of interest.\n            bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n            bands (list, optional): A list of bands to retrieve. Defaults to [].\n            maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n            data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n        Returns:\n            tuple: A tuple containing a sorted list of unique dates and a list of data results.\n        \"\"\"\n        unique_dates: list[str] = []\n        results: list[dict[str, Any]] = [{}]\n        return unique_dates, results\n\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ):\n        \"\"\"\n        Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n        Args:\n            data_collection_name (str): Name of the data collection to fetch data from.\n            date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n            bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n            bands (list, optional): List of bands to retrieve. Defaults to all bands.\n            maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n            data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n            save_file (str, optional): Path to save the output file. Defaults to None.\n            working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n        Returns:\n            xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n        \"\"\"\n        da = xr.DataArray()\n        return da\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.data_connectors.connector_template.ConnectorTemplate.list_collections","title":"<code>list_collections</code>","text":"<p>Lists the available collections.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Any]</code> <p>A list of collection names.</p> Source code in <code>terrakit/download/data_connectors/connector_template.py</code> <pre><code>def list_collections(self) -&gt; list[Any]:\n    \"\"\"\n    Lists the available collections.\n\n    Returns:\n        list: A list of collection names.\n    \"\"\"\n    logger.info(\"Listing available collections\")\n    return self.collections\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.data_connectors.connector_template.ConnectorTemplate.find_data","title":"<code>find_data</code>","text":"<p>This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the Sentinel Hub data collection to search.</p> required <code>date_start</code> <code>str</code> <p>The start date for the time interval in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>The end date for the time interval in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>Polygon</code> <p>A polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>tuple</code> <p>A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).</p> <code>None</code> <code>bands</code> <code>list</code> <p>A list of bands to retrieve. Defaults to [].</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).</p> <code>100</code> <code>data_connector_spec</code> <code>list</code> <p>A dictionary containing the data connector specification.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>A tuple containing a sorted list of unique dates and a list of data results.</p> Source code in <code>terrakit/download/data_connectors/connector_template.py</code> <pre><code>def find_data(\n    self,\n    data_collection_name: str,\n    date_start: str,\n    date_end: str,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n    Args:\n        data_collection_name (str): The name of the Sentinel Hub data collection to search.\n        date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n        date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n        area_polygon (Polygon, optional): A polygon defining the area of interest.\n        bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n        bands (list, optional): A list of bands to retrieve. Defaults to [].\n        maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n        data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n    Returns:\n        tuple: A tuple containing a sorted list of unique dates and a list of data results.\n    \"\"\"\n    unique_dates: list[str] = []\n    results: list[dict[str, Any]] = [{}]\n    return unique_dates, results\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.data_connectors.connector_template.ConnectorTemplate.get_data","title":"<code>get_data</code>","text":"<p>Fetches data from SentinelHub for the specified collection, date range, area, and bands.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>Name of the data collection to fetch data from.</p> required <code>date_start</code> <code>str</code> <p>Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to retrieve. Defaults to all bands.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover threshold (0-100). Defaults to 100.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Data connector specification. Defaults to None.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the output file. Defaults to None.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files. Defaults to '.'.</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>xarray</code> <p>An xarray Datasets containing the fetched data with dimensions (time, band, y, x).</p> Source code in <code>terrakit/download/data_connectors/connector_template.py</code> <pre><code>def get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n):\n    \"\"\"\n    Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n    Args:\n        data_collection_name (str): Name of the data collection to fetch data from.\n        date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n        bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n        bands (list, optional): List of bands to retrieve. Defaults to all bands.\n        maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n        data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n        save_file (str, optional): Path to save the output file. Defaults to None.\n        working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n    Returns:\n        xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n    \"\"\"\n    da = xr.DataArray()\n    return da\n</code></pre>"},{"location":"adding_connectors/#data-connector-abstract-class-documentation","title":"Data Connector Abstract class Documentation","text":""},{"location":"adding_connectors/#terrakit.download.connector","title":"<code>terrakit.download.connector</code>","text":""},{"location":"adding_connectors/#terrakit.download.connector.Connector","title":"<code>Connector</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for all connectors. This class insists that any subclass must have a list_collections(), find_data() and get_data() method.</p> <p>Methods:</p> Name Description <code>list_collections</code> <p>Returns a list of available data collections.</p> <code>find_data</code> <p>Finds data within specified parameters and returns a list of unique dates and relevant metadata.</p> <code>get_data</code> <p>Retrieves data based on given parameters and saves to file.</p> Source code in <code>terrakit/download/connector.py</code> <pre><code>class Connector(ABC):\n    \"\"\"\n    An abstract base class for all connectors.\n    This class insists that any subclass must have a list_collections(), find_data()\n    and get_data() method.\n\n    Attributes:\n        None\n\n    Methods:\n        list_collections: Returns a list of available data collections.\n        find_data: Finds data within specified parameters and returns a list of unique dates and relevant metadata.\n        get_data: Retrieves data based on given parameters and saves to file.\n    \"\"\"\n\n    @abstractmethod\n    def list_collections(self) -&gt; list[Any]:\n        \"\"\"\n        Returns a list of available data collections.\n\n        Returns:\n            list[Any]: List of available data collection names.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def find_data(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        Finds data within specified parameters and returns relevant metadata.\n\n        Args:\n            data_collection_name (str): The name of the data collection to search.\n            date_start (str): The start date for the data retrieval.\n            date_end (str): The end date for the data retrieval.\n            area_polygon (Optional[Any]): Polygon defining the area of interest. Either specify area_polygon or bbox.\n            bbox (Optional[Any]): Bounding box defining the area of interest. Either specify area_polygon or bbox.\n            bands (list[str]): List of bands to retrieve.\n            maxcc (int): Maximum cloud cover percentage.\n            data_connector_spec (Optional[Any]): Additional specifications for the data connector.\n\n        Returns:\n            Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]: A tuple containing a list of data identifiers and a list of metadata dictionaries, or (None, None) if no data is found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ) -&gt; Union[xr.DataArray, None]:\n        \"\"\"\n        Retrieves data based on given parameters and optional saving to file.\n\n        Args:\n            data_collection_name (str): The name of the data collection to retrieve.\n            date_start (str): The start date for data retrieval.\n            date_end (str): The end date for data retrieval.\n            area_polygon (Optional[Any]): Polygon defining the area of interest.\n            bbox (Optional[Any]): Bounding box defining the area of interest.\n            bands (list[str]): List of bands to retrieve.\n            maxcc (int): Maximum cloud cover percentage.\n            data_connector_spec (Optional[Any]): Additional specifications for the data connector.\n            save_file (Optional[str]): Path to save the retrieved data file.\n            working_dir (str): Working directory for saving the file.\n\n        Returns:\n            Union[xr.DataArray, None]: The retrieved xarray DataArray or None if no data is found.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.connector.Connector.list_collections","title":"<code>list_collections</code>  <code>abstractmethod</code>","text":"<p>Returns a list of available data collections.</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>list[Any]: List of available data collection names.</p> Source code in <code>terrakit/download/connector.py</code> <pre><code>@abstractmethod\ndef list_collections(self) -&gt; list[Any]:\n    \"\"\"\n    Returns a list of available data collections.\n\n    Returns:\n        list[Any]: List of available data collection names.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.connector.Connector.find_data","title":"<code>find_data</code>  <code>abstractmethod</code>","text":"<p>Finds data within specified parameters and returns relevant metadata.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the data collection to search.</p> required <code>date_start</code> <code>str</code> <p>The start date for the data retrieval.</p> required <code>date_end</code> <code>str</code> <p>The end date for the data retrieval.</p> required <code>area_polygon</code> <code>Optional[Any]</code> <p>Polygon defining the area of interest. Either specify area_polygon or bbox.</p> <code>None</code> <code>bbox</code> <code>Optional[Any]</code> <p>Bounding box defining the area of interest. Either specify area_polygon or bbox.</p> <code>None</code> <code>bands</code> <code>list[str]</code> <p>List of bands to retrieve.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage.</p> <code>100</code> <code>data_connector_spec</code> <code>Optional[Any]</code> <p>Additional specifications for the data connector.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]: A tuple containing a list of data identifiers and a list of metadata dictionaries, or (None, None) if no data is found.</p> Source code in <code>terrakit/download/connector.py</code> <pre><code>@abstractmethod\ndef find_data(\n    self,\n    data_collection_name: str,\n    date_start: str,\n    date_end: str,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    Finds data within specified parameters and returns relevant metadata.\n\n    Args:\n        data_collection_name (str): The name of the data collection to search.\n        date_start (str): The start date for the data retrieval.\n        date_end (str): The end date for the data retrieval.\n        area_polygon (Optional[Any]): Polygon defining the area of interest. Either specify area_polygon or bbox.\n        bbox (Optional[Any]): Bounding box defining the area of interest. Either specify area_polygon or bbox.\n        bands (list[str]): List of bands to retrieve.\n        maxcc (int): Maximum cloud cover percentage.\n        data_connector_spec (Optional[Any]): Additional specifications for the data connector.\n\n    Returns:\n        Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]: A tuple containing a list of data identifiers and a list of metadata dictionaries, or (None, None) if no data is found.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"adding_connectors/#terrakit.download.connector.Connector.get_data","title":"<code>get_data</code>  <code>abstractmethod</code>","text":"<p>Retrieves data based on given parameters and optional saving to file.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the data collection to retrieve.</p> required <code>date_start</code> <code>str</code> <p>The start date for data retrieval.</p> required <code>date_end</code> <code>str</code> <p>The end date for data retrieval.</p> required <code>area_polygon</code> <code>Optional[Any]</code> <p>Polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>Optional[Any]</code> <p>Bounding box defining the area of interest.</p> <code>None</code> <code>bands</code> <code>list[str]</code> <p>List of bands to retrieve.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage.</p> <code>100</code> <code>data_connector_spec</code> <code>Optional[Any]</code> <p>Additional specifications for the data connector.</p> <code>None</code> <code>save_file</code> <code>Optional[str]</code> <p>Path to save the retrieved data file.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for saving the file.</p> <code>'.'</code> <p>Returns:</p> Type Description <code>Union[DataArray, None]</code> <p>Union[xr.DataArray, None]: The retrieved xarray DataArray or None if no data is found.</p> Source code in <code>terrakit/download/connector.py</code> <pre><code>@abstractmethod\ndef get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n) -&gt; Union[xr.DataArray, None]:\n    \"\"\"\n    Retrieves data based on given parameters and optional saving to file.\n\n    Args:\n        data_collection_name (str): The name of the data collection to retrieve.\n        date_start (str): The start date for data retrieval.\n        date_end (str): The end date for data retrieval.\n        area_polygon (Optional[Any]): Polygon defining the area of interest.\n        bbox (Optional[Any]): Bounding box defining the area of interest.\n        bands (list[str]): List of bands to retrieve.\n        maxcc (int): Maximum cloud cover percentage.\n        data_connector_spec (Optional[Any]): Additional specifications for the data connector.\n        save_file (Optional[str]): Path to save the retrieved data file.\n        working_dir (str): Working directory for saving the file.\n\n    Returns:\n        Union[xr.DataArray, None]: The retrieved xarray DataArray or None if no data is found.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"adding_download_transformations/","title":"Add a New Download Transformation","text":"<p>To add a new data connector, use the download_transformation_template.py as a starting point. The new transformation should take an <code>xarray.DataArray</code> as an input and also return an <code>xarray.DataArray</code> as output. Update any data connectors that may need to use the new transformation by importing the function at the top of the file. For instance:</p> <pre><code>from terrakit.download.transformations.impute_nans_xarray import impute_nans_xarray\n</code></pre> <p>Please also include tests for your new transformation function.</p> <p>Make sure to also update the documentation. Each transformation function has a separate entry in ./adding_download_transformations.md making it easy to add new docs.</p>"},{"location":"adding_download_transformations/#transformation-template-function-documentation","title":"Transformation template function Documentation","text":""},{"location":"adding_download_transformations/#terrakit.download.transformations.download_transformation_template.template_transformation_fnc","title":"<code>template_transformation_fnc</code>","text":"<p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: The transformed DataArray.</p> Source code in <code>terrakit/download/transformations/download_transformation_template.py</code> <pre><code>def template_transformation_fnc(da: DataArray) -&gt; DataArray:\n    \"\"\"\n    &lt;UPDATE HERE WITH NEW TRANSFORMATION DOCSTRING&gt;\n\n    Parameters:\n        da (xarray.DataArray): The input DataArray.\n\n    Returns:\n        xarray.DataArray: The transformed DataArray.\n    \"\"\"\n    #########\u00a0UPDATE HERE WITH NEW TRANSFORMATION CODE #########\n    return da\n</code></pre>"},{"location":"adding_download_transformations/#download-data-pipeline","title":"Download Data Pipeline","text":"<p>To include a new transformation in the TerraKit Download Data Pipeline, a few code changes are needed. These are clearly sign posted, making in simple to extend the TerraKit library.</p> <p>To enable the new transformation to be selected, update line 71 of <code>terrakit/validate/download_model.py</code>. Please set the default option to <code>False</code>.</p> <pre><code># terrakit/validate/download_model.py\n\"\"\" &gt;&gt;&gt; INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \n&lt;new_transformation_option&gt;: bool = False\n\"\"\"\n</code></pre> <p>Next update <code>terrakit/download/download_data.py</code> to import the model into the download data pipeline code.</p> <pre><code># terrakit/download/download_data.py\n\"\"\" &gt;&gt;&gt; IMPORT NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \nfrom .transformations.&lt;new_transformation&gt; import &lt;new_transformation&gt;\n\"\"\"\n</code></pre> <p>Finally update <code>terrakit/download/download_data.py</code> to implement the transformation after data has been downloaded. Consider limiting the useage to a given data connector, unless if can be safely applied to all data connector. <pre><code># terrakit/download/download_data.py line 286\n\"\"\" &gt;&gt;&gt; INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \nif self.transform.&lt;new_transformation_func&gt;:\n    dai = &lt;new_tranformation_fnc(da)&gt;\n\"\"\"\n</code></pre></p>"},{"location":"chip/","title":"Tile and Chip","text":"<p>More info coming soon</p>"},{"location":"download_data/","title":"Download data","text":"<p>Once labels have been processed, next up in the TerraKit pipeline is downloading the data.</p> <p>Use the <code>download_data</code> function (or the <code>download</code> CLI subcommand) to download data from a set of data connectors for a time and location specified by the shapefiles output from the <code>process_labels</code> pipeline step.  </p> <p>Here's an example of how to use the <code>download_data</code> step in the TerraKit pipeline:</p> <pre><code>config = {\n    \"download\": {\n        \"data_sources\": [\n            {\n                \"data_connector\": \"sentinel_aws\",\n                \"collection_name\": \"sentinel-2-l2a\",\n                \"bands\": [\"blue\", \"green\", \"red\"],\n                \"save_file\": \"\",\n            },\n        ],\n        \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},\n        \"transform\": {\n            \"scale_data_xarray\": True,\n            \"impute_nans\": True,\n            \"reproject\": True,\n        },\n        \"max_cloud_cover\": 80,\n    },\n}\n\nqueried_data = download_data(\n    data_sources=config[\"download\"][\"data_sources\"],\n    date_allowance=config[\"download\"][\"date_allowance\"],\n    transform=config[\"download\"][\"transform\"],\n    max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],\n    dataset_name=DATASET_NAME,\n    working_dir=WORKING_DIR,\n    keep_files=False,\n)\n</code></pre> <p>Write the same arguments in a config file that the TerraKit CLI can use:</p> <p><pre><code># ./docs/examples/config.yaml\ndownload:\n  data_sources:\n  - data_connector: \"sentinel_aws\"\n    collection_name: \"sentinel-2-l2a\"\n    bands: [\"blue\", \"green\", \"red\"]\n  date_allowance: \n    pre_days: 0\n    post_days: 21\n  transform:\n    scale_data_xarray: True\n    impute_nans: true\n    reproject: True\n</code></pre> <pre><code>#!/bin/bash\nterrakit --config ./docs/examples/config.yaml download\n</code></pre></p> <p>Alternatively, use the TerraKit data_connectors directly by specify the collection, bbox, date and bands of interest.</p> <pre><code>from terrakit import DataConnector\n\ndc = DataConnector(connector_type=\"sentinel_aws\")\ndc.connector.list_collections()\n</code></pre>"},{"location":"download_data/#configure-the-download-pipeline","title":"Configure the Download pipeline","text":"<p>Use the following parameters to configure the TerraKit Download pipeline.</p>"},{"location":"download_data/#active","title":"Active","text":"<p><code>active</code>: Enables the labels pipeline to run. Set to <code>False</code> to skip the step. Default: <code>True</code></p>"},{"location":"download_data/#data-allowance-data_allowance","title":"Data Allowance: <code>data_allowance</code>","text":"<p>Date range allowance for data query.</p>"},{"location":"download_data/#transform-transform","title":"Transform: <code>transform</code>","text":"<p>Transformation parameters for data.</p>"},{"location":"download_data/#data-sources-data_sources","title":"Data Sources: <code>data_sources</code>","text":"<p>List of data sources to query. The list should contain a valid <code>DataSource</code> object which specifies the <code>data_connector</code>, <code>collection_name</code> and <code>bands</code> to download. Optionally specify a unique filename to used for the save the downloaded files as using <code>save_file</code>. If not specified, the data will be downloaded as saved as <code>{working_dir}/{data_connector}_{collection_name}.tif</code>.</p> <pre><code># Example of a valid DataSource dictionary.\ndownload_data(\n    data_sources = [{\n        \"data_connector\": \"sentinel_aws\",\n        \"collection_name\": \"sentinel-2-l2a\",\n        \"bands\": [\"blue\", \"green\", \"red\"],\n    }]\n)\n</code></pre> <p>Specify multiple data sources as follows:</p> <p><pre><code># Example of a valid multiple DataSource dictionaries passed as a list to the `data_sources` argument.\ndownload_data(\n    data_sources = [{\n        \"data_connector\": \"sentinel_aws\",\n        \"collection_name\": \"sentinel-2-l2a\",\n        \"bands\": [\"blue\", \"green\", \"red\"],\n    },\n    {\n        \"data_connector\": \"sentinelhub\",\n        \"collection_name\": \"s1_grd\",\n        \"bands\": [\"B04\", \"B03\", \"B02\"]\n    }]\n)\n</code></pre> To specify multiple data sources with the CLI with the following config:</p> <pre><code># ./docs/examples/config.yaml\ndownload:\n  data_sources:\n  - data_connector: \"sentinel_aws\"\n    collection_name: \"sentinel-2-l2a\"\n    bands: [\"blue\", \"green\", \"red\"]\n  - data_connector: \"sentinelhub\"\n    collection_name: \"s1_grd\"\n    bands: [\"B04\", \"B03\", \"B02\"]\n  date_allowance: \n    pre_days: 0\n    post_days: 21\n  transform:\n    scale_data_xarray: True\n    impute_nans: true\n    reproject: True\n</code></pre>"},{"location":"download_data/#max-cloud-cover-max_cloud_cover","title":"Max Cloud Cover: <code>max_cloud_cover</code>","text":"<p>Maximum cloud cover percentage for data selection.</p>"},{"location":"download_data/#datetime-bounding-box-shape-file-datetime_bbox_shp_file","title":"Datetime Bounding Box Shape File: <code>datetime_bbox_shp_file</code>","text":"<p>Path to a shapefile containing datetime and bounding box information. This shapefile will have been saved as <code>{working_dir}/{dataset_name}_all_bboxes.shp</code> if the <code>process_labels</code> set has already been run. If <code>datetime_bbox_shp_file</code> is not explicitly specified, TerraKit will first check for the default value (<code>./tmp/terrakit_curated_dataset_all_bboxes.shp</code>), followed by checking the working directory for <code>{dataset_name}_all_bboxes.shp</code>. </p> <p>The shapefile <code>{dataset_name}_all_bboxes.shp</code> must contain a <code>datetime</code> field and <code>geometry</code> field.</p>"},{"location":"download_data/#labels-shape-file-labels_shp_file","title":"Labels Shape File: <code>labels_shp_file</code>","text":"<p>Path to a shapefile containing datetime and label geometery information. This shapefile will have been saved as <code>{working_dir}/{dataset_name}_labels.shp</code> if the <code>process_labels</code> set has already been run. If <code>datetime_bbox_shp_file</code> is not explicitly specified, TerraKit will first check for the default value (<code>./tmp/terrakit_curated_dataset_labels.shp</code>), followed by checking the working directory for <code>{dataset_name}_labels.shp</code>. </p> <p>The shapefile <code>{dataset_name}_labels.shp</code> must contain a <code>datetime</code> field and <code>geometry</code> field.</p>"},{"location":"download_data/#keep-files-keep_files","title":"Keep files: <code>keep_files</code>","text":"<p>Flag to preserve shapefiles in the working directory once they have been used by the download data step. Downloaded files will not be removed. Set to <code>True</code> to ensure shapefiles remain in place.</p>"},{"location":"download_data/#data-connectors","title":"Data Connectors","text":"<p>Data connectors are classes which enable a user to search for data and query data from a particular data source using a common set of functions.  Each data connector has the following mandatory methods:</p> <ul> <li>list_collections()</li> <li>find_data()</li> <li>get_data()</li> </ul>"},{"location":"download_data/#available-data-connectors","title":"Available data connectors","text":"<p>The following data connectors and associated collections are available:</p> Connectors Collections sentinelhub s2_l1c, dem, s1_grd, hls_l30, s2_l2a, hls_s30 nasa_earthdata HLSL30_2.0, HLSS30_2.0 sentinel_aws sentinel-2-l2a IBMResearchSTAC 'HLSS30', 'esa-sentinel-2A-msil1c', 'HLS_S30',, 'atmospheric-weather-era5', 'deforestation-umd', 'Radar-10min', 'tasmax-rcp85-land-cpm-uk-2.2km', 'vector-osm-power', 'ukcp18-land-cpm-uk-2.2km', 'treecovermaps-eudr', 'ch4' + more TheWeatherCompany weathercompany-daily-forecast"},{"location":"download_data/#data-connector-access","title":"Data connector access","text":"<p>Each data connector has a different access requirements. For example, connecting to SentinelHub and NASA EarthData, you will need to obtain credentials from each provider. Once these have been obtained, they can be added to a <code>.env</code> file at the root directory level using the following syntax:</p> <pre><code>SH_CLIENT_ID=\"&lt;SentinelHub Client ID&gt;\"\nSH_CLIENT_SECRET=\"&lt;SentinelHub Client Secret&gt;\"\nNASA_EARTH_BEARER_TOKEN=\"&lt;NASA EarthData Bearer Token&gt;\"\n</code></pre>"},{"location":"download_data/#nasa-earthdata","title":"NASA Earthdata","text":"<p>To access NASA Earthdata, register for an Earthdata Login profile and requests a bearer token. https://urs.earthdata.nasa.gov/profile</p>"},{"location":"download_data/#sentinel-hub","title":"Sentinel Hub","text":"<p>To access sentinel hub, register for an account and requests an OAuth client using the Sentinel Hub dashboard https://www.planet.com</p>"},{"location":"download_data/#sentinel-aws","title":"Sentinel AWS","text":"<p>Access sentinel AWS data is open and does not require any credentials.</p>"},{"location":"download_data/#the-weather-company","title":"The Weather Company","text":"<p>To access The Weather Company, register for an account and requests an API Key https://www.weathercompany.com/weather-data-apis/. Once you have an API key, set the following environment variable:</p> <pre><code>THE_WEATHER_COMPANY_API_KEY=\"&lt;The Weather Company API key&gt;\"\n</code></pre>"},{"location":"download_data/#ibm-research-stac","title":"IBM Research STAC","text":"<p>Access IBM Research STAC is currently restricted to IBMers and partners. If you're elegible, you need to register for an IBM AppID account and set the following environment variables:</p> <pre><code>APPID_ISSUER=&lt;issuer&gt;\nAPPID_USERNAME=&lt;user-email&gt;\nAPPID_PASSWORD=&lt;user-password&gt;\nCLIENT_ID=&lt;client-id&gt;\nCLIENT_SECRET=&lt;client-secret&gt;\n</code></pre> <p>Please reach out the maintainers of this repo.</p> <p>IBMers don't need credentials to access the internal instance of the STAC service.</p>"},{"location":"download_data/#try-out","title":"Try out","text":"<p>Data Connectors can be used outside the TerraKit Pipeline. Take a look at the TerraKit: Easy geospatial data search and query notebook for more help getting started with TerraKit Data Connectors.</p>"},{"location":"process_labels/","title":"Process labels in preparation for curating a new dataset","text":"<p>The first step in the TerraKit dataset curation pipeline is processing the labels. </p> <p>Use the <code>process_labels</code> function (or the CLI <code>labels</code> subcommand) to provide a directory containing geospatial labels in either vector or raster form. The function will return two DataFrame, the first containing bound box and temporal information for all of the geospatial locations identified in the labels files, the second containing the labels. Each DataFrame is also saved as a <code>.shp</code> file. </p> <p>The temporal information is expected in the label filename with <code>YYYY-MM-DD</code> format.</p> <p>Here's an example of how to use the <code>process_labels</code> step in the TerraKit pipeline:</p> <p><pre><code>from terrakit.transform.labels import process_labels\nlabel_args = {\n    \"dataset_name\": \"MyDataset\",\n    \"working_dir\": \"./tmp\",\n    \"labels\": {\n        \"labels_folder\":  \"./docs/example/test_wildfire_vector\",\n    },\n}\n\nlabels_gdf, grouped_bbox_gdf = process_labels(\n    dataset_name=label_args[\"dataset_name\"],\n    working_dir=label_args[\"working_dir\"],\n    labels_folder=label_args[\"labels\"][\"labels_folder\"],\n)\n</code></pre> TerraKit will output <code>./tmp/MyDataset_all_bboxes.shp</code> and <code>./tmp/MyDataset_labels.shp</code>.</p> <p>Alternatively, write the same arguments in a config file that the TerraKit CLI can use:</p> <p><pre><code># ./config.yaml\ndataset_name: \"MyDataset\"\nworking_dir: \"./tmp\"\nlabels:\n  labels_folder: \"./docs/example/test_wildfire_vector\"\n</code></pre> <pre><code>#!/bin/bash\nterrakit --config ./docs/examples/config.yaml labels\n</code></pre></p>"},{"location":"process_labels/#configure-the-labels-pipeline","title":"Configure the Labels pipeline","text":"<p>Use the following parameters to configure the TerraKit Labels pipeline.</p>"},{"location":"process_labels/#active","title":"Active","text":"<p><code>active</code>: Enables the labels pipeline to run. Set to <code>False</code> to skip the step. Default: <code>True</code></p>"},{"location":"process_labels/#labels-folder","title":"Labels folder","text":"<p><code>labels_folder</code>: Points to a directory containing geospatial label files to be processed. Required parameter.</p>"},{"location":"process_labels/#datetime-information","title":"Datetime Information","text":"<p><code>datetime_info</code>: Set to <code>filename</code> by default, TerraKit will look for temporal information in the label filename in the format <code>YYYY-MM-DD</code>. Alternatively set to <code>csv</code> to provide datetime information in an accompanying csv file in the format:</p> <p><pre><code># metadata.csv\nfilename,date\nEMSR748_AOI01_DEL_MONIT05_observedEventA_v1.json,2024-08-26\nEMSR801_AOI01_DEL_MONIT02_observedEventA_v1.json,2025-04-23\n</code></pre> TerraKit will look a file called <code>metadata.csv</code> in the <code>labels_folder</code>.</p>"},{"location":"process_labels/#label_type","title":"label_type","text":"<p><code>label_type</code>: Set to either <code>raster</code> or <code>vector</code>. TerraKit expects label data in either vector or raster format. </p>"},{"location":"process_labels/#download-example-labels","title":"Download example labels","text":"<p>To download a set of example labels, use the <code>rapid_mapping_geojson_downloader</code> function to get started:</p> <pre><code>from terrakit.general_utils.labels_downloader import rapid_mapping_geojson_downloader\n\nexample_label_1 = rapid_mapping_geojson_downloader(event_id=\"748\", aoi=\"01\", monitoring_number=\"05\", version=\"v1\", dest=\"./docs/examples/test_wildfire_vector\")\n\nexample_label_2 = rapid_mapping_geojson_downloader(event_id=\"801\", aoi=\"01\", monitoring_number=\"02\", version=\"v1\", dest=\"./docs/examples/test_wildfire_vector\")\n</code></pre>"},{"location":"store/","title":"Store","text":"<p>More info coming soon</p>"},{"location":"upload/","title":"Upload","text":"<p>More info coming soon</p>"},{"location":"api/chip/","title":"Chip Documentation","text":"<p>Documentation for the <code>terrakit.chip.tilling</code> module.</p>"},{"location":"api/chip/#terrakit.chip.tiling.chip_and_label_data","title":"<code>chip_and_label_data</code>","text":"<p>Entry point function to the class for Chipping.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>working_dir</code> <code>str</code> <p>Working directory for input and output files.</p> <code>'./tmp'</code> <code>active</code> <code>bool</code> <p>Flag to activate or deactivate chipping.</p> <code>True</code> <code>data_suffix</code> <code>str</code> <p>Suffix of the input data files.</p> <code>'.tif'</code> <code>label_suffix</code> <code>str</code> <p>Suffix of the input label files.</p> <code>'_labels.tif'</code> <code>chip_suffix</code> <code>str</code> <p>Suffix for chipped data files.</p> <code>'.data.tif'</code> <code>chip_label_suffix</code> <code>str</code> <p>Suffix for chipped label files.</p> <code>'.label.tif'</code> <code>sample_dim</code> <code>int</code> <p>Dimension for chipping.</p> <code>256</code> <code>queried_data</code> <code>list</code> <p>List of files to be queried.</p> <code>[]</code> <code>keep_files</code> <code>bool</code> <p>Flag to keep original files after chipping.</p> <code>True</code> <code>match_suffix</code> <code>bool</code> <p>Flag to match suffixes of original and chipped data.</p> <code>True</code> <code>stats</code> <code>bool</code> <p>Bool to choose to calculate dataset stats or not, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of Chipped files.</p> Example <pre><code>import terrakit\n\nchip_args = {\n    \"dataset_name\": \"test_dataset\",\n    \"chip\": {\"sample_dim\": 256},\n}\n\nres = terrakit.chip_and_label_data(\n    dataset_name=chip_args[\"dataset_name\"],\n    sample_dim=chip_args[\"chip\"][\"sample_dim\"],\n    keep_files=True,\n)\n</code></pre> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def chip_and_label_data(\n    dataset_name: str,\n    working_dir: str = \"./tmp\",\n    active: bool = True,\n    data_suffix: str = \".tif\",\n    label_suffix: str = \"_labels.tif\",\n    chip_suffix: str = \".data.tif\",\n    chip_label_suffix: str = \".label.tif\",\n    sample_dim: int = 256,\n    queried_data: list = [],\n    keep_files: bool = True,\n    match_suffix: bool = True,  # TODO: If set  to true, then chip_suffix = data_suffix and chip_label_suffix = label_suffix.\n    stats: bool = True,\n) -&gt; list[str]:\n    \"\"\"\n    Entry point function to the class for Chipping.\n\n    Parameters:\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory for input and output files.\n        active (bool): Flag to activate or deactivate chipping.\n        data_suffix (str): Suffix of the input data files.\n        label_suffix (str): Suffix of the input label files.\n        chip_suffix (str): Suffix for chipped data files.\n        chip_label_suffix (str): Suffix for chipped label files.\n        sample_dim (int): Dimension for chipping.\n        queried_data (list): List of files to be queried.\n        keep_files (bool): Flag to keep original files after chipping.\n        match_suffix (bool): Flag to match suffixes of original and chipped data.\n        stats (bool, optional): Bool to choose to calculate dataset stats or not, by default True\n\n    Returns:\n        list[str]: List of Chipped files.\n\n    Example:\n        ```python\n        import terrakit\n\n        chip_args = {\n            \"dataset_name\": \"test_dataset\",\n            \"chip\": {\"sample_dim\": 256},\n        }\n\n        res = terrakit.chip_and_label_data(\n            dataset_name=chip_args[\"dataset_name\"],\n            sample_dim=chip_args[\"chip\"][\"sample_dim\"],\n            keep_files=True,\n        )\n        ```\n    \"\"\"\n    logging.info(f\"Processing labels with arguments: {locals()}\")\n    pipeline_model = pipeline_model_validation(\n        dataset_name=dataset_name, working_dir=working_dir\n    )\n\n    chip, chip_model = chip_model_validation(\n        pipeline_model=pipeline_model,\n        active=active,\n        data_suffix=data_suffix,\n        label_suffix=label_suffix,\n        chip_suffix=chip_suffix,\n        chip_label_suffix=chip_label_suffix,\n        sample_dim=sample_dim,\n        queried_data=queried_data,\n        keep_files=keep_files,\n        match_suffix=match_suffix,\n        stats=stats,\n    )\n\n    if not active:\n        logging.warning(\n            \"IMPORTANT: Chip_and_label_data is not active. Skipping chip and label data step. Set chip.active = True to activate this step.\"\n        )\n        return []\n\n    try:\n        chip_and_label_list: list[str] = chip.chip_and_label()\n    except rasterio.errors.RasterioIOError as e:\n        logger.error(f\"RasterioIoError while chipping data: {e}\")\n        raise TerrakitBaseException(\"Error while chipping data...\") from e\n    except Exception as e:\n        logger.error(f\"Error while chipping data: {e}\")\n        raise TerrakitBaseException(\"Error while chipping data...\") from e\n\n    # Save dataset metadata to file\n    chip_metadata = {\n        \"step_id\": \"chip\",\n        \"activity\": \"Chip tiles and labels.\",\n        \"method\": \"terrakit.chip.tiling.chip_and_label_data\",\n        \"working_dir\": str(working_dir),\n        \"parameters\": json.loads(chip_model.model_dump_json()),\n    }\n    if stats:\n        dataset_properties = load_dataset_properties(working_dir)\n        chip_metadata[\"dataset_statistics\"] = dataset_properties\n\n    dataset_metdata(pipeline_model, chip_metadata)\n\n    return chip_and_label_list\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.chip_model_validation","title":"<code>chip_model_validation</code>","text":"<p>Function to validate the chip model.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_model</code> <code>PipelineModel</code> <p>Pipeline model instance.</p> required <code>active</code> <code>bool</code> <p>Flag to activate or deactivate chipping.</p> required <code>data_suffix</code> <code>str</code> <p>Suffix of the input data files.</p> required <code>label_suffix</code> <code>str</code> <p>Suffix of the input label files.</p> required <code>chip_suffix</code> <code>str</code> <p>Suffix for chipped data files.</p> required <code>chip_label_suffix</code> <code>str</code> <p>Suffix for chipped label files.</p> required <code>sample_dim</code> <code>int</code> <p>Dimension for chipping.</p> required <code>queried_data</code> <code>list</code> <p>List of files to be queried.</p> required <code>keep_files</code> <code>bool</code> <p>Flag to keep original files after chipping.</p> required <code>match_suffix</code> <code>bool</code> <p>Flag to match suffixes of original and chipped data.</p> required <p>Returns:</p> Name Type Description <code>ChipAndLabelCls</code> <code>ChipAndLabelCls</code> <p>Initialized ChipAndLabelCls instance.</p> <code>ChipAndLabelModel</code> <code>ChipAndLabelModel</code> <p>Validated ChipModel instance.</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If validation of the chip model fails.</p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def chip_model_validation(\n    pipeline_model: PipelineModel,\n    active: bool,\n    data_suffix: str,\n    label_suffix: str,\n    chip_suffix: str,\n    chip_label_suffix: str,\n    sample_dim: int,\n    queried_data: list,\n    keep_files: bool,\n    match_suffix: bool,\n    stats: bool,\n) -&gt; tuple[ChipAndLabelCls, ChipAndLabelModel]:\n    \"\"\"\n    Function to validate the chip model.\n\n    Parameters:\n        pipeline_model (PipelineModel): Pipeline model instance.\n        active (bool): Flag to activate or deactivate chipping.\n        data_suffix (str): Suffix of the input data files.\n        label_suffix (str): Suffix of the input label files.\n        chip_suffix (str): Suffix for chipped data files.\n        chip_label_suffix (str): Suffix for chipped label files.\n        sample_dim (int): Dimension for chipping.\n        queried_data (list): List of files to be queried.\n        keep_files (bool): Flag to keep original files after chipping.\n        match_suffix (bool): Flag to match suffixes of original and chipped data.\n\n    Returns:\n        ChipAndLabelCls: Initialized ChipAndLabelCls instance.\n        ChipAndLabelModel: Validated ChipModel instance.\n\n    Raises:\n        TerrakitValidationError: If validation of the chip model fails.\n    \"\"\"\n    try:\n        chip = ChipAndLabelCls(\n            dataset_name=pipeline_model.dataset_name,\n            working_dir=pipeline_model.working_dir,  # type: ignore[arg-type]\n            active=active,\n            data_suffix=data_suffix,\n            label_suffix=label_suffix,\n            chip_suffix=chip_suffix,\n            chip_label_suffix=chip_label_suffix,\n            sample_dim=sample_dim,\n            queried_data=queried_data,\n            keep_files=keep_files,\n            match_suffix=match_suffix,\n            stats=stats,\n        )  # Initialize class with chip specific args\n        chip_model = ChipAndLabelModel.model_validate(\n            chip\n        )  # validate chip model - do this in the chip class\n    except ValidationError as e:\n        for error in e.errors():\n            logging.error(\n                f\"Invalid label arguments: {error['msg']}. \\n\\t'{error['loc'][0]}' currently set to '{error['input']}. Please update to a valid entry.\"\n            )\n        raise TerrakitValidationError(\n            \"Invalid label arguments\", details=e.errors()\n        ) from e\n    logging.info(f\"Chipping data with arguments: {chip_model}\")\n    return chip, chip_model\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.ChipAndLabelCls","title":"<code>ChipAndLabelCls</code>","text":"<p>Class for chipping and labeling raster data.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> <code>working_dir</code> <code>str</code> <p>Working directory for input and output files.</p> <code>active</code> <code>bool</code> <p>Flag to activate or deactivate chipping.</p> <code>data_suffix</code> <code>str</code> <p>Suffix of the input data files.</p> <code>label_suffix</code> <code>str</code> <p>Suffix of the input label files.</p> <code>chip_suffix</code> <code>str</code> <p>Suffix for chipped data files.</p> <code>chip_label_suffix</code> <code>str</code> <p>Suffix for chipped label files.</p> <code>sample_dim</code> <code>int</code> <p>Dimension for chipping.</p> <code>queried_data</code> <code>list</code> <p>List of files to be queried.</p> <code>keep_files</code> <code>bool</code> <p>Flag to keep original files after chipping.</p> <code>match_suffix</code> <code>bool</code> <p>Flag to match suffixes of original and chipped data.</p> Example <p>To instantiate LabelsCls: <pre><code>from terrakit.chip.tiling import ChipAndLabelCls\n\nChipAndLabelCls(\n    dataset_name=\"my_dataset\",\n    working_dir=\"./tmp\",\n    active=True,\n    data_suffix=\".tif\",\n    label_suffix=\"_labels.tif\",\n    chip_suffix=\".data.tif\",\n    chip_label_suffix=\".label.tif\",\n    sample_dim=256,\n    queried_data=[],\n    keep_files=True,\n    match_suffix=True,\n)\n</code></pre></p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>class ChipAndLabelCls:\n    \"\"\"\n    Class for chipping and labeling raster data.\n\n    Attributes:\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory for input and output files.\n        active (bool): Flag to activate or deactivate chipping.\n        data_suffix (str): Suffix of the input data files.\n        label_suffix (str): Suffix of the input label files.\n        chip_suffix (str): Suffix for chipped data files.\n        chip_label_suffix (str): Suffix for chipped label files.\n        sample_dim (int): Dimension for chipping.\n        queried_data (list): List of files to be queried.\n        keep_files (bool): Flag to keep original files after chipping.\n        match_suffix (bool): Flag to match suffixes of original and chipped data.\n\n    Example:\n        To instantiate LabelsCls:\n        ```python\n        from terrakit.chip.tiling import ChipAndLabelCls\n\n        ChipAndLabelCls(\n            dataset_name=\"my_dataset\",\n            working_dir=\"./tmp\",\n            active=True,\n            data_suffix=\".tif\",\n            label_suffix=\"_labels.tif\",\n            chip_suffix=\".data.tif\",\n            chip_label_suffix=\".label.tif\",\n            sample_dim=256,\n            queried_data=[],\n            keep_files=True,\n            match_suffix=True,\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        dataset_name: str = \"terrakit_curated_dataset\",\n        working_dir: str = \"./tmp\",\n        active: bool = True,\n        data_suffix: str = \".tif\",\n        label_suffix: str = \"_labels.tif\",\n        chip_suffix: str = \".data.tif\",\n        chip_label_suffix: str = \".label.tif\",\n        sample_dim: int = 256,\n        queried_data: list = [],\n        keep_files: bool = True,\n        match_suffix: bool = True,\n        stats: bool = True,\n    ):\n        \"\"\"\n        Initialize LabelsCls with specified parameters.\n\n        Parameters:\n            dataset_name (str): Name of the dataset.\n            working_dir (str): Working directory for input and output files.\n            active (bool): Flag to activate or deactivate chipping.\n            data_suffix (str): Suffix of the input data files.\n            label_suffix (str): Suffix of the input label files.\n            chip_suffix (str): Suffix for chipped data files.\n            chip_label_suffix (str): Suffix for chipped label files.\n            sample_dim (int): Dimension for chipping.\n            queried_data (list): List of files to be queried.\n            keep_files (bool): Flag to keep original files after chipping.\n            match_suffix (bool): Flag to match suffixes of original and chipped data.\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.working_dir = working_dir\n        self.active = active\n        self.data_suffix = data_suffix\n        self.label_suffix = label_suffix\n        self.chip_suffix = chip_suffix\n        self.chip_label_suffix = chip_label_suffix\n        self.sample_dim = sample_dim\n        self.queried_data = queried_data\n        self.keep_files = keep_files\n        self.match_suffix = match_suffix\n        self.stats = stats\n\n    def get_windows(self, data_meta: dict):\n        \"\"\"\n        Function to get windows of the patch.\n\n        Parameters:\n            data_meta (dict): data metadata\n\n        Returns:\n            list: List of windows of the patch.\n        \"\"\"\n        x_coords = [X for X in range(0, data_meta[\"width\"], self.sample_dim)]\n        y_coords = [X for X in range(0, data_meta[\"height\"], self.sample_dim)]\n\n        # If tile will extend beyond bounds of the data, move start of window back\n        x_coords = [\n            (\n                data_meta[\"width\"] - self.sample_dim\n                if X + self.sample_dim &gt; data_meta[\"width\"]\n                else X\n            )\n            for X in x_coords\n        ]\n        y_coords = [\n            (\n                data_meta[\"height\"] - self.sample_dim\n                if Y + self.sample_dim &gt; data_meta[\"height\"]\n                else Y\n            )\n            for Y in y_coords\n        ]\n        windows = []\n        for x in x_coords:\n            for y in y_coords:\n                windows.append(Window(x, y, self.sample_dim, self.sample_dim))\n        return windows\n\n    def create_patch(self, src, win):\n        \"\"\"\n        Function to create patches of the read tif file.\n\n        Parameters:\n            src (Rasterio): Rasterio object of the opened file.\n            win (int): Window index.\n\n        Returns:\n            numpy, dict: Array of the patched tile and dictionary of patched tile metadata.\n        \"\"\"\n\n        win_transform = src.window_transform(win)\n        kwargs = src.meta.copy()\n        kwargs.update(\n            {\n                \"height\": win.height,\n                \"width\": win.width,\n                \"transform\": win_transform,\n            }\n        )\n\n        band_data = []\n        for i in range(1, kwargs[\"count\"] + 1):\n            band_data.append(np.expand_dims(src.read(i, window=win), 0))\n        data = np.concatenate(band_data, axis=0)\n        return data, kwargs\n\n    def files_to_chip(\n        self,\n        working_dir: str,\n    ):\n        \"\"\"\n        Function to chip files.\n\n        Parameters:\n            working_dir (str): Directory of both the input and destination where chipped files will be generated to.\n\n        Returns:\n            list: List of all queried data.\n        \"\"\"\n        if len(self.queried_data) == 0:\n            logger.info(\"looking for files to chip in working dir..\\n\")\n            candidate_list = glob(f\"{working_dir}/*{self.data_suffix}\")\n        else:\n            candidate_list = self.queried_data\n        logger.info(f\"Found candidate files to chip: {candidate_list}\")\n        queried_data = []\n        for file in candidate_list:\n            if self.label_suffix not in file:\n                logger.info(\n                    f\"Adding {file} as it does not match the following 'label_suffix': '{self.label_suffix}'\"\n                )\n                queried_data.append(file)\n        return queried_data\n\n    def chip_and_label(\n        self,\n    ) -&gt; list:\n        \"\"\"\n        Function to Chip the label and data rasters.\n\n        1. Load the input and label images.\n        2. Based on the requested dimension, create list of start and finish indices (in future add options to overlap etc).\n        3. Loop through and subset for each.\n        4. Calculate updated spatial extent metadata.\n        5. Write to file.\n\n        Returns:\n            list: List of all chipped data and label files.\n        \"\"\"\n        data_files_to_chip = self.files_to_chip(self.working_dir)\n        logging.info(f\"Chipping data: {data_files_to_chip}\")\n\n        # Initalize stats:\n        data_file_count = len(data_files_to_chip)\n        sums = [None] * data_file_count\n        sums_sqs = [None] * data_file_count\n        count = 0\n        tile_stats = []\n        dataset_stats = []\n\n        chip_and_label_list: list = []\n\n        for query in tqdm(data_files_to_chip):\n            windows: list\n            query_stem = query.replace(self.data_suffix, \"\")\n            query_label = query.replace(self.data_suffix, self.label_suffix)\n            query_label_stem = query_label.replace(self.label_suffix, \"\")\n            logger.info(f\"{self.data_suffix}\")\n            logger.info(f\"{query}\")\n            logger.info(f\"{query_stem}\")\n            logger.info(f\"{query_label}\")\n            logger.info(f\"{query_label_stem}\")\n\n            with rasterio.open(query) as src:\n                # Per tile operations\n                data_meta = src.meta\n                windows = self.get_windows(data_meta)\n\n                logging.info(f\"Chipping data: {query}\")\n\n                for win_index, win in enumerate(windows):\n                    data, kwargs = self.create_patch(src, win)\n\n                    data_file_name = f\"{query_stem}_{win_index}{self.chip_suffix}\"\n                    if self.chip_suffix.endswith(\".tif\"):\n                        with rasterio.open(data_file_name, \"w\", **kwargs) as dst:\n                            dst.write(data)\n                    else:\n                        # Wrap in xarray DataArray if needed\n                        da = xr.DataArray(data)\n                        da.to_netcdf(data_file_name)\n\n                    chip_and_label_list.append(data_file_name)\n                    win_index += 1\n\n                # gather stats for each tile\n                if self.stats:\n                    bands = src.count\n                    image = src.read()[range(bands), :, :]\n                    sums[count] = image.sum(axis=(1, 2))\n                    sums_sqs[count] = (image**2).sum(axis=(1, 2))\n                    count += 1\n\n                    tile_sums = sum([x for x in sums if x is not None])  # type: ignore[misc]\n                    tile_stats.append(\n                        {\"tile\": query, \"sum\": tile_sums.tolist(), \"bands\": bands}  # type: ignore[attr-defined]\n                    )\n\n            with rasterio.open(query_label) as src:\n                logging.info(f\"Chipping label data: {query_label}\")\n                for win_index, win in enumerate(windows):\n                    data, kwargs = self.create_patch(src, win)\n\n                    label_file_name = (\n                        f\"{query_label_stem}_{win_index}{self.chip_label_suffix}\"\n                    )\n                    if self.chip_suffix.endswith(\".tif\"):\n                        with rasterio.open(\n                            label_file_name,\n                            \"w\",\n                            **kwargs,\n                        ) as dst:\n                            dst.write(data)\n                    else:\n                        # Wrap in xarray DataArray if needed\n                        da = xr.DataArray(data)\n                        da.to_netcdf(label_file_name)\n                    chip_and_label_list.append(label_file_name)\n\n            if self.keep_files is False:\n                logging.info(f\"Cleaning up files...{query}\")\n                os.remove(query)\n                os.remove(query_label)\n        logging.info(f\"finished chipping {chip_and_label_list}\")\n\n        # Calculate dataset stats\n        if self.stats:\n            sums = [x for x in sums if x is not None]\n            sums_sqs = [x for x in sums_sqs if x is not None]\n            total_sum = sum(sums)  # type: ignore[arg-type]\n            total_sum_sqs = sum(sums_sqs)  # type: ignore[arg-type]\n            pixel_count = count * image.shape[1] * image.shape[2]\n            total_mean = np.float64(total_sum / pixel_count)\n            total_var = (total_sum_sqs / pixel_count) - (total_mean**2)\n            total_std = np.float64(np.sqrt(total_var))\n            dataset_stats = format_dataset_stats(\n                self.dataset_name,\n                self.chip_suffix,\n                total_mean,\n                total_std,\n                bands,\n                tile_stats,\n            )\n            save_dataset_properties(self.working_dir, dataset_stats)\n        logging.info(f\"Returning chip_and_label_list: {chip_and_label_list}\")\n        return chip_and_label_list\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.ChipAndLabelCls.get_windows","title":"<code>get_windows</code>","text":"<p>Function to get windows of the patch.</p> <p>Parameters:</p> Name Type Description Default <code>data_meta</code> <code>dict</code> <p>data metadata</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of windows of the patch.</p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def get_windows(self, data_meta: dict):\n    \"\"\"\n    Function to get windows of the patch.\n\n    Parameters:\n        data_meta (dict): data metadata\n\n    Returns:\n        list: List of windows of the patch.\n    \"\"\"\n    x_coords = [X for X in range(0, data_meta[\"width\"], self.sample_dim)]\n    y_coords = [X for X in range(0, data_meta[\"height\"], self.sample_dim)]\n\n    # If tile will extend beyond bounds of the data, move start of window back\n    x_coords = [\n        (\n            data_meta[\"width\"] - self.sample_dim\n            if X + self.sample_dim &gt; data_meta[\"width\"]\n            else X\n        )\n        for X in x_coords\n    ]\n    y_coords = [\n        (\n            data_meta[\"height\"] - self.sample_dim\n            if Y + self.sample_dim &gt; data_meta[\"height\"]\n            else Y\n        )\n        for Y in y_coords\n    ]\n    windows = []\n    for x in x_coords:\n        for y in y_coords:\n            windows.append(Window(x, y, self.sample_dim, self.sample_dim))\n    return windows\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.ChipAndLabelCls.create_patch","title":"<code>create_patch</code>","text":"<p>Function to create patches of the read tif file.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Rasterio</code> <p>Rasterio object of the opened file.</p> required <code>win</code> <code>int</code> <p>Window index.</p> required <p>Returns:</p> Type Description <p>numpy, dict: Array of the patched tile and dictionary of patched tile metadata.</p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def create_patch(self, src, win):\n    \"\"\"\n    Function to create patches of the read tif file.\n\n    Parameters:\n        src (Rasterio): Rasterio object of the opened file.\n        win (int): Window index.\n\n    Returns:\n        numpy, dict: Array of the patched tile and dictionary of patched tile metadata.\n    \"\"\"\n\n    win_transform = src.window_transform(win)\n    kwargs = src.meta.copy()\n    kwargs.update(\n        {\n            \"height\": win.height,\n            \"width\": win.width,\n            \"transform\": win_transform,\n        }\n    )\n\n    band_data = []\n    for i in range(1, kwargs[\"count\"] + 1):\n        band_data.append(np.expand_dims(src.read(i, window=win), 0))\n    data = np.concatenate(band_data, axis=0)\n    return data, kwargs\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.ChipAndLabelCls.files_to_chip","title":"<code>files_to_chip</code>","text":"<p>Function to chip files.</p> <p>Parameters:</p> Name Type Description Default <code>working_dir</code> <code>str</code> <p>Directory of both the input and destination where chipped files will be generated to.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of all queried data.</p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def files_to_chip(\n    self,\n    working_dir: str,\n):\n    \"\"\"\n    Function to chip files.\n\n    Parameters:\n        working_dir (str): Directory of both the input and destination where chipped files will be generated to.\n\n    Returns:\n        list: List of all queried data.\n    \"\"\"\n    if len(self.queried_data) == 0:\n        logger.info(\"looking for files to chip in working dir..\\n\")\n        candidate_list = glob(f\"{working_dir}/*{self.data_suffix}\")\n    else:\n        candidate_list = self.queried_data\n    logger.info(f\"Found candidate files to chip: {candidate_list}\")\n    queried_data = []\n    for file in candidate_list:\n        if self.label_suffix not in file:\n            logger.info(\n                f\"Adding {file} as it does not match the following 'label_suffix': '{self.label_suffix}'\"\n            )\n            queried_data.append(file)\n    return queried_data\n</code></pre>"},{"location":"api/chip/#terrakit.chip.tiling.ChipAndLabelCls.chip_and_label","title":"<code>chip_and_label</code>","text":"<p>Function to Chip the label and data rasters.</p> <ol> <li>Load the input and label images.</li> <li>Based on the requested dimension, create list of start and finish indices (in future add options to overlap etc).</li> <li>Loop through and subset for each.</li> <li>Calculate updated spatial extent metadata.</li> <li>Write to file.</li> </ol> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of all chipped data and label files.</p> Source code in <code>terrakit/chip/tiling.py</code> <pre><code>def chip_and_label(\n    self,\n) -&gt; list:\n    \"\"\"\n    Function to Chip the label and data rasters.\n\n    1. Load the input and label images.\n    2. Based on the requested dimension, create list of start and finish indices (in future add options to overlap etc).\n    3. Loop through and subset for each.\n    4. Calculate updated spatial extent metadata.\n    5. Write to file.\n\n    Returns:\n        list: List of all chipped data and label files.\n    \"\"\"\n    data_files_to_chip = self.files_to_chip(self.working_dir)\n    logging.info(f\"Chipping data: {data_files_to_chip}\")\n\n    # Initalize stats:\n    data_file_count = len(data_files_to_chip)\n    sums = [None] * data_file_count\n    sums_sqs = [None] * data_file_count\n    count = 0\n    tile_stats = []\n    dataset_stats = []\n\n    chip_and_label_list: list = []\n\n    for query in tqdm(data_files_to_chip):\n        windows: list\n        query_stem = query.replace(self.data_suffix, \"\")\n        query_label = query.replace(self.data_suffix, self.label_suffix)\n        query_label_stem = query_label.replace(self.label_suffix, \"\")\n        logger.info(f\"{self.data_suffix}\")\n        logger.info(f\"{query}\")\n        logger.info(f\"{query_stem}\")\n        logger.info(f\"{query_label}\")\n        logger.info(f\"{query_label_stem}\")\n\n        with rasterio.open(query) as src:\n            # Per tile operations\n            data_meta = src.meta\n            windows = self.get_windows(data_meta)\n\n            logging.info(f\"Chipping data: {query}\")\n\n            for win_index, win in enumerate(windows):\n                data, kwargs = self.create_patch(src, win)\n\n                data_file_name = f\"{query_stem}_{win_index}{self.chip_suffix}\"\n                if self.chip_suffix.endswith(\".tif\"):\n                    with rasterio.open(data_file_name, \"w\", **kwargs) as dst:\n                        dst.write(data)\n                else:\n                    # Wrap in xarray DataArray if needed\n                    da = xr.DataArray(data)\n                    da.to_netcdf(data_file_name)\n\n                chip_and_label_list.append(data_file_name)\n                win_index += 1\n\n            # gather stats for each tile\n            if self.stats:\n                bands = src.count\n                image = src.read()[range(bands), :, :]\n                sums[count] = image.sum(axis=(1, 2))\n                sums_sqs[count] = (image**2).sum(axis=(1, 2))\n                count += 1\n\n                tile_sums = sum([x for x in sums if x is not None])  # type: ignore[misc]\n                tile_stats.append(\n                    {\"tile\": query, \"sum\": tile_sums.tolist(), \"bands\": bands}  # type: ignore[attr-defined]\n                )\n\n        with rasterio.open(query_label) as src:\n            logging.info(f\"Chipping label data: {query_label}\")\n            for win_index, win in enumerate(windows):\n                data, kwargs = self.create_patch(src, win)\n\n                label_file_name = (\n                    f\"{query_label_stem}_{win_index}{self.chip_label_suffix}\"\n                )\n                if self.chip_suffix.endswith(\".tif\"):\n                    with rasterio.open(\n                        label_file_name,\n                        \"w\",\n                        **kwargs,\n                    ) as dst:\n                        dst.write(data)\n                else:\n                    # Wrap in xarray DataArray if needed\n                    da = xr.DataArray(data)\n                    da.to_netcdf(label_file_name)\n                chip_and_label_list.append(label_file_name)\n\n        if self.keep_files is False:\n            logging.info(f\"Cleaning up files...{query}\")\n            os.remove(query)\n            os.remove(query_label)\n    logging.info(f\"finished chipping {chip_and_label_list}\")\n\n    # Calculate dataset stats\n    if self.stats:\n        sums = [x for x in sums if x is not None]\n        sums_sqs = [x for x in sums_sqs if x is not None]\n        total_sum = sum(sums)  # type: ignore[arg-type]\n        total_sum_sqs = sum(sums_sqs)  # type: ignore[arg-type]\n        pixel_count = count * image.shape[1] * image.shape[2]\n        total_mean = np.float64(total_sum / pixel_count)\n        total_var = (total_sum_sqs / pixel_count) - (total_mean**2)\n        total_std = np.float64(np.sqrt(total_var))\n        dataset_stats = format_dataset_stats(\n            self.dataset_name,\n            self.chip_suffix,\n            total_mean,\n            total_std,\n            bands,\n            tile_stats,\n        )\n        save_dataset_properties(self.working_dir, dataset_stats)\n    logging.info(f\"Returning chip_and_label_list: {chip_and_label_list}\")\n    return chip_and_label_list\n</code></pre>"},{"location":"api/download/","title":"Download Data Documentation","text":"<p>Documentation for the <code>terrakit.download.download</code> module.</p>"},{"location":"api/download/#terrakit.download.download_data.download_data","title":"<code>download_data</code>","text":"<p>Download and preprocess geospatial data.</p> <p>Parameters:</p> Name Type Description Default <code>data_sources</code> <code>Union[list[DataSource], list[dict[str, Any]]]</code> <p>List of data sources to query.</p> <code>[loads(model_dump_json())]</code> <code>date_allowance</code> <code>Union[DateAllowance, dict[str, Any]]</code> <p>Date range allowance for data query.</p> <code>loads(model_dump_json())</code> <code>transform</code> <code>Union[Transform, dict[str, Any]]</code> <p>Transformation parameters for data.</p> <code>loads(model_dump_json())</code> <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> <code>'terrakit_curated_dataset'</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files.</p> <code>'./tmp'</code> <code>active</code> <code>bool</code> <p>Flag to activate/deactivate data download.</p> <code>True</code> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover percentage for data selection.</p> <code>80</code> <code>datetime_bbox_shp_file</code> <code>str</code> <p>Path to shapefile containing datetime bounding boxes.</p> <code>'./tmp/terrakit_curated_dataset_all_bboxes.shp'</code> <code>labels_shp_file</code> <code>str</code> <p>Path to shapefile containing labels.</p> <code>'./tmp/terrakit_curated_dataset_labels.shp'</code> <code>keep_files</code> <code>bool</code> <p>Flag to keep shapefiles once they have been used. Downloaded files will not be removed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of queried data file paths.</p> <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If a RuntimeError occurs while finding or querying data</p> <code>TerrakitValueError</code> <p>If a TerrakitValueError occurs while finding or reading shp files</p> <code>TerrakitValidationError</code> <p>If a TerrakitValidationError occurs while finding or reading shp files</p> <p>Example:     <pre><code>import terrakit\n\nconfig = {\n    \"download\": {\n        \"data_sources\": [\n            {\n                \"data_connector\": \"sentinel_aws\",\n                \"collection_name\": \"sentinel-2-l2a\",\n                \"bands\": [\"blue\", \"green\", \"red\"],\n                \"save_file\": \"\",\n            },\n        ],\n        \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},\n        \"transform\": {\n            \"scale_data_xarray\": True,\n            \"impute_nans\": True,\n            \"reproject\": True,\n        },\n        \"max_cloud_cover\": 80,\n    },\n}\n\nqueried_data = terrakit.download_data(\n    data_sources=config[\"download\"][\"data_sources\"],\n    date_allowance=config[\"download\"][\"date_allowance\"],\n    transform=config[\"download\"][\"transform\"],\n    max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],\n    dataset_name=\"test_dataset\",\n    working_dir=\"./tmp\",\n    keep_files=False,\n)\n</code></pre></p> Source code in <code>terrakit/download/download_data.py</code> <pre><code>def download_data(\n    data_sources: Union[list[DataSource], list[dict[str, Any]]] = [\n        json.loads(DataSource().model_dump_json())\n    ],\n    date_allowance: Union[DateAllowance, dict[str, Any]] = json.loads(\n        DateAllowance().model_dump_json()\n    ),\n    transform: Union[Transform, dict[str, Any]] = json.loads(\n        Transform().model_dump_json()\n    ),\n    dataset_name: str = \"terrakit_curated_dataset\",\n    working_dir: str = \"./tmp\",\n    active: bool = True,\n    max_cloud_cover: int = 80,\n    datetime_bbox_shp_file: str = \"./tmp/terrakit_curated_dataset_all_bboxes.shp\",\n    labels_shp_file: str = \"./tmp/terrakit_curated_dataset_labels.shp\",\n    keep_files: bool = False,\n) -&gt; list:\n    \"\"\"\n    Download and preprocess geospatial data.\n\n    Args:\n        data_sources (Union[list[DataSource], list[dict[str, Any]]]): List of data sources to query.\n        date_allowance (Union[DateAllowance, dict[str, Any]]): Date range allowance for data query.\n        transform (Union[Transform, dict[str, Any]]): Transformation parameters for data.\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory for temporary files.\n        active (bool): Flag to activate/deactivate data download.\n        max_cloud_cover (int): Maximum cloud cover percentage for data selection.\n        datetime_bbox_shp_file (str): Path to shapefile containing datetime bounding boxes.\n        labels_shp_file (str): Path to shapefile containing labels.\n        keep_files (bool): Flag to keep shapefiles once they have been used. Downloaded files will not be removed.\n\n    Returns:\n        list: List of queried data file paths.\n\n    Raises:\n        TerrakitBaseException: If a RuntimeError occurs while finding or querying data\n        TerrakitValueError: If a TerrakitValueError occurs while finding or reading shp files\n        TerrakitValidationError: If a TerrakitValidationError occurs while finding or reading shp files\n    Example:\n        ```python\n        import terrakit\n\n        config = {\n            \"download\": {\n                \"data_sources\": [\n                    {\n                        \"data_connector\": \"sentinel_aws\",\n                        \"collection_name\": \"sentinel-2-l2a\",\n                        \"bands\": [\"blue\", \"green\", \"red\"],\n                        \"save_file\": \"\",\n                    },\n                ],\n                \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},\n                \"transform\": {\n                    \"scale_data_xarray\": True,\n                    \"impute_nans\": True,\n                    \"reproject\": True,\n                },\n                \"max_cloud_cover\": 80,\n            },\n        }\n\n        queried_data = terrakit.download_data(\n            data_sources=config[\"download\"][\"data_sources\"],\n            date_allowance=config[\"download\"][\"date_allowance\"],\n            transform=config[\"download\"][\"transform\"],\n            max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],\n            dataset_name=\"test_dataset\",\n            working_dir=\"./tmp\",\n            keep_files=False,\n        )\n        ```\n    \"\"\"\n    if not active:\n        logging.warning(\n            \"IMPORTANT: Download is not active. Skipping download data step. Set download.active = True to activate download.\"\n        )\n        return []\n\n    logging.info(f\"Processing download_data with arguments: {locals()}\")\n    pipeline_model = pipeline_model_validation(\n        dataset_name=dataset_name, working_dir=working_dir\n    )\n    download, download_model = download_validation(\n        pipeline_model=pipeline_model,\n        date_allowance=date_allowance,\n        transform=transform,\n        data_sources=data_sources,\n        active=active,\n        max_cloud_cover=max_cloud_cover,\n        datetime_bbox_shp_file=datetime_bbox_shp_file,\n        labels_shp_file=labels_shp_file,\n        keep_files=keep_files,\n    )\n\n    logging.info(\"Listing collections..\")\n    for source in download.data_sources:\n        dc = DataConnector(connector_type=source.data_connector)\n        logging.info(dc.connector.list_collections())\n\n    # Find and query data\n    try:\n        queried_data: list = download.find_and_query_data_for_matching_dates()\n    except RuntimeError as e:\n        logger.error(\"-----&gt; ERROR &lt;------\")\n        logger.error(e)\n        logger.error(\"-----&gt; ERROR &lt;------\")\n        raise TerrakitBaseException(\"Error while finding data...\") from e\n    except TerrakitValueError as e:\n        raise e\n    except TerrakitValidationError as e:\n        raise e\n\n    # Rasterize\n    file_save_count = download.rasterize_vectors_to_the_queried_data(\n        queried_data=queried_data\n    )\n\n    if file_save_count &gt; 0:\n        logging.info(f\"Successfully rasterized {file_save_count} files\")\n\n    download_metadata = {\n        \"step_id\": \"download\",\n        \"activity\": \"Extract datetime and bounding boxes from labels. Download data for a given date and bbox according to parameters.\",\n        \"method\": \"terrakit.download.download_data\",\n        \"working_dir\": str(working_dir),\n        \"parameters\": json.loads(download_model.model_dump_json()),\n    }\n\n    dataset_metdata(pipeline_model, download_metadata)\n\n    return queried_data\n</code></pre>"},{"location":"api/download/#terrakit.download.download_data.download_validation","title":"<code>download_validation</code>","text":"<p>Validate and initialize the download process.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_model</code> <code>PipelineModel</code> <p>Pipeline model containing dataset and working directory information.</p> required <code>date_allowance</code> <code>Union[DateAllowance, dict[str, Any]]</code> <p>Date range allowance for data query.</p> required <code>transform</code> <code>Union[Transform, dict[str, Any]]</code> <p>Transformation parameters for data.</p> required <code>data_sources</code> <code>Union[list[DataSource], list[dict[str, Any]]]</code> <p>List of data sources to query.</p> required <code>active</code> <code>bool</code> <p>Flag to activate/deactivate data download.</p> <code>True</code> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover percentage for data selection.</p> <code>80</code> <code>datetime_bbox_shp_file</code> <code>str</code> <p>Path to shapefile containing datetime bounding boxes.</p> <code>'./tmp/terrakit_curated_dataset_all_bboxes.shp'</code> <code>labels_shp_file</code> <code>str</code> <p>Path to shapefile containing labels.</p> <code>'./tmp/terrakit_curated_dataset_labels.shp'</code> <code>keep_files</code> <code>bool</code> <p>Flag to keep shapefiles once they have been used. Downloaded files will not be removed.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DownloadCls</code> <code>DownloadCls</code> <p>Initialized DownloadCls object.</p> <code>DownloadModel</code> <code>DownloadModel</code> <p>Validated DownloadModel instance.</p> <p>Examples:</p> <pre><code>from terrakit.validate.download_model import DownloadModel\ndownload_model = DownloadModel.model_validate(download)\n</code></pre> Source code in <code>terrakit/download/download_data.py</code> <pre><code>def download_validation(\n    pipeline_model: PipelineModel,\n    date_allowance: Union[DateAllowance, dict[str, Any]],\n    transform: Union[Transform, dict[str, Any]],\n    data_sources: Union[list[DataSource], list[dict[str, Any]]],\n    active: bool = True,\n    max_cloud_cover: int = 80,\n    datetime_bbox_shp_file: str = \"./tmp/terrakit_curated_dataset_all_bboxes.shp\",\n    labels_shp_file: str = \"./tmp/terrakit_curated_dataset_labels.shp\",\n    keep_files: bool = False,\n) -&gt; tuple[DownloadCls, DownloadModel]:\n    \"\"\"\n    Validate and initialize the download process.\n\n    Args:\n        pipeline_model (PipelineModel): Pipeline model containing dataset and working directory information.\n        date_allowance (Union[DateAllowance, dict[str, Any]]): Date range allowance for data query.\n        transform (Union[Transform, dict[str, Any]]): Transformation parameters for data.\n        data_sources (Union[list[DataSource], list[dict[str, Any]]]): List of data sources to query.\n        active (bool): Flag to activate/deactivate data download.\n        max_cloud_cover (int): Maximum cloud cover percentage for data selection.\n        datetime_bbox_shp_file (str): Path to shapefile containing datetime bounding boxes.\n        labels_shp_file (str): Path to shapefile containing labels.\n        keep_files (bool): Flag to keep shapefiles once they have been used. Downloaded files will not be removed.\n\n    Returns:\n        DownloadCls: Initialized DownloadCls object.\n        DownloadModel: Validated DownloadModel instance.\n\n    Examples:\n        ```python\n        from terrakit.validate.download_model import DownloadModel\n        download_model = DownloadModel.model_validate(download)\n        ```\n    \"\"\"\n    logging.info(f\"Processing download_data with arguments: {locals()}\")\n    data_source_list: list = []\n    for source in data_sources:\n        if isinstance(source, dict):\n            if \"data_connector\" not in source:\n                msg = \"Dict in data_source list did not contain 'data_connector'\"\n                raise TerrakitValidationError(msg)\n            if \"collection_name\" not in source:\n                msg = \"Dict in data_source list did not contain 'collection_name'\"\n                raise TerrakitValidationError(msg)\n            if \"bands\" not in source:\n                msg = \"Dict in data_source list did not contain 'bands'\"\n                raise TerrakitValidationError(msg)\n            if \"save_file\" not in source:\n                logger.info(\n                    \"save_file not explicitly set. This will be set dynamically by the data connector instead.\"\n                )\n                source[\"save_file\"] = None\n    for source in data_sources:\n        data_source_list.append(\n            DataSource(\n                data_connector=source[\"data_connector\"],  # type: ignore[index]\n                collection_name=source[\"collection_name\"],  # type: ignore[index]\n                bands=source[\"bands\"],  # type: ignore[index]\n                save_file=source[\"save_file\"],  # type: ignore[index]\n            )\n        )\n\n    if isinstance(date_allowance, dict):\n        if \"pre_days\" not in date_allowance:\n            msg = \"Dict in date_allowance list did not contain 'pre_days'\"\n            raise TerrakitValidationError(msg)\n        if \"post_days\" not in date_allowance:\n            msg = \"Dict in date_allowance list did not contain 'post_days'\"\n    date_allowance = DateAllowance(\n        pre_days=date_allowance[\"pre_days\"],\n        post_days=date_allowance[\"post_days\"],  # type: ignore[index]\n    )\n\n    if isinstance(transform, dict):\n        if \"scale_data_xarray\" not in transform:\n            raise TerrakitValidationError(msg)\n        if \"impute_nans\" not in transform:\n            msg = \"Dict in transform list did not contain 'impute_nans'\"\n        if \"reproject\" not in transform:\n            msg = \"Dict in transform list did not contain 'reproject'\"\n    transform = Transform(\n        scale_data_xarray=transform[\"scale_data_xarray\"],  # type: ignore[index]\n        impute_nans=transform[\"impute_nans\"],  # type: ignore[index]\n        reproject=transform[\"reproject\"],  # type: ignore[index]\n    )\n    download = DownloadCls(\n        dataset_name=pipeline_model.dataset_name,\n        working_dir=pipeline_model.working_dir,  # type: ignore[arg-type]\n        active=active,\n        max_cloud_cover=max_cloud_cover,\n        datetime_bbox_shp_file=datetime_bbox_shp_file,\n        keep_files=keep_files,\n        data_sources=data_source_list,\n        date_allowance=date_allowance,\n        labels_shp_file=labels_shp_file,\n        transform=transform,\n    )\n\n    download_model = DownloadModel.model_validate(download)\n    logging.info(f\"Downloading data with arguments: {download_model}\")\n    return download, download_model\n</code></pre>"},{"location":"api/download/#terrakit.download.download_data.DownloadCls","title":"<code>DownloadCls</code>","text":"<p>Class to handle the download and preprocessing of geospatial data.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> <code>working_dir</code> <code>str</code> <p>Working directory for shapefiles and downloaded tiles.</p> <code>transform</code> <code>Transform</code> <p>Transformation parameters for data.</p> <code>date_allowance</code> <code>DateAllowance</code> <p>Date range allowance for data query.</p> <code>data_sources</code> <code>list[DataSource]</code> <p>List of data sources to query.</p> <code>active</code> <code>bool</code> <p>Flag to activate/deactivate data download.</p> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover percentage for data selection.</p> <code>keep_files</code> <code>bool</code> <p>Flag to keep shapefiles once they have been used. Downloaded files will not be removed.</p> <code>datetime_bbox_shp_file</code> <code>str</code> <p>Path to shapefile containing datetime and bounding boxes to be downloaded.</p> <code>labels_shp_file</code> <code>str</code> <p>Path to shapefile containing labels.</p> Example <p>To instantiate and validate DownloadCls: <pre><code>from terrakit.download.download_data import DownloadCls\nfrom terrakit.validate.download_model import (\n    Transform,\n    DataSource,\n    DateAllowance,\n)\n\ndata_source = DataSource(\n    data_connector = \"sentinel_aws\",\n    collection_name = \"sentinel-2-l2a\",\n    bands = [\"blue\", \"green\", \"red\"],\n    save_file = \"\",\n)\ndate_allowance = DateAllowance(\n    pre_days = 0, post_days = 21\n)\ntransform = Transform(\n    scale_data_xarray=True,\n    impute_nans=True,\n    reproject=True,\n)\ndownload = DownloadCls(\n    dataset_name=\"terrakit_curated_dataset\",\n    working_dir=\"./tmp\",\n    active=True,\n    max_cloud_cover=80,\n    datetime_bbox_shp_file=\"./tmp/terrakit_curated_dataset_all_bboxes.shp\",\n    keep_files=False,\n    data_sources=[data_source],\n    date_allowance=date_allowance,\n    labels_shp_file= \"./tmp/terrakit_curated_dataset_labels.shp\",\n    transform=transform,\n)\n</code></pre></p> Source code in <code>terrakit/download/download_data.py</code> <pre><code>class DownloadCls:\n    \"\"\"\n    Class to handle the download and preprocessing of geospatial data.\n\n    Attributes:\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory for shapefiles and downloaded tiles.\n        transform (Transform): Transformation parameters for data.\n        date_allowance (DateAllowance): Date range allowance for data query.\n        data_sources (list[DataSource]): List of data sources to query.\n        active (bool): Flag to activate/deactivate data download.\n        max_cloud_cover (int): Maximum cloud cover percentage for data selection.\n        keep_files (bool): Flag to keep shapefiles once they have been used. Downloaded files will not be removed.\n        datetime_bbox_shp_file (str): Path to shapefile containing datetime and bounding boxes to be downloaded.\n        labels_shp_file (str): Path to shapefile containing labels.\n\n    Example:\n        To instantiate and validate DownloadCls:\n        ```python\n        from terrakit.download.download_data import DownloadCls\n        from terrakit.validate.download_model import (\n            Transform,\n            DataSource,\n            DateAllowance,\n        )\n\n        data_source = DataSource(\n            data_connector = \"sentinel_aws\",\n            collection_name = \"sentinel-2-l2a\",\n            bands = [\"blue\", \"green\", \"red\"],\n            save_file = \"\",\n        )\n        date_allowance = DateAllowance(\n            pre_days = 0, post_days = 21\n        )\n        transform = Transform(\n            scale_data_xarray=True,\n            impute_nans=True,\n            reproject=True,\n        )\n        download = DownloadCls(\n            dataset_name=\"terrakit_curated_dataset\",\n            working_dir=\"./tmp\",\n            active=True,\n            max_cloud_cover=80,\n            datetime_bbox_shp_file=\"./tmp/terrakit_curated_dataset_all_bboxes.shp\",\n            keep_files=False,\n            data_sources=[data_source],\n            date_allowance=date_allowance,\n            labels_shp_file= \"./tmp/terrakit_curated_dataset_labels.shp\",\n            transform=transform,\n        )\n\n        ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        transform: Transform = json.loads(Transform().model_dump_json()),\n        date_allowance: DateAllowance = json.loads(DateAllowance().model_dump_json()),\n        data_sources: list[DataSource] = [json.loads(DataSource().model_dump_json())],\n        dataset_name: str = \"terrakit_curated_dataset\",\n        working_dir: str = \"./tmp\",\n        active: bool = True,\n        max_cloud_cover: int = 80,\n        keep_files: bool = False,\n        datetime_bbox_shp_file: str = \"./tmp/terrakit_curated_dataset_all_bboxes.shp\",\n        labels_shp_file: str = \"./tmp/terrakit_curated_dataset_labels.shp\",\n    ):\n        \"\"\"\n        Initialize DownloadCls with specified parameters.\n\n        Parameters:\n            transform (Union[Transform, dict[str, Any]]): Transformation parameters for data.\n            date_allowance (Union[DateAllowance, dict[str, Any]]): Date range allowance for data query.\n            data_sources (Union[list[DataSource], list[dict[str, Any]]]): List of data sources to query.\n            dataset_name (str): Name of the dataset.\n            working_dir (str): Working directory for temporary files.\n            active (bool): Flag to activate/deactivate data download.\n            max_cloud_cover (int): Maximum cloud cover percentage for data selection.\n            keep_files (bool): Flag to keep shapefiles once they have been used. Downloaded files will not be removed.\n            datetime_bbox_shp_file (str): Path to shapefile containing datetime bounding boxes.\n            labels_shp_file (str): Path to shapefile containing labels.\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.working_dir = working_dir\n        self.transform = transform\n        self.date_allowance = date_allowance\n        self.active = active\n        self.max_cloud_cover = max_cloud_cover\n        self.keep_files = keep_files\n        self.datetime_bbox_shp_file = datetime_bbox_shp_file\n        self.labels_shp_file = labels_shp_file\n        self.data_sources = data_sources\n\n    \"\"\"Supported shapefile types\"\"\"\n    SHP_FILE_TYPES = Literal[\"labels\", \"bbox\"]\n\n    def _find_shp_file(self, shp_file_type: SHP_FILE_TYPES, shp_file_path: str) -&gt; str:\n        \"\"\"\n        Find and return the path to the specified shapefile.\n\n        Args:\n            shp_file_type (SHP_FILE_TYPES): Type of shapefile ('labels' or 'bbox').\n            shp_file_path (str): Path to the shapefile.\n\n        Returns:\n            str: Path to the shapefile.\n\n        Raises:\n            TerrakitValidationError: If the specified shapefile does not exist.\n        \"\"\"\n        # Check if shp_file_path is passed in as a parameter\n        if os.path.isfile(shp_file_path) is False:\n            # Otherwise, check the working directory for the default shp file: {working_dir}/{dataset_name}_{shp_file_suffix}.shp\n            if shp_file_type == \"labels\":\n                shp_file_suffix = \"labels\"\n            else:\n                shp_file_suffix = \"all_bboxes\"\n\n            shp_filename = f\"{self.dataset_name}_{shp_file_suffix}.shp\"\n\n            # If the dataset_name is empty, then the default shp files will just be called \"all_bboxes.shp\" and \"labels.shp\"\n            if self.dataset_name == \"\":\n                shp_filename = f\"{shp_file_suffix}.shp\"\n            else:\n                shp_filename = f\"{self.dataset_name}_{shp_file_suffix}.shp\"\n\n            shp_file_path = f\"{self.working_dir}/{shp_filename}\"\n\n        # Now we have the shp file path we are looking for, check that it exists.\n        if os.path.isfile(shp_file_path) is False:\n            msg = f\"The specified shp file '{shp_file_path}' does not exist. Please make sure the file exists.\"\n            logger.warning(msg)\n            raise TerrakitValidationError(msg)\n        return shp_file_path\n\n    def _read_shp_file(self, shp_file_path) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Read the specified shapefile into a GeoDataFrame.\n\n        Args:\n            shp_file_path (str): Path to the shapefile.\n\n        Returns:\n            gpd.GeoDataFrame: GeoDataFrame containing the shapefile data.\n\n        Raises:\n            TerrakitValueError: If the shapefile cannot be read.\n            TerrakitValidationError: If the shapefile does not contain 'geometry' and 'datetime' columns.\n        \"\"\"\n        try:\n            shp_file_gdf = gpd.read_file(shp_file_path)\n        except TypeError as e:\n            err_msg = f\"Error reading shp file: {shp_file_path}. {e}\"\n            logger.warning(err_msg)\n            raise TerrakitValueError(err_msg)\n        except ValueError as e:\n            err_msg = f\"Error reading shp file: {shp_file_path}. {e}\"\n            logger.warning(err_msg)\n            raise TerrakitValueError(err_msg)\n        except Exception as e:\n            err_msg = f\"Error reading shp file: {shp_file_path}. {e}\"\n            logger.warning(err_msg)\n            raise TerrakitValueError(err_msg)\n        if \"geometry\" not in shp_file_gdf or \"datetime\" not in shp_file_gdf:\n            msg = \"Input data must contain both 'geometry' and 'datetime' columns.\"\n            logger.warning(msg)\n            raise TerrakitValidationError(msg)\n        return shp_file_gdf\n\n    def find_and_query_data_for_matching_dates(\n        self,\n    ) -&gt; list:\n        \"\"\"\n        Find and query data for matching dates from the specified data sources.\n\n        Returns:\n            list: List of queried data file paths.\n        \"\"\"\n        bbox_shp_file = self._find_shp_file(\n            shp_file_type=\"bbox\", shp_file_path=self.datetime_bbox_shp_file\n        )\n        grouped_bbox_gdf = self._read_shp_file(bbox_shp_file)\n\n        queried_data = []\n        for li in range(0, len(grouped_bbox_gdf)):\n            l = grouped_bbox_gdf.loc[li]  # noqa\n\n            from_date = (\n                datetime.strptime(l.datetime, \"%Y-%m-%d\")\n                - timedelta(days=self.date_allowance.pre_days)\n            ).strftime(\"%Y-%m-%d\")\n            to_date = (\n                datetime.strptime(l.datetime, \"%Y-%m-%d\")\n                + timedelta(days=self.date_allowance.post_days)\n            ).strftime(\"%Y-%m-%d\")\n\n            for source in self.data_sources:\n                dc = DataConnector(connector_type=source.data_connector)\n\n                logger.info(source.collection_name)\n                logger.info(from_date)\n                logger.info(to_date)\n                logger.info(list(l.geometry.bounds))\n                unique_dates, results = dc.connector.find_data(  # type: ignore[attr-defined]\n                    data_collection_name=source.collection_name,\n                    date_start=from_date,\n                    date_end=to_date,\n                    bbox=list(l.geometry.bounds),\n                    bands=source.bands,\n                )\n\n                if len(unique_dates) == 0:  # type: ignore[arg-type]\n                    logger.warning(\n                        f\"No data found for given request: {source}, {from_date}, {to_date}, {list(l.geometry.bounds)}.\"\n                    )\n                    return []\n\n                logger.info(unique_dates)\n\n                # Now find the closest date from the search\n                time_diffs_abs = [\n                    abs(\n                        datetime.strptime(X, \"%Y-%m-%d\")\n                        - datetime.strptime(l.datetime, \"%Y-%m-%d\")\n                    )\n                    for X in unique_dates  # type: ignore[union-attr]\n                ]\n                closest_index = time_diffs_abs.index(min(time_diffs_abs))\n\n                closest_date = unique_dates[closest_index]  # type: ignore[index]\n\n                save_file = f\"{self.working_dir}/{source.data_connector}_{source.collection_name}.tif\"\n\n                da = dc.connector.get_data(  # type: ignore[attr-defined]\n                    data_collection_name=source.collection_name,\n                    date_start=closest_date,\n                    date_end=closest_date,\n                    bbox=list(l.geometry.bounds),\n                    bands=source.bands,\n                    save_file=save_file,\n                    maxcc=self.max_cloud_cover,\n                )\n\n                try:\n                    if self.transform.scale_data_xarray:\n                        dai = scale_data_xarray(da, list(np.ones(len(source.bands))))  # type: ignore[arg-type]\n                    if self.transform.impute_nans:\n                        dai = impute_nans_xarray(dai)\n                    \"\"\" &gt;&gt;&gt; INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \n                    if self.transform.&lt;new_transformation_func&gt;:\n                        dai = &lt;new_tranformation_fnc(da)&gt;\n                    \"\"\"\n                    save_data_array_to_file(dai, save_file, imputed=True)\n                except TerrakitBaseException as e:\n                    raise TerrakitBaseException(\n                        f\"Error while transforming data... {e}\"\n                    ) from e\n\n                for t in da.time.values:  # type: ignore[union-attr]\n                    date = t.astype(str)[:10]\n\n                for i, t in enumerate(da.time.values):  # type: ignore[union-attr]\n                    date = t.astype(str)[:10]\n                    queried_data.append(\n                        save_file.replace(\".tif\", f\"_{date}_imputed.tif\")\n                    )\n\n                if self.keep_files is False:\n                    logger.info(f\"Deleting {save_file.replace('.tif', f'_{date}.tif')}\")\n                    os.remove(save_file.replace(\".tif\", f\"_{date}.tif\"))\n            logging.info(f\"Queried data: {queried_data}\")\n        return queried_data\n\n    def rasterize_vectors_to_the_queried_data(self, queried_data: list) -&gt; int:\n        \"\"\"\n        Rasterize vector data to the queried raster data.\n\n        Args:\n            queried_data (list): List of queried raster file paths.\n\n        Returns:\n            int: Number of files rasterized.\n        \"\"\"\n        labels_shp_file = self._find_shp_file(\n            shp_file_type=\"labels\", shp_file_path=self.labels_shp_file\n        )\n        label_gdf = self._read_shp_file(labels_shp_file)\n\n        logging.info(\"Rasterizing vectors to the queried data\")\n        file_save_count = 0\n        for q in queried_data:\n            with rasterio.open(q, \"r\") as src:\n                out_meta = src.meta\n                out_meta.update({\"count\": 1})\n                image = rasterio.features.rasterize(\n                    ((g, 1) for g in label_gdf.geometry),\n                    out_shape=src.shape,\n                    transform=src.transform,\n                )\n                # Write the burned image to geotiff\n                logging.info(f\"Writing to {q.replace('.tif', '')}_labels.tif\")\n                with rasterio.open(\n                    f\"{q.replace('.tif', '')}_labels.tif\", \"w\", **out_meta\n                ) as dst:\n                    dst.write(image, indexes=1)\n                    file_save_count = +1\n        return file_save_count\n</code></pre>"},{"location":"api/download/#terrakit.download.download_data.DownloadCls.find_and_query_data_for_matching_dates","title":"<code>find_and_query_data_for_matching_dates</code>","text":"<p>Find and query data for matching dates from the specified data sources.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of queried data file paths.</p> Source code in <code>terrakit/download/download_data.py</code> <pre><code>def find_and_query_data_for_matching_dates(\n    self,\n) -&gt; list:\n    \"\"\"\n    Find and query data for matching dates from the specified data sources.\n\n    Returns:\n        list: List of queried data file paths.\n    \"\"\"\n    bbox_shp_file = self._find_shp_file(\n        shp_file_type=\"bbox\", shp_file_path=self.datetime_bbox_shp_file\n    )\n    grouped_bbox_gdf = self._read_shp_file(bbox_shp_file)\n\n    queried_data = []\n    for li in range(0, len(grouped_bbox_gdf)):\n        l = grouped_bbox_gdf.loc[li]  # noqa\n\n        from_date = (\n            datetime.strptime(l.datetime, \"%Y-%m-%d\")\n            - timedelta(days=self.date_allowance.pre_days)\n        ).strftime(\"%Y-%m-%d\")\n        to_date = (\n            datetime.strptime(l.datetime, \"%Y-%m-%d\")\n            + timedelta(days=self.date_allowance.post_days)\n        ).strftime(\"%Y-%m-%d\")\n\n        for source in self.data_sources:\n            dc = DataConnector(connector_type=source.data_connector)\n\n            logger.info(source.collection_name)\n            logger.info(from_date)\n            logger.info(to_date)\n            logger.info(list(l.geometry.bounds))\n            unique_dates, results = dc.connector.find_data(  # type: ignore[attr-defined]\n                data_collection_name=source.collection_name,\n                date_start=from_date,\n                date_end=to_date,\n                bbox=list(l.geometry.bounds),\n                bands=source.bands,\n            )\n\n            if len(unique_dates) == 0:  # type: ignore[arg-type]\n                logger.warning(\n                    f\"No data found for given request: {source}, {from_date}, {to_date}, {list(l.geometry.bounds)}.\"\n                )\n                return []\n\n            logger.info(unique_dates)\n\n            # Now find the closest date from the search\n            time_diffs_abs = [\n                abs(\n                    datetime.strptime(X, \"%Y-%m-%d\")\n                    - datetime.strptime(l.datetime, \"%Y-%m-%d\")\n                )\n                for X in unique_dates  # type: ignore[union-attr]\n            ]\n            closest_index = time_diffs_abs.index(min(time_diffs_abs))\n\n            closest_date = unique_dates[closest_index]  # type: ignore[index]\n\n            save_file = f\"{self.working_dir}/{source.data_connector}_{source.collection_name}.tif\"\n\n            da = dc.connector.get_data(  # type: ignore[attr-defined]\n                data_collection_name=source.collection_name,\n                date_start=closest_date,\n                date_end=closest_date,\n                bbox=list(l.geometry.bounds),\n                bands=source.bands,\n                save_file=save_file,\n                maxcc=self.max_cloud_cover,\n            )\n\n            try:\n                if self.transform.scale_data_xarray:\n                    dai = scale_data_xarray(da, list(np.ones(len(source.bands))))  # type: ignore[arg-type]\n                if self.transform.impute_nans:\n                    dai = impute_nans_xarray(dai)\n                \"\"\" &gt;&gt;&gt; INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \n                if self.transform.&lt;new_transformation_func&gt;:\n                    dai = &lt;new_tranformation_fnc(da)&gt;\n                \"\"\"\n                save_data_array_to_file(dai, save_file, imputed=True)\n            except TerrakitBaseException as e:\n                raise TerrakitBaseException(\n                    f\"Error while transforming data... {e}\"\n                ) from e\n\n            for t in da.time.values:  # type: ignore[union-attr]\n                date = t.astype(str)[:10]\n\n            for i, t in enumerate(da.time.values):  # type: ignore[union-attr]\n                date = t.astype(str)[:10]\n                queried_data.append(\n                    save_file.replace(\".tif\", f\"_{date}_imputed.tif\")\n                )\n\n            if self.keep_files is False:\n                logger.info(f\"Deleting {save_file.replace('.tif', f'_{date}.tif')}\")\n                os.remove(save_file.replace(\".tif\", f\"_{date}.tif\"))\n        logging.info(f\"Queried data: {queried_data}\")\n    return queried_data\n</code></pre>"},{"location":"api/download/#terrakit.download.download_data.DownloadCls.rasterize_vectors_to_the_queried_data","title":"<code>rasterize_vectors_to_the_queried_data</code>","text":"<p>Rasterize vector data to the queried raster data.</p> <p>Parameters:</p> Name Type Description Default <code>queried_data</code> <code>list</code> <p>List of queried raster file paths.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of files rasterized.</p> Source code in <code>terrakit/download/download_data.py</code> <pre><code>def rasterize_vectors_to_the_queried_data(self, queried_data: list) -&gt; int:\n    \"\"\"\n    Rasterize vector data to the queried raster data.\n\n    Args:\n        queried_data (list): List of queried raster file paths.\n\n    Returns:\n        int: Number of files rasterized.\n    \"\"\"\n    labels_shp_file = self._find_shp_file(\n        shp_file_type=\"labels\", shp_file_path=self.labels_shp_file\n    )\n    label_gdf = self._read_shp_file(labels_shp_file)\n\n    logging.info(\"Rasterizing vectors to the queried data\")\n    file_save_count = 0\n    for q in queried_data:\n        with rasterio.open(q, \"r\") as src:\n            out_meta = src.meta\n            out_meta.update({\"count\": 1})\n            image = rasterio.features.rasterize(\n                ((g, 1) for g in label_gdf.geometry),\n                out_shape=src.shape,\n                transform=src.transform,\n            )\n            # Write the burned image to geotiff\n            logging.info(f\"Writing to {q.replace('.tif', '')}_labels.tif\")\n            with rasterio.open(\n                f\"{q.replace('.tif', '')}_labels.tif\", \"w\", **out_meta\n            ) as dst:\n                dst.write(image, indexes=1)\n                file_save_count = +1\n    return file_save_count\n</code></pre>"},{"location":"api/labels/","title":"Labels Documentation","text":"<p>Documentation for the <code>terrakit.transform.labels</code> module.</p>"},{"location":"api/labels/#terrakit.transform.labels.process_labels","title":"<code>process_labels</code>","text":"<p>Entry point function for processing labels.</p> <p>The function validates and initalizes the LabelsCls, then loads, groups, and saves bounding boxes for labels and tiles.</p> <p>Parameters:</p> Name Type Description Default <code>labels_folder</code> <code>str</code> <p>Path to the folder containing label files.</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset. Defaults to \"terrakit_curated_dataset\".</p> <code>'terrakit_curated_dataset'</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files. Defaults to \"./tmp\".</p> <code>'./tmp'</code> <code>active</code> <code>bool</code> <p>Indicates if labels are active. Defaults to True.</p> <code>True</code> <code>label_type</code> <code>str</code> <p>Type of labels, currently supports 'vector'. Defaults to \"vector\".</p> <code>'vector'</code> <code>datetime_info</code> <code>str</code> <p>Specifies how to extract datetime information, either 'filename' or 'csv'. Defaults to \"filename\".</p> <code>'filename'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: GeoDataFrame containing all successfully loaded and grouped label data.</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If there are issues with label data or validation.</p> Example <pre><code>import terrakit\n\nlabel_args = {\n    \"dataset_name\": \"test_dataset\",\n    \"labels\": {\n        \"labels_folder\": labels_folder,\n    },\n}\n\nlabels_gdf, grouped_bbox_gdf = terrakit.process_labels(\n    dataset_name=label_args[\"dataset_name\"],\n    labels_folder=label_args[\"labels\"][\"labels_folder\"],\n)\n</code></pre> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def process_labels(\n    labels_folder: str,\n    dataset_name: str = \"terrakit_curated_dataset\",\n    working_dir: str = \"./tmp\",\n    active: bool = True,\n    label_type: str = \"vector\",\n    datetime_info: str = \"filename\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Entry point function for processing labels.\n\n    The function validates and initalizes the LabelsCls, then loads, groups, and saves bounding boxes for labels and tiles.\n\n    Parameters:\n        labels_folder (str): Path to the folder containing label files.\n        dataset_name (str, optional): Name of the dataset. Defaults to \"terrakit_curated_dataset\".\n        working_dir (str, optional): Working directory for temporary files. Defaults to \"./tmp\".\n        active (bool, optional): Indicates if labels are active. Defaults to True.\n        label_type (str, optional): Type of labels, currently supports 'vector'. Defaults to \"vector\".\n        datetime_info (str, optional): Specifies how to extract datetime information, either 'filename' or 'csv'. Defaults to \"filename\".\n\n    Returns:\n        pd.DataFrame: GeoDataFrame containing all successfully loaded and grouped label data.\n\n    Raises:\n        TerrakitValidationError: If there are issues with label data or validation.\n\n    Example:\n        ```python\n        import terrakit\n\n        label_args = {\n            \"dataset_name\": \"test_dataset\",\n            \"labels\": {\n                \"labels_folder\": labels_folder,\n            },\n        }\n\n        labels_gdf, grouped_bbox_gdf = terrakit.process_labels(\n            dataset_name=label_args[\"dataset_name\"],\n            labels_folder=label_args[\"labels\"][\"labels_folder\"],\n        )\n        ```\n    \"\"\"\n    logging.info(f\"Processing labels with arguments: {locals()}\")\n    pipeline_model = pipeline_model_validation(\n        dataset_name=dataset_name, working_dir=working_dir\n    )\n    labels, labels_model = labels_model_validation(\n        pipeline_model=pipeline_model,\n        active=active,\n        labels_folder=labels_folder,\n        label_type=label_type,\n        datetime_info=datetime_info,\n    )\n\n    if not active:\n        logging.warning(\n            \"IMPORTANT: Labels are not active. Set labels.active = True to activate labels.\"\n        )\n        return pd.DataFrame(), pd.DataFrame()\n\n    if datetime_info == \"csv\":\n        metadata_csv = glob(f\"{labels_folder}/metadata.csv\")\n        # If using csv, check metadata.csv exists\n        if len(metadata_csv) == 0:\n            raise TerrakitValidationError(\n                \"No metadata.csv file provided in labels directory. Please include 'metadata.csv' in labels dir with headers 'filename,date' and date information for each labels file. Alternatively set 'datetime_info' to 'filename'.\"\n            )\n        # Check headers are as expected\n        excepted_headers = [\"filename\", \"date\"]\n        with open(f\"{labels_folder}/metadata.csv\", \"r\") as f:\n            reader = csv.reader(f)\n            headers = next(reader)\n            if headers != excepted_headers:\n                raise TerrakitValidationError(\n                    \"metadata.csv missing 'filename,date' headers. Please update 'metadata.csv' to include these headers.\"\n                )\n    try:\n        labels_gdf = labels.load_label_files(label_type)  # type: ignore [arg-type]\n    except TerrakitValidationError as e:\n        raise e\n    except TerrakitBaseException as e:\n        raise e\n    except Exception as e:\n        raise e\n\n    try:\n        grouped_boxes_gdf = labels.get_grouped_bbox_gdf(labels_gdf)\n    except TerrakitValidationError as e:\n        raise e\n    except TerrakitBaseException as e:\n        raise e\n    except Exception as e:\n        raise e\n\n    # Save dataset metadata to file\n    if dataset_name == \"\":\n        output_files = [\"all_bboxes.shp\", \"labels.shp\"]\n    else:\n        output_files = [f\"{dataset_name}_all_bboxes.shp\", f\"{dataset_name}_labels.shp\"]\n    labels_metadata = {\n        \"step_id\": \"labels\",\n        \"activity\": \"Process label files to bound box and label shp files\",\n        \"method\": \"terrakit.transform.labels.process_labels\",\n        \"working_dir\": str(working_dir),\n        \"parameters\": json.loads(labels_model.model_dump_json()),\n        \"input_files\": list(labels_gdf[\"filename\"].unique()),\n        \"output_label_dates\": list(labels_gdf[\"datetime\"].unique()),\n        \"output_files\": output_files,\n    }\n\n    dataset_metdata(pipeline_model, labels_metadata)\n\n    return labels_gdf, grouped_boxes_gdf\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.labels_model_validation","title":"<code>labels_model_validation</code>","text":"<p>Validate and initialize LabelsCls with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_model</code> <code>PipelineModel</code> <p>Pipeline model containing dataset_name and working_dir.</p> required <code>active</code> <code>bool</code> <p>Indicates if labels are active.</p> required <code>labels_folder</code> <code>str</code> <p>Path to the folder containing label files.</p> required <code>label_type</code> <code>str</code> <p>Type of labels, currently supports 'vector'.</p> required <code>datetime_info</code> <code>str</code> <p>Specifies how to extract datetime information, either 'filename' or 'csv'.</p> required <p>Returns:</p> Name Type Description <code>LabelsCls</code> <code>LabelsCls</code> <p>Initialized LabelsCls instance.</p> <code>LabelsModel</code> <code>LabelsModel</code> <p>Validated LabelsModel instance.</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If validation of labels parameters fails.</p> <p>Examples:</p> <pre><code>from terrakit.validate.labels_model import LabelsModel\nfrom terrakit.validate.pipeline_model import PipelineModel\n\npipeline_model = PipelineModel(dataset_name = \"terrakit_curated_dataset\", working_dir = \"./tmp\")\nlabels, labels_model = labels_model_validation(\n    pipeline_model = pipeline_model,\n    active = True,\n    labels_folder = \"./docs/examples/test_wildfire_vector\",\n    label_type = \"vector\",\n    datetime_info = \"filename\",\n)\n</code></pre> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def labels_model_validation(\n    pipeline_model: PipelineModel,\n    active: bool,\n    labels_folder: str,\n    label_type: str,\n    datetime_info: str,\n) -&gt; tuple[LabelsCls, LabelsModel]:\n    \"\"\"\n    Validate and initialize LabelsCls with provided parameters.\n\n    Parameters:\n        pipeline_model (PipelineModel): Pipeline model containing dataset_name and working_dir.\n        active (bool): Indicates if labels are active.\n        labels_folder (str): Path to the folder containing label files.\n        label_type (str): Type of labels, currently supports 'vector'.\n        datetime_info (str): Specifies how to extract datetime information, either 'filename' or 'csv'.\n\n    Returns:\n        LabelsCls: Initialized LabelsCls instance.\n        LabelsModel: Validated LabelsModel instance.\n\n    Raises:\n        TerrakitValidationError: If validation of labels parameters fails.\n\n    Examples:\n        ```python\n        from terrakit.validate.labels_model import LabelsModel\n        from terrakit.validate.pipeline_model import PipelineModel\n\n        pipeline_model = PipelineModel(dataset_name = \"terrakit_curated_dataset\", working_dir = \"./tmp\")\n        labels, labels_model = labels_model_validation(\n            pipeline_model = pipeline_model,\n            active = True,\n            labels_folder = \"./docs/examples/test_wildfire_vector\",\n            label_type = \"vector\",\n            datetime_info = \"filename\",\n        )\n        ```\n    \"\"\"\n    try:\n        labels = LabelsCls(\n            dataset_name=pipeline_model.dataset_name,\n            working_dir=pipeline_model.working_dir,  # type: ignore[arg-type]\n            active=active,\n            labels_folder=labels_folder,  # type: ignore[arg-type]\n            label_type=label_type,\n            datetime_info=datetime_info,\n        )\n        labels_model = LabelsModel.model_validate(labels)\n    except ValidationError as e:\n        for error in e.errors():\n            logging.error(\n                f\"Invalid label arguments: {error['msg']}. \\n\\t'{error['loc'][0]}' currently set to '{error['input']}. Please update to a valid entry.\"\n            )\n        raise TerrakitValidationError(\n            \"Invalid label arguments\", details=e.errors()\n        ) from e\n    logging.info(f\"Processing labels with arguments: {labels_model}\")\n    return labels, labels_model\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.LabelsCls","title":"<code>LabelsCls</code>","text":"<p>Class to handle loading and processing of label data.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> <code>working_dir</code> <code>str</code> <p>Working directory where tile and label shapefiles will be saved.</p> <code>active</code> <code>bool</code> <p>Indicates if labels are active.</p> <code>labels_folder</code> <code>str</code> <p>Path to the folder containing label files.</p> <code>label_type</code> <code>str</code> <p>Type of labels, currently supports 'vector' or 'raster'.</p> <code>datetime_info</code> <code>str</code> <p>Specifies how to extract datetime information, either 'filename' or 'csv'.</p> Example <p>To instantiate LabelsCls: <pre><code>from terrakit.transform.labels import LabelsCls\n\nlabels = LabelsCls(dataset_name=\"my_dataset\",\n    working_dir=\"./tmp\",\n    active=True,\n    labels_folder=\"./docs/examples/test_wildfire_vector\",\n    label_type=\"vector\",\n    datetime_info=\"filename\"\n)\n</code></pre></p> Source code in <code>terrakit/transform/labels.py</code> <pre><code>class LabelsCls:\n    \"\"\"\n    Class to handle loading and processing of label data.\n\n    Attributes:\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory where tile and label shapefiles will be saved.\n        active (bool): Indicates if labels are active.\n        labels_folder (str): Path to the folder containing label files.\n        label_type (str): Type of labels, currently supports 'vector' or 'raster'.\n        datetime_info (str): Specifies how to extract datetime information, either 'filename' or 'csv'.\n\n    Example:\n        To instantiate LabelsCls:\n        ```python\n        from terrakit.transform.labels import LabelsCls\n\n        labels = LabelsCls(dataset_name=\"my_dataset\",\n            working_dir=\"./tmp\",\n            active=True,\n            labels_folder=\"./docs/examples/test_wildfire_vector\",\n            label_type=\"vector\",\n            datetime_info=\"filename\"\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        labels_folder: str,\n        dataset_name: str = \"terrakit_curated_dataset\",\n        working_dir: str = \"./tmp\",\n        active: bool = True,\n        label_type: str = \"vector\",\n        datetime_info: str = \"filename\",\n    ):\n        \"\"\"\n        Initialize LabelsCls with specified parameters.\n\n        Parameters:\n            labels_folder (str): Path to the folder containing label files.\n            dataset_name (str, optional): Name of the dataset. Defaults to \"terrakit_curated_dataset\".\n            working_dir (str, optional): Working directory for temporary files. Defaults to \"./tmp\".\n            active (bool, optional): Indicates if labels are active. Defaults to True.\n            label_type (str, optional): Type of labels, currently supports 'vector'. Defaults to \"vector\".\n            datetime_info (str, optional): Specifies how to extract datetime information, either 'filename' or 'csv'. Defaults to \"filename\".\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.working_dir = working_dir\n        self.active = active\n        self.labels_folder = labels_folder\n        self.label_type = label_type\n        self.datetime_info = datetime_info\n\n    def save_shp_file(self, shp_file_name, gdf):\n        \"\"\"\n        Save a GeoDataFrame to a shapefile.\n\n        Parameters:\n            shp_file_name (str): Name of the shapefile to be saved.\n            gdf (geopandas.GeoDataFrame): GeoDataFrame to be saved.\n\n        Returns:\n            gdf (geopandas.GeoDataFrame): Saved GeoDataFrame.\n\n        Raises:\n            TerrakitBaseException: If there is an error saving the shapefile.\n        \"\"\"\n        if self.dataset_name == \"\":\n            shp_filename = shp_file_name\n        else:\n            shp_filename = f\"{self.dataset_name}_{shp_file_name}\"\n\n        save_file: str = f\"{self.working_dir}/{shp_filename}\"\n        if os.path.exists(save_file):\n            logging.warning(\n                f\"File '{save_file}' already exists and will not be overwritten.\"\n            )\n            return gdf\n\n        try:\n            logging.info(f\"Saving {save_file}\")\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                gdf.to_file(save_file)\n        except ValueError as e:\n            err_msg = f\"There was an issue saving to {save_file}.\"\n            logger.error(err_msg)\n            raise TerrakitBaseException(err_msg, e)  # type: ignore[arg-type]\n\n    def get_metadata_csv(self) -&gt; str:\n        \"\"\"\n        Get the path to the metadata CSV file for the labels folder.\n\n        Returns:\n            str: Full path string to the metadata CSV file found in the labels folder\n\n        Raises:\n            TerrakitBaseException: If more than one metadata.csv file is found.\n        \"\"\"\n        metadata_csv = glob(f\"{self.labels_folder}/metadata.csv\")\n        if len(metadata_csv) != 1:\n            raise TerrakitBaseException(\n                \"There should only be one 'metadata.csv' in the labels directory.\"\n            )\n        return metadata_csv[0]\n\n    def load_metadata_csv(self) -&gt; dict:\n        \"\"\"Load metadata from metadata.csv and return as a dictionary.\n\n        Returns:\n            dict: Dictionary containing filenames as keys and dates as values.\n\n        Raises:\n            TerrakitBaseException: If an exception is raised opening or reading the metadata CSV file.\n        \"\"\"\n        metadata_csv = self.get_metadata_csv()\n        metadata_dict = {}\n        with open(metadata_csv, mode=\"r\", newline=\"\") as metadata:\n            reader = csv.DictReader(metadata, fieldnames=[\"filename\", \"date\"])\n            for row in reader:\n                metadata_dict[row[\"filename\"]] = row[\"date\"]\n        return metadata_dict\n\n    def raster_to_gdf(\n        self,\n        tif_path,\n        band: int = 1,\n        label_value: int = 1,\n    ):\n        \"\"\"\n        Convert a raster file to a GeoDataFrame.\n\n        Parameters:\n            tif_path (str): Path to the raster file.\n            band (int, optional): Band number to read. Defaults to 1.\n            label_value (int, optional): Value in the raster to consider as labels. Defaults to 1.\n\n        Returns:\n            gpd.GeoDataFrame: GeoDataFrame containing the raster features with the specified label value.\n        \"\"\"\n        with rasterio.open(tif_path) as src:\n            area = src.read(band)\n            crs = src.crs\n\n            results = (\n                {\"value\": v, \"geometry\": shape(g)}\n                for g, v in shapes(area, transform=src.transform)\n            )\n\n            gdf = gpd.GeoDataFrame(list(results), crs=crs).to_crs(CRS)\n            gdf = gdf[gdf[\"value\"] == label_value].copy()\n\n        return gdf\n\n    def get_label_date(self, file_path, label_type, metadata_csv=None):\n        \"\"\"\n        Extract datetime information from a label file.\n\n        Parameters:\n            file_path (str): Path to the label file.\n            label_type (str): Type of label, either 'vector' or 'raster'.\n            metadata_csv (dict, optional): Dictionary containing metadata if datetime info is from a CSV. Defaults to None.\n\n        Returns:\n            tuple: A tuple containing a boolean indicating success and the extracted datetime string.\n\n        Raises:\n            TerrakitBaseException: If there is an error extracting datetime information.\n        \"\"\"\n        pattern = r\"\\d{4}-\\d{2}-\\d{2}\"  # YYYY-MM-DD\n        filename = Path(file_path).name\n        if self.datetime_info == \"filename\":\n            if label_type == \"vector\":\n                # Check filename for YYYY-MM-DD pattern\n                match = re.search(pattern, filename)\n                if match:\n                    # Extract the matched part\n                    label_date_string = match.group()\n                else:\n                    logger.error(\n                        f\"No datetime found in {file_path}. Please update the filename to include datetime information using the format YYYY-MM-DD.\"\n                    )\n                    return False, \"\"\n\n            elif label_type == \"raster\":\n                label_date_string = extract_date_from_filename(\n                    filename, prefer=\"first\"\n                ).strftime(\"%Y-%m-%d\")\n        elif self.datetime_info == \"csv\":\n            # Look up date for filename in metadata_csv\n            if filename not in metadata_csv:\n                logger.error(\n                    f\"No matching entry found in {metadata_csv} for {filename}\"\n                )\n                return False, \"\"\n            elif not re.search(pattern, metadata_csv[filename]):\n                logger.error(\n                    f\"Date string '{metadata_csv[filename]}' for '{filename}' does not match expected format: {pattern}. Please update the 'metadata.csv' to include datetime information using the format YYYY-MM-DD.\"\n                )\n                return False, \"\"\n            else:\n                label_date_string = metadata_csv[filename].strip()\n        else:\n            logger.error(\n                \"No datetime info specified. Please update 'datetime_info' to one of {'filename', 'csv'}.\"\n            )\n            return False, \"\"\n        return True, label_date_string\n\n    def load_label_files(self, label_type: Literal[\"vector\", \"raster\"]) -&gt; pd.DataFrame:\n        \"\"\"\n        Load label files from the labels folder and return as a GeoDataFrame.\n\n        Parameters:\n            label_type (Literal[\"vector\", \"raster\"]): Either load vector or raster label files\n\n        Returns:\n            pd.DataFrame: GeoDataFrame containing all successfully loaded GeoJSON files.\n\n        Raises:\n            TerrakitValueError: After processing each label file, if an error occurred during processing, then an exception is raised.\n        \"\"\"\n\n        # Initialize an empty list to store GeoDataFrames\n        gdf_list = []\n\n        # Prepare list of counting any files that have failed to be processed correctly.\n        if label_type == \"vector\":\n            file_list = glob(f\"{self.labels_folder}/*json\")\n        elif label_type == \"raster\":\n            file_list = glob(f\"{self.labels_folder}/*tif\")\n        file_count = len(file_list)\n        failed_files = []\n\n        if self.datetime_info == \"csv\":\n            # check for metadata.csv\n            metadata_csv = self.load_metadata_csv()\n\n        # Iterate over files in the specified folder\n        for file_path in file_list:\n            filename = Path(file_path).name\n            # Reset label_date_string for each file in the labels folder.\n            label_date_string = None\n\n            # Check get the datetime info.\n            try:\n                if self.datetime_info == \"csv\":\n                    success, label_date_string = self.get_label_date(\n                        file_path, label_type, metadata_csv\n                    )\n                else:\n                    success, label_date_string = self.get_label_date(\n                        file_path, label_type\n                    )\n                if success is False:\n                    gdf = None\n                    failed_files.append(file_path)\n            except Exception as e:\n                gdf = None\n                logger.error(\n                    f\"Error: An error occurred extract a data from '{file_path}'. Check this is a valid geojson or raster file: {e}\"\n                )\n                failed_files.append(file_path)\n\n            # read the label file\n            try:\n                if label_type == \"vector\":\n                    gdf = gpd.read_file(file_path)\n                elif label_type == \"raster\":\n                    gdf = self.raster_to_gdf(file_path)\n            except pyogrio.errors.DataSourceError:\n                gdf = None\n                logger.error(\n                    f\"Error: An error occurred while reading '{file_path}'. Check this is a valid geojson or raster file.\"\n                )\n                failed_files.append(file_path)\n\n            # Append the datetime to the GeoDataFrame\n            if gdf is not None:\n                if \"geometry\" not in gdf:\n                    gdf = None\n                    logger.error(\n                        \"No 'geometry' field found in {file_path}. Please update to validate geojson.\"\n                    )\n                    failed_files.append(file_path)\n                elif label_date_string:\n                    logging.info(\n                        f\"Setting datetime to {label_date_string} for {filename}.\"\n                    )\n                    gdf[\"datetime\"] = label_date_string\n                    gdf[\"filename\"] = filename\n                    gdf_list.append(gdf)\n                    logger.info(f\"Successfully processed {file_path}\")\n\n        logger.info(\n            f\"{len(gdf_list)}/{file_count} label files were successfully processed.\"\n        )\n        # Concatenate all GeoDataFrames into one\n        if len(gdf_list) &gt; 0:\n            final_gdf = pd.concat(gdf_list, ignore_index=True)\n            if len(failed_files) &gt; 0:\n                logger.warning(\n                    f\"There was an issue processing the following label files: {failed_files}. Please check the logs for issues raised.\"\n                )\n        else:\n            err_msg = \"Warning: There was an issue loading labels. Please check label data and retry.\"\n            logging.warning(err_msg)\n            raise TerrakitValueError(err_msg)\n        self.save_shp_file(\"labels.shp\", final_gdf)\n        return final_gdf\n\n    def get_grouped_bbox_gdf(self, label_gdf: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Group bounding boxes by date and return a GeoDataFrame.\n\n        Parameters:\n            label_gdf (pd.DataFrame): GeoDataFrame containing label data.\n\n        Returns:\n            pd.DataFrame: GeoDataFrame with grouped bounding boxes.\n        \"\"\"\n        logging.info(\n            f\"Getting grouped bounding boxes for {self.dataset_name} at {self.working_dir}\"\n        )\n        label_bbox_grouped_bbox_list: list[pd.DataFrame] = []\n\n        label_bbox_gdf = copy.deepcopy(label_gdf)\n\n        label_bbox_gdf[\"geometry\"] = label_gdf.geometry.apply(\n            lambda x: box(*x.bounds)\n        ).tolist()\n\n        for d in list(label_bbox_gdf.datetime.unique()):\n            label_bbox_date_gdf = label_bbox_gdf[label_bbox_gdf.datetime == d]\n\n            # Find intersecting bounding boxes and merge, then repeat\n            intersects = label_bbox_date_gdf.sjoin(\n                label_bbox_date_gdf, how=\"left\", predicate=\"intersects\"\n            )\n            label_bbox_date_grouped_gdf = intersects.dissolve(aggfunc=\"min\")\n\n            label_bbox_date_grouped_gdf.drop([\"index_right\"], axis=1, inplace=True)\n            intersects = label_bbox_date_grouped_gdf.sjoin(\n                label_bbox_date_grouped_gdf, how=\"left\", predicate=\"intersects\"\n            )\n            label_bbox_date_grouped_gdf = intersects.dissolve(aggfunc=\"min\")\n\n            # Calculate the bounding box from the combined area\n            label_bbox_date_grouped_bbox_gdf = copy.deepcopy(\n                label_bbox_date_grouped_gdf\n            )\n            label_bbox_date_grouped_bbox_gdf[\"geometry\"] = (\n                label_bbox_date_grouped_bbox_gdf.geometry.apply(\n                    lambda x: box(*x.bounds)\n                ).tolist()\n            )\n            label_bbox_date_grouped_bbox_gdf[\"datetime\"] = (\n                label_bbox_date_grouped_bbox_gdf[\"datetime_left_left\"]\n            )\n\n            label_bbox_grouped_bbox_list = label_bbox_grouped_bbox_list + [\n                label_bbox_date_grouped_bbox_gdf\n            ]\n\n        label_bbox_grouped_bbox_gdf = pd.concat(\n            label_bbox_grouped_bbox_list, ignore_index=True\n        )\n\n        self.save_shp_file(\"all_bboxes.shp\", label_bbox_grouped_bbox_gdf)\n        return label_bbox_grouped_bbox_gdf\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.LabelsCls.save_shp_file","title":"<code>save_shp_file(shp_file_name, gdf)</code>","text":"<p>Save a GeoDataFrame to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>shp_file_name</code> <code>str</code> <p>Name of the shapefile to be saved.</p> required <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame to be saved.</p> required <p>Returns:</p> Name Type Description <code>gdf</code> <code>GeoDataFrame</code> <p>Saved GeoDataFrame.</p> <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If there is an error saving the shapefile.</p> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def save_shp_file(self, shp_file_name, gdf):\n    \"\"\"\n    Save a GeoDataFrame to a shapefile.\n\n    Parameters:\n        shp_file_name (str): Name of the shapefile to be saved.\n        gdf (geopandas.GeoDataFrame): GeoDataFrame to be saved.\n\n    Returns:\n        gdf (geopandas.GeoDataFrame): Saved GeoDataFrame.\n\n    Raises:\n        TerrakitBaseException: If there is an error saving the shapefile.\n    \"\"\"\n    if self.dataset_name == \"\":\n        shp_filename = shp_file_name\n    else:\n        shp_filename = f\"{self.dataset_name}_{shp_file_name}\"\n\n    save_file: str = f\"{self.working_dir}/{shp_filename}\"\n    if os.path.exists(save_file):\n        logging.warning(\n            f\"File '{save_file}' already exists and will not be overwritten.\"\n        )\n        return gdf\n\n    try:\n        logging.info(f\"Saving {save_file}\")\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            gdf.to_file(save_file)\n    except ValueError as e:\n        err_msg = f\"There was an issue saving to {save_file}.\"\n        logger.error(err_msg)\n        raise TerrakitBaseException(err_msg, e)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.LabelsCls.get_metadata_csv","title":"<code>get_metadata_csv() -&gt; str</code>","text":"<p>Get the path to the metadata CSV file for the labels folder.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full path string to the metadata CSV file found in the labels folder</p> <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If more than one metadata.csv file is found.</p> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def get_metadata_csv(self) -&gt; str:\n    \"\"\"\n    Get the path to the metadata CSV file for the labels folder.\n\n    Returns:\n        str: Full path string to the metadata CSV file found in the labels folder\n\n    Raises:\n        TerrakitBaseException: If more than one metadata.csv file is found.\n    \"\"\"\n    metadata_csv = glob(f\"{self.labels_folder}/metadata.csv\")\n    if len(metadata_csv) != 1:\n        raise TerrakitBaseException(\n            \"There should only be one 'metadata.csv' in the labels directory.\"\n        )\n    return metadata_csv[0]\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.LabelsCls.load_metadata_csv","title":"<code>load_metadata_csv() -&gt; dict</code>","text":"<p>Load metadata from metadata.csv and return as a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing filenames as keys and dates as values.</p> <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If an exception is raised opening or reading the metadata CSV file.</p> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def load_metadata_csv(self) -&gt; dict:\n    \"\"\"Load metadata from metadata.csv and return as a dictionary.\n\n    Returns:\n        dict: Dictionary containing filenames as keys and dates as values.\n\n    Raises:\n        TerrakitBaseException: If an exception is raised opening or reading the metadata CSV file.\n    \"\"\"\n    metadata_csv = self.get_metadata_csv()\n    metadata_dict = {}\n    with open(metadata_csv, mode=\"r\", newline=\"\") as metadata:\n        reader = csv.DictReader(metadata, fieldnames=[\"filename\", \"date\"])\n        for row in reader:\n            metadata_dict[row[\"filename\"]] = row[\"date\"]\n    return metadata_dict\n</code></pre>"},{"location":"api/labels/#terrakit.transform.labels.LabelsCls.get_grouped_bbox_gdf","title":"<code>get_grouped_bbox_gdf(label_gdf: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Group bounding boxes by date and return a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>label_gdf</code> <code>DataFrame</code> <p>GeoDataFrame containing label data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: GeoDataFrame with grouped bounding boxes.</p> Source code in <code>terrakit/transform/labels.py</code> <pre><code>def get_grouped_bbox_gdf(self, label_gdf: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Group bounding boxes by date and return a GeoDataFrame.\n\n    Parameters:\n        label_gdf (pd.DataFrame): GeoDataFrame containing label data.\n\n    Returns:\n        pd.DataFrame: GeoDataFrame with grouped bounding boxes.\n    \"\"\"\n    logging.info(\n        f\"Getting grouped bounding boxes for {self.dataset_name} at {self.working_dir}\"\n    )\n    label_bbox_grouped_bbox_list: list[pd.DataFrame] = []\n\n    label_bbox_gdf = copy.deepcopy(label_gdf)\n\n    label_bbox_gdf[\"geometry\"] = label_gdf.geometry.apply(\n        lambda x: box(*x.bounds)\n    ).tolist()\n\n    for d in list(label_bbox_gdf.datetime.unique()):\n        label_bbox_date_gdf = label_bbox_gdf[label_bbox_gdf.datetime == d]\n\n        # Find intersecting bounding boxes and merge, then repeat\n        intersects = label_bbox_date_gdf.sjoin(\n            label_bbox_date_gdf, how=\"left\", predicate=\"intersects\"\n        )\n        label_bbox_date_grouped_gdf = intersects.dissolve(aggfunc=\"min\")\n\n        label_bbox_date_grouped_gdf.drop([\"index_right\"], axis=1, inplace=True)\n        intersects = label_bbox_date_grouped_gdf.sjoin(\n            label_bbox_date_grouped_gdf, how=\"left\", predicate=\"intersects\"\n        )\n        label_bbox_date_grouped_gdf = intersects.dissolve(aggfunc=\"min\")\n\n        # Calculate the bounding box from the combined area\n        label_bbox_date_grouped_bbox_gdf = copy.deepcopy(\n            label_bbox_date_grouped_gdf\n        )\n        label_bbox_date_grouped_bbox_gdf[\"geometry\"] = (\n            label_bbox_date_grouped_bbox_gdf.geometry.apply(\n                lambda x: box(*x.bounds)\n            ).tolist()\n        )\n        label_bbox_date_grouped_bbox_gdf[\"datetime\"] = (\n            label_bbox_date_grouped_bbox_gdf[\"datetime_left_left\"]\n        )\n\n        label_bbox_grouped_bbox_list = label_bbox_grouped_bbox_list + [\n            label_bbox_date_grouped_bbox_gdf\n        ]\n\n    label_bbox_grouped_bbox_gdf = pd.concat(\n        label_bbox_grouped_bbox_list, ignore_index=True\n    )\n\n    self.save_shp_file(\"all_bboxes.shp\", label_bbox_grouped_bbox_gdf)\n    return label_bbox_grouped_bbox_gdf\n</code></pre>"},{"location":"api/store/","title":"Store Documentation","text":"<p>Documentation for the <code>terrakit.store</code> modules.</p> <p>More info coming soon.</p>"},{"location":"api/terrakit/","title":"TerraKit API Documentation","text":"<p>TerraKit makes creating ML-ready EO datasets easy. Quickly finding, retrieving and processing geospatial information from a range of data connectors. To get started install terrakit:</p> <pre><code>uv add terrakit\n</code></pre> <p>Specify a directory containing a set of labels. Then run the TerraKit Pipeline steps to find, download, chip and store a set of labels and data from <code>sentinel AWS</code>.</p> <pre><code>import terrakit\n\nterrakit.process_labels(labels_folder=\"./docs/examples/test_wildfire_vector/\")\nterrakit.download_data()\nterrakit.chip_and_label_data()\nterrakit.taco_store_data()\n</code></pre> <p>To find out more, check out the Examples tab, or explore the API docs.</p>"},{"location":"api/validators/","title":"Data Connector Validation Documentation","text":"<p>Documentation for the <code>terrakit.validate</code> modules.</p>"},{"location":"api/validators/#terrakit.validate.pipeline_model","title":"<code>pipeline_model</code>","text":""},{"location":"api/validators/#terrakit.validate.pipeline_model.PipelineModel","title":"<code>PipelineModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for configuring the TerraKit Pipeline. This class defines the attributes common across all pipeline steps.</p> <p>Attributes:</p> Name Type Description <code>dataset_name</code> <code>str</code> <p>Name of the dataset. Default is \"terrakit_curated_dataset\".</p> <code>working_dir</code> <code>Path</code> <p>Working directory for the pipeline. Default is \"./tmp\". The directory is created if it does not already exist.</p> Source code in <code>terrakit/validate/pipeline_model.py</code> <pre><code>class PipelineModel(BaseModel):\n    \"\"\"\n    A model for configuring the TerraKit Pipeline. This class defines the attributes common\n    across all pipeline steps.\n\n    Attributes:\n        dataset_name (str): Name of the dataset. Default is \"terrakit_curated_dataset\".\n        working_dir (Path): Working directory for the pipeline. Default is \"./tmp\". The directory is created if it does not already exist.\n    \"\"\"\n\n    dataset_name: str = \"terrakit_curated_dataset\"\n    working_dir: Path = Path(\"./tmp\")\n\n    @field_validator(\"dataset_name\", mode=\"before\")\n    def check_dataset_name(cls, v):\n        \"\"\"\n        Validate that the dataset_name does not contain special characters.\n\n        Args:\n            v (str): The dataset name to validate.\n\n        Returns:\n            str: The validated dataset name.\n        \"\"\"\n\n        return v\n\n    @field_validator(\"working_dir\", mode=\"before\")\n    def check_working_dir(cls, v) -&gt; Path:\n        \"\"\"\n        Validate and create the working directory if it does not exist.\n\n        Args:\n            v (Path): The working directory path.\n\n        Returns:\n            Path: The validated and existing working directory path.\n\n        Raises:\n            ValueError: If the provided path is not a directory.\n        \"\"\"\n        if v is None:\n            v = \"./tmp\"\n        pathname = Path(v)\n        logging.debug(f\"Working directory set to: {pathname}\")\n        if not pathname.exists():\n            logging.info(f\"Creating working directory: {pathname}\")\n            pathname.mkdir(parents=True, exist_ok=True)\n        elif os.path.isdir(pathname) is False:\n            raise ValueError(\n                f\"Working directory must be a path, not a file: 'working_dir' set to {v}\"\n            )\n        return pathname\n</code></pre>"},{"location":"api/validators/#terrakit.validate.pipeline_model.PipelineModel.check_dataset_name","title":"<code>check_dataset_name</code>","text":"<p>Validate that the dataset_name does not contain special characters.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>str</code> <p>The dataset name to validate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The validated dataset name.</p> Source code in <code>terrakit/validate/pipeline_model.py</code> <pre><code>@field_validator(\"dataset_name\", mode=\"before\")\ndef check_dataset_name(cls, v):\n    \"\"\"\n    Validate that the dataset_name does not contain special characters.\n\n    Args:\n        v (str): The dataset name to validate.\n\n    Returns:\n        str: The validated dataset name.\n    \"\"\"\n\n    return v\n</code></pre>"},{"location":"api/validators/#terrakit.validate.pipeline_model.PipelineModel.check_working_dir","title":"<code>check_working_dir</code>","text":"<p>Validate and create the working directory if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Path</code> <p>The working directory path.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The validated and existing working directory path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided path is not a directory.</p> Source code in <code>terrakit/validate/pipeline_model.py</code> <pre><code>@field_validator(\"working_dir\", mode=\"before\")\ndef check_working_dir(cls, v) -&gt; Path:\n    \"\"\"\n    Validate and create the working directory if it does not exist.\n\n    Args:\n        v (Path): The working directory path.\n\n    Returns:\n        Path: The validated and existing working directory path.\n\n    Raises:\n        ValueError: If the provided path is not a directory.\n    \"\"\"\n    if v is None:\n        v = \"./tmp\"\n    pathname = Path(v)\n    logging.debug(f\"Working directory set to: {pathname}\")\n    if not pathname.exists():\n        logging.info(f\"Creating working directory: {pathname}\")\n        pathname.mkdir(parents=True, exist_ok=True)\n    elif os.path.isdir(pathname) is False:\n        raise ValueError(\n            f\"Working directory must be a path, not a file: 'working_dir' set to {v}\"\n        )\n    return pathname\n</code></pre>"},{"location":"api/validators/#terrakit.validate.pipeline_model.pipeline_model_validation","title":"<code>pipeline_model_validation</code>","text":"<p>Validate the TerraKit Pipeline model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>working_dir</code> <code>str</code> <p>Working directory for the pipeline.</p> required <p>Returns:</p> Name Type Description <code>PipelineModel</code> <p>The validated PipelineModel instance.</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If the provided arguments are invalid.</p> Source code in <code>terrakit/validate/pipeline_model.py</code> <pre><code>def pipeline_model_validation(dataset_name: str, working_dir: str):\n    \"\"\"\n    Validate the TerraKit Pipeline model configuration.\n\n    Args:\n        dataset_name (str): Name of the dataset.\n        working_dir (str): Working directory for the pipeline.\n\n    Returns:\n        PipelineModel: The validated PipelineModel instance.\n\n    Raises:\n        TerrakitValidationError: If the provided arguments are invalid.\n    \"\"\"\n    try:\n        parent_params = PipelineModel(\n            dataset_name=dataset_name, working_dir=working_dir\n        )\n        pipeline_model = PipelineModel.model_validate(parent_params)\n    except ValidationError as e:\n        for error in e.errors():\n            logging.error(\n                f\"Invalid parent arguments: {error['msg']}. \\n\\t'{error['loc'][0]}' currently set to '{error['input']}'. Please update to a valid entry.\"\n            )\n        raise TerrakitValidationError(\n            \"Invalid parent arguments\", details=e.errors()\n        ) from e\n    logging.info(f\"Processing with parent arguments: {pipeline_model}\")\n    return pipeline_model\n</code></pre>"},{"location":"api/validators/#terrakit.validate.labels_model","title":"<code>labels_model</code>","text":""},{"location":"api/validators/#terrakit.validate.labels_model.LabelsModel","title":"<code>LabelsModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for configuration of the process labels TerraKit pipeline step.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>ConfigDict</code> <p>Configuration dictionary for the model.</p> <code>labels_folder</code> <code>Path</code> <p>Path to the folder containing label files.</p> <code>active</code> <code>bool</code> <p>Indicates if the labels step is active. Default is True.</p> <code>label_type</code> <code>Literal['vector']</code> <p>Type of labels, currently only 'vector' is supported. Default is 'vector'.</p> <code>datetime_info</code> <code>Literal['filename', 'csv']</code> <p>Specifies how datetime information is stored, either by 'filename' or 'csv'. Default is 'filename'.</p> Source code in <code>terrakit/validate/labels_model.py</code> <pre><code>class LabelsModel(BaseModel):\n    \"\"\"\n    Model for configuration of the process labels TerraKit pipeline step.\n\n    Attributes:\n        model_config (ConfigDict): Configuration dictionary for the model.\n        labels_folder (Path): Path to the folder containing label files.\n        active (bool): Indicates if the labels step is active. Default is True.\n        label_type (Literal[\"vector\"]): Type of labels, currently only 'vector' is supported. Default is 'vector'.\n        datetime_info (Literal[\"filename\", \"csv\"]): Specifies how datetime information is stored, either by 'filename' or 'csv'. Default is 'filename'.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    labels_folder: Path\n    active: bool = True\n    label_type: Literal[\"vector\", \"raster\"] = \"vector\"\n    datetime_info: Literal[\"filename\", \"csv\"] = \"filename\"\n\n    @field_validator(\"labels_folder\", mode=\"after\")\n    def check_labels_folder(cls, v):\n        \"\"\"\n        Validates that the labels_folder exists, is not empty, and contains at least one supported file.\n\n        Raises:\n            ValueError: If the labels_folder does not exist, is empty, or does not contain any supported files.\n        \"\"\"\n        if os.path.exists(v) is False:\n            raise ValueError(\n                f\"Labels folder '{v}' does not exist. Please provide a valid labels folder\"\n            )\n\n        if os.listdir(v) == 0:\n            raise ValueError(\n                f\"Labels folder '{v}' does not contain any files. Please provide a valid labels folder with at least one labels file\"\n            )\n\n        return v\n\n    @field_validator(\"label_type\", mode=\"after\")\n    def check_labels_type(cls, v, info: ValidationInfo):\n        labels_folder = info.data.get(\"labels_folder\")\n\n        check_for_valid_type = False\n        valid_file_type = \"\"\n        if v == \"vector\":\n            valid_file_type = \"json\"\n            for filename in os.listdir(labels_folder):\n                if filename.endswith(valid_file_type):\n                    check_for_valid_type = True\n\n        if v == \"raster\":\n            valid_file_type = \"tif\"\n            for filename in os.listdir(labels_folder):\n                if filename.endswith(valid_file_type):\n                    check_for_valid_type = True\n\n        if check_for_valid_type is False:\n            raise ValueError(\n                f\"Labels folder '{labels_folder}' does not contain any supported files. Please provide a valid labels folder with at least one valid .{valid_file_type} file.\"\n            )\n        return v\n\n    @field_validator(\"datetime_info\", mode=\"before\")\n    def check_datetime_info(cls, v):\n        \"\"\"\n        Placeholder for future validation of datetime_info.\n\n        Currently, no specific checks are implemented for datetime_info.\n        \"\"\"\n\n        return v\n</code></pre>"},{"location":"api/validators/#terrakit.validate.labels_model.LabelsModel.check_labels_folder","title":"<code>check_labels_folder</code>","text":"<p>Validates that the labels_folder exists, is not empty, and contains at least one supported file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the labels_folder does not exist, is empty, or does not contain any supported files.</p> Source code in <code>terrakit/validate/labels_model.py</code> <pre><code>@field_validator(\"labels_folder\", mode=\"after\")\ndef check_labels_folder(cls, v):\n    \"\"\"\n    Validates that the labels_folder exists, is not empty, and contains at least one supported file.\n\n    Raises:\n        ValueError: If the labels_folder does not exist, is empty, or does not contain any supported files.\n    \"\"\"\n    if os.path.exists(v) is False:\n        raise ValueError(\n            f\"Labels folder '{v}' does not exist. Please provide a valid labels folder\"\n        )\n\n    if os.listdir(v) == 0:\n        raise ValueError(\n            f\"Labels folder '{v}' does not contain any files. Please provide a valid labels folder with at least one labels file\"\n        )\n\n    return v\n</code></pre>"},{"location":"api/validators/#terrakit.validate.labels_model.LabelsModel.check_datetime_info","title":"<code>check_datetime_info</code>","text":"<p>Placeholder for future validation of datetime_info.</p> <p>Currently, no specific checks are implemented for datetime_info.</p> Source code in <code>terrakit/validate/labels_model.py</code> <pre><code>@field_validator(\"datetime_info\", mode=\"before\")\ndef check_datetime_info(cls, v):\n    \"\"\"\n    Placeholder for future validation of datetime_info.\n\n    Currently, no specific checks are implemented for datetime_info.\n    \"\"\"\n\n    return v\n</code></pre>"},{"location":"api/validators/#terrakit.validate.download_model","title":"<code>terrakit.validate.download_model</code>","text":""},{"location":"api/validators/#terrakit.validate.download_model.DateAllowance","title":"<code>DateAllowance</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for specifying date allowance around the target date.</p> <p>Attributes:</p> Name Type Description <code>pre_days</code> <code>int</code> <p>Number of days before the target date to include. Default is 0.</p> <code>post_days</code> <code>int</code> <p>Number of days after the target date to include. Default is 7.</p> Example <pre><code>from terrakit.validate.download_model import DateAllowance\n\ndate_allowance = DateAllowance(\n    pre_days = 0, post_days = 21\n)\n</code></pre> Source code in <code>terrakit/validate/download_model.py</code> <pre><code>class DateAllowance(BaseModel):\n    \"\"\"\n    Model for specifying date allowance around the target date.\n\n    Attributes:\n        pre_days (int): Number of days before the target date to include. Default is 0.\n        post_days (int): Number of days after the target date to include. Default is 7.\n\n    Example:\n        ```python\n        from terrakit.validate.download_model import DateAllowance\n\n        date_allowance = DateAllowance(\n            pre_days = 0, post_days = 21\n        )\n        ```\n    \"\"\"\n\n    pre_days: int = 0\n    post_days: int = 7\n</code></pre>"},{"location":"api/validators/#terrakit.validate.download_model.Transform","title":"<code>Transform</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for specifying data transformation options.</p> <p>Attributes:</p> Name Type Description <code>scale_data_xarray</code> <code>bool</code> <p>Whether to scale the data using xarray. Default is True.</p> <code>impute_nans</code> <code>bool</code> <p>Whether to impute NaN values. Default is True.</p> <code>reproject</code> <code>bool</code> <p>Whether to reproject the data. Default is True.</p> Example <pre><code>from terrakit.validate.download_model import Transfrom\n\ntransform = Transform(\n    scale_data_xarray=True,\n    impute_nans=True,\n    reproject=True,\n)\n</code></pre> Source code in <code>terrakit/validate/download_model.py</code> <pre><code>class Transform(BaseModel):\n    \"\"\"\n    Model for specifying data transformation options.\n\n    Attributes:\n        scale_data_xarray (bool): Whether to scale the data using xarray. Default is True.\n        impute_nans (bool): Whether to impute NaN values. Default is True.\n        reproject (bool): Whether to reproject the data. Default is True.\n\n    Example:\n        ```python\n        from terrakit.validate.download_model import Transfrom\n\n        transform = Transform(\n            scale_data_xarray=True,\n            impute_nans=True,\n            reproject=True,\n        )\n        ```\n    \"\"\"\n\n    scale_data_xarray: bool = True\n    impute_nans: bool = True\n    reproject: bool = True\n    \"\"\" &gt;&gt;&gt; INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt; \n    &lt;new_transformation_option&gt;: bool = False\n    \"\"\"\n</code></pre>"},{"location":"api/validators/#terrakit.validate.download_model.Transform.reproject","title":"<code>reproject: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>INCLUDE NEW TRANSFORMATIONS HERE &lt;&lt;&lt;  : bool = False"},{"location":"api/validators/#terrakit.validate.download_model.DataSource","title":"<code>DataSource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for specifying data source configuration.</p> <p>Attributes:</p> Name Type Description <code>data_connector</code> <code>str</code> <p>The data connector to use. Default is \"sentinel_aws\".</p> <code>collection_name</code> <code>str</code> <p>The collection name to download. Default is \"sentinel-2-l2a\".</p> <code>bands</code> <code>list[str]</code> <p>The bands to download. Default is [\"blue\", \"green\", \"red\"].</p> <code>save_file</code> <code>str | None</code> <p>The file path to save the downloaded data. Default is None.</p> Example <pre><code>from terrakit.validate.download_model import DataSource\n\ndata_source = DataSource(\n    data_connector = \"sentinel_aws\",\n    collection_name = \"sentinel-2-l2a\",\n    bands = [\"blue\", \"green\", \"red\"],\n    save_file = \"\",\n)\n</code></pre> Source code in <code>terrakit/validate/download_model.py</code> <pre><code>class DataSource(BaseModel):\n    \"\"\"\n    Model for specifying data source configuration.\n\n    Attributes:\n        data_connector (str): The data connector to use. Default is \"sentinel_aws\".\n        collection_name (str): The collection name to download. Default is \"sentinel-2-l2a\".\n        bands (list[str]): The bands to download. Default is [\"blue\", \"green\", \"red\"].\n        save_file (str | None): The file path to save the downloaded data. Default is None.\n\n    Example:\n        ```python\n        from terrakit.validate.download_model import DataSource\n\n        data_source = DataSource(\n            data_connector = \"sentinel_aws\",\n            collection_name = \"sentinel-2-l2a\",\n            bands = [\"blue\", \"green\", \"red\"],\n            save_file = \"\",\n        )\n        ```\n    \"\"\"\n\n    data_connector: str = \"sentinel_aws\"\n    collection_name: str = \"sentinel-2-l2a\"\n    bands: list[str] = [\"blue\", \"green\", \"red\"]\n    save_file: str | None = None\n</code></pre>"},{"location":"api/validators/#terrakit.validate.download_model.DownloadModel","title":"<code>DownloadModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for configuring the download process.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <code>ConfigDict</code> <p>Configuration dictionary.</p> <code>transform</code> <code>Transform</code> <p>Transformation options.</p> <code>date_allowance</code> <code>DateAllowance</code> <p>Date allowance around the target date.</p> <code>active</code> <code>bool</code> <p>Whether the download step is active. Default is True.</p> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover allowed. Default is 80.</p> <code>keep_files</code> <code>bool</code> <p>Whether to keep redundent shapefiles. Default is False.</p> <code>datetime_bbox_shp_file</code> <code>str</code> <p>File path for datetime bounding box shapefile. Default is \"./terrakit_curated_dataset_all_bboxes.shp\".</p> <code>labels_shp_file</code> <code>str</code> <p>File path for labels shapefile. Default is \"./tmp/terrakit_curated_dataset_labels.shp\".</p> <code>data_sources</code> <code>list[DataSource]</code> <p>List of data sources to download. Default is an empty list.</p> Source code in <code>terrakit/validate/download_model.py</code> <pre><code>class DownloadModel(BaseModel):\n    \"\"\"\n    Model for configuring the download process.\n\n    Attributes:\n        model_config (ConfigDict): Configuration dictionary.\n        transform (Transform): Transformation options.\n        date_allowance (DateAllowance): Date allowance around the target date.\n        active (bool): Whether the download step is active. Default is True.\n        max_cloud_cover (int): Maximum cloud cover allowed. Default is 80.\n        keep_files (bool): Whether to keep redundent shapefiles. Default is False.\n        datetime_bbox_shp_file (str): File path for datetime bounding box shapefile. Default is \"./terrakit_curated_dataset_all_bboxes.shp\".\n        labels_shp_file (str): File path for labels shapefile. Default is \"./tmp/terrakit_curated_dataset_labels.shp\".\n        data_sources (list[DataSource]): List of data sources to download. Default is an empty list.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n\n    transform: Transform\n    date_allowance: DateAllowance\n    active: bool = True\n    max_cloud_cover: int = 80\n    keep_files: bool = False\n    datetime_bbox_shp_file: str = \"./tmp/terrakit_curated_dataset_all_bboxes.shp\"\n    labels_shp_file: str = \"./tmp/terrakit_curated_dataset_labels.shp\"\n    data_sources: list[DataSource] = Field(default_factory=list)\n</code></pre>"},{"location":"api/validators/#terrakit.validate.tiling_model","title":"<code>tiling_model</code>","text":""},{"location":"api/validators/#terrakit.validate.data_connector","title":"<code>terrakit.validate.data_connector</code>","text":""},{"location":"api/validators/#terrakit.validate.data_connector.ConnectorType","title":"<code>ConnectorType</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>connector_type</code> <code>Literal</code> <p>The type of connector to be use to download data.</p> Example <p><pre><code>terrakit.DataConnector(connector_type=\"nasa_earthdata\")\n</code></pre> or <pre><code>terrakit.DataConnector({\"connector_type\": \"nasa_earthdata\"})\n</code></pre></p> Source code in <code>terrakit/validate/data_connector.py</code> <pre><code>class ConnectorType(BaseModel):\n    \"\"\"\n    Attributes:\n        connector_type (Literal): The type of connector to be use to download data.\n\n    Example:\n        ```\n        terrakit.DataConnector(connector_type=\"nasa_earthdata\")\n        ```\n        or\n        ```\n        terrakit.DataConnector({\"connector_type\": \"nasa_earthdata\"})\n        ```\n    \"\"\"\n\n    connector_type: Literal[\n        \"nasa_earthdata\",\n        \"sentinelhub\",\n        \"sentinel_aws\",\n        \"IBMResearchSTAC\",\n        \"TheWeatherCompany\",\n    ]\n    \"\"\"The type of connector to be use to download data. nasa_earthdata, sentinelhub, sentinel_aws, IBMResearchSTAC or TheWeatherCompany\"\"\"\n</code></pre>"},{"location":"api/validators/#terrakit.validate.data_connector.ConnectorType.connector_type","title":"<code>connector_type: Literal['nasa_earthdata', 'sentinelhub', 'sentinel_aws', 'IBMResearchSTAC', 'TheWeatherCompany']</code>  <code>instance-attribute</code>","text":"<p>The type of connector to be use to download data. nasa_earthdata, sentinelhub, sentinel_aws, IBMResearchSTAC or TheWeatherCompany</p>"},{"location":"api/validators/#terrakit.validate.helpers","title":"<code>helpers</code>","text":""},{"location":"api/validators/#terrakit.validate.helpers.check_collection_exists","title":"<code>check_collection_exists</code>","text":"<p>Check if the provided data_collection_name exists in the collections list.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the collection to check.</p> required <code>collections</code> <code>list</code> <p>A list of available collections.</p> required <p>Raises:</p> Type Description <code>TerrakitValueError</code> <p>If the collection does not exist.</p> Source code in <code>terrakit/validate/helpers.py</code> <pre><code>def check_collection_exists(data_collection_name: str, collections: list):\n    \"\"\"\n    Check if the provided data_collection_name exists in the collections list.\n\n    Parameters:\n        data_collection_name (str): The name of the collection to check.\n        collections (list): A list of available collections.\n\n    Raises:\n        TerrakitValueError: If the collection does not exist.\n    \"\"\"\n    if data_collection_name not in collections:\n        error_msg = f\"Invalid collection '{data_collection_name}'. Please choose from one of the following collection {collections}\"\n        logger.error(error_msg)\n        raise TerrakitValueError(error_msg)\n</code></pre>"},{"location":"api/validators/#terrakit.validate.helpers.check_start_end_date","title":"<code>check_start_end_date</code>","text":"<p>Validate the start and end dates ensuring the end date is after the start date.</p> <p>Parameters:</p> Name Type Description Default <code>date_start</code> <code>str</code> <p>The start date in ISO format (YYYY-MM-DD).</p> required <code>date_end</code> <code>str</code> <p>The end date in ISO format (YYYY-MM-DD).</p> required <p>Raises:</p> Type Description <code>TerrakitValueError</code> <p>If the date range is invalid.</p> Source code in <code>terrakit/validate/helpers.py</code> <pre><code>def check_start_end_date(date_start: str, date_end: str) -&gt; None:\n    \"\"\"\n    Validate the start and end dates ensuring the end date is after the start date.\n\n    Parameters:\n        date_start (str): The start date in ISO format (YYYY-MM-DD).\n        date_end (str): The end date in ISO format (YYYY-MM-DD).\n\n    Raises:\n        TerrakitValueError: If the date range is invalid.\n    \"\"\"\n    check_datetime(start=True, date_str=date_start)\n    check_datetime(start=False, date_str=date_end)\n\n    start = date.fromisoformat(date_start)\n    end = date.fromisoformat(date_end)\n    delta = end - start\n    if delta.days &lt; 0:\n        err_msg = f\"Invalid date range: {date_start} to {date_end}. End date must be greater than start date.\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg)\n</code></pre>"},{"location":"api/validators/#terrakit.validate.helpers.check_datetime","title":"<code>check_datetime</code>","text":"<p>Validate a date string ensuring it's in ISO format and not in the future.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>bool</code> <p>True if validating the start date, False for end date.</p> required <code>date_str</code> <code>str</code> <p>The date string to validate.</p> required <p>Raises:</p> Type Description <code>TerrakitValueError</code> <p>If the date format is incorrect or the date is in the future.</p> Source code in <code>terrakit/validate/helpers.py</code> <pre><code>def check_datetime(start: bool, date_str: str) -&gt; None:\n    \"\"\"\n    Validate a date string ensuring it's in ISO format and not in the future.\n\n    Parameters:\n        start (bool): True if validating the start date, False for end date.\n        date_str (str): The date string to validate.\n\n    Raises:\n        TerrakitValueError: If the date format is incorrect or the date is in the future.\n    \"\"\"\n    if start:\n        start_or_end = \"start\"\n    else:\n        start_or_end = \"end\"\n    try:\n        query_date = date.fromisoformat(date_str)\n    except ValueError as e:\n        err_msg = f\"Invalid {start_or_end} date format: {date_str}. Please use ISO format (YYYY-MM-DD).\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg, e)  # type: ignore [arg-type]\n\n    if query_date &gt; datetime.date(datetime.now()):\n        err_msg = f\"Invalid {start_or_end} date: {date_str}. Date must be in the past.\"\n        logger.error(err_msg)\n        raise TerrakitValueError(\n            err_msg,\n        )\n    if query_date &lt; datetime.strptime(\"01/01/1950\", \"%d/%m/%Y\").date():\n        err_msg = (\n            f\"Invalid {start_or_end} date: {date_str}. Date must be after 01/01/1950.\"\n        )\n        logger.error(err_msg)\n        raise TerrakitValueError(\n            err_msg,\n        )\n</code></pre>"},{"location":"api/validators/#terrakit.validate.helpers.check_area_polygon","title":"<code>check_area_polygon</code>","text":"<p>For connector_types that do not yet support 'area_polygon', this function provides a check to use 'bbox' instead.</p> <p>Parameters:</p> Name Type Description Default <code>area_polygon</code> <p>The area polygon to check.</p> required <code>connector_type</code> <code>str</code> <p>The type of connector.</p> required <p>Raises:</p> Type Description <code>TerrakitValueError</code> <p>If 'area_polygon' is provided instead of 'bbox'.</p> Source code in <code>terrakit/validate/helpers.py</code> <pre><code>def check_area_polygon(area_polygon, connector_type: str) -&gt; None:\n    \"\"\"\n    For connector_types that do not yet support 'area_polygon', this function provides a check to use 'bbox' instead.\n\n    Parameters:\n        area_polygon: The area polygon to check.\n        connector_type (str): The type of connector.\n\n    Raises:\n        TerrakitValueError: If 'area_polygon' is provided instead of 'bbox'.\n    \"\"\"\n    if area_polygon is not None:\n        err_msg = f\"Error: Issue finding data from {connector_type}. Please use 'bbox' instead of 'area_polygon'\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg)\n</code></pre>"},{"location":"api/validators/#terrakit.validate.helpers.check_bbox","title":"<code>check_bbox</code>","text":"<p>Validate the bounding box ensuring it's a list of four floats and not a degenerate rectangle.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list</code> <p>The bounding box to check.</p> required <code>connector_type</code> <code>str</code> <p>The type of connector.</p> required <p>Raises:</p> Type Description <code>TerrakitValueError</code> <p>If the bounding box is invalid.</p> Source code in <code>terrakit/validate/helpers.py</code> <pre><code>def check_bbox(bbox: list, connector_type: str) -&gt; None:\n    \"\"\"\n    Validate the bounding box ensuring it's a list of four floats and not a degenerate rectangle.\n\n    Parameters:\n        bbox (list): The bounding box to check.\n        connector_type (str): The type of connector.\n\n    Raises:\n        TerrakitValueError: If the bounding box is invalid.\n    \"\"\"\n    if bbox is None:\n        error_msg = f\"Error: Issue finding data from {connector_type}. Please specify at least one of 'bbox' and 'area_polygon'\"\n        logger.error(error_msg)\n        raise TerrakitValueError(error_msg)\n    if isinstance(bbox, list) is False:\n        err_msg = f\"Error: Issue finding data from {connector_type} with bbox '{bbox}'. Please specify 'bbox' as a list of floats.\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg)\n    if len(bbox) != 4:\n        err_msg = f\"Error: Issue finding data from {connector_type} with bbox '{bbox}'. Please specify 'bbox' as a list of length 4.\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg)\n    for item in bbox:\n        try:\n            float(item)\n        except ValueError:\n            err_msg = f\"Error: Issue finding data from {connector_type} with bbox '{bbox}'. Please specify 'bbox' as a list of floats. The entry '{item}' is not a float.\"\n            logger.error(err_msg)\n            raise TerrakitValueError(err_msg)\n    if len(set(bbox)) == 1:\n        err_msg = f\"Error: Issue finding data from {connector_type} with bbox '{bbox}'. Cannot determine area from 'bbox'. Please specify a valid area.\"\n        logger.error(err_msg)\n        raise TerrakitValueError(err_msg)\n    west, south, east, north = bbox\n    if not (-180 &lt;= west &lt; east &lt;= 180 and -90 &lt;= south &lt; north &lt;= 90):\n        raise TerrakitValueError(\n            f\"Error: Issue finding data from {connector_type} with bbox '{bbox}'. Bbox is expected as 'west, south, east, north' or 'minx, miny, maxx, maxy' using EPSG: 4326 coordinate system.\"\n        )\n</code></pre>"},{"location":"api/data_connectors/data_connectors/","title":"Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.terrakit</code> module.</p>"},{"location":"api/data_connectors/data_connectors/#terrakit.terrakit","title":"<code>terrakit.terrakit</code>","text":""},{"location":"api/data_connectors/data_connectors/#terrakit.terrakit.DataConnector","title":"<code>DataConnector</code>","text":"<p>A class to manage data connectors.</p> <p>Attributes:</p> Name Type Description <code>connector</code> <code>DataConnector</code> <p>An instance of the connector class specified during initialization.</p> <code>connector_type</code> <code>str</code> <p>The type of data connector.</p> Example <pre><code># Example usage:\nfrom terrakit import DataConnector\n\ndc = DataConnector(connector_type=\"sentinel_aws\")\ndc.connector.list_collections()\n</code></pre> <p>or</p> <pre><code>dc = DataConnector(\"sentinel_aws\")\ndc.connector.list_collections()\n</code></pre> Source code in <code>terrakit/terrakit.py</code> <pre><code>class DataConnector:\n    \"\"\"\n    A class to manage data connectors.\n\n    Attributes:\n        connector (DataConnector): An instance of the connector class specified during initialization.\n        connector_type (str): The type of data connector.\n\n    Example:\n        ```python\n        # Example usage:\n        from terrakit import DataConnector\n\n        dc = DataConnector(connector_type=\"sentinel_aws\")\n        dc.connector.list_collections()\n        ```\n\n        or\n\n        ```python\n        dc = DataConnector(\"sentinel_aws\")\n        dc.connector.list_collections()\n        ```\n    \"\"\"\n\n    def __init__(self, connector_type: str):\n        \"\"\"\n        Initialize DataConnector with the specified connector type.\n\n        Parameters:\n            connector_type (str): The type of data connector to initialize.\n\n        \"\"\"\n        logger.info(f\"Initializing DataConnector with connector type: {connector_type}\")\n        try:\n            connector_type = ConnectorType(connector_type=connector_type)\n            self.connector = DataConnectorFactory.get_connector(\n                connector_type=connector_type\n            )\n        except ValidationError as e:\n            raise TerrakitValidationError(\n                message=f\"Invalid connector type: '{connector_type}'\"\n            ) from e\n        self.connector_type: str = connector_type\n</code></pre>"},{"location":"api/data_connectors/data_connectors/#terrakit.terrakit.DataConnectorFactory","title":"<code>DataConnectorFactory</code>","text":"<p>A factory class for creating data connector objects.</p> Source code in <code>terrakit/terrakit.py</code> <pre><code>class DataConnectorFactory:\n    \"\"\"\n    A factory class for creating data connector objects.\n    \"\"\"\n\n    @staticmethod\n    def get_connector(connector_type: ConnectorType) -&gt; Connector:\n        \"\"\"\n        Create and return a data connector object based on the specified connector type.\n\n        Parameters:\n            connector_type (str): The type of data connector to create. Supported types are:\n                - \"sentinelhub\"\n                - \"nasa_earthdata\"\n                - \"sentinel_aws\"\n\n        Returns:\n            object: An instance of the specified data connector class.\n\n        Raises:\n            ValueError: If an invalid connector type is provided.\n        \"\"\"\n        if connector_type.connector_type == \"sentinelhub\":\n            return SentinelHub()\n        elif connector_type.connector_type == \"nasa_earthdata\":\n            return NASA_EarthData()\n        elif connector_type.connector_type == \"sentinel_aws\":\n            return Sentinel_AWS()\n        elif connector_type.connector_type == \"IBMResearchSTAC\":\n            return IBMResearchSTAC()\n        elif connector_type.connector_type == \"TheWeatherCompany\":\n            return TheWeatherCompany()\n        # -----&gt; Include new connectors here &lt; ------\n        # elif connector_type == \"&lt;new_connector&gt;\"\n        #   return NewConnectorClass()\n        else:\n            raise TerrakitValidationError(\n                f\"Invalid connector type: '{connector_type.connector_type}'\"\n            )\n</code></pre>"},{"location":"api/data_connectors/data_connectors/#terrakit.terrakit.DataConnectorFactory.get_connector","title":"<code>get_connector(connector_type: ConnectorType) -&gt; Connector</code>  <code>staticmethod</code>","text":"<p>Create and return a data connector object based on the specified connector type.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>str</code> <p>The type of data connector to create. Supported types are: - \"sentinelhub\" - \"nasa_earthdata\" - \"sentinel_aws\"</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>Connector</code> <p>An instance of the specified data connector class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid connector type is provided.</p> Source code in <code>terrakit/terrakit.py</code> <pre><code>@staticmethod\ndef get_connector(connector_type: ConnectorType) -&gt; Connector:\n    \"\"\"\n    Create and return a data connector object based on the specified connector type.\n\n    Parameters:\n        connector_type (str): The type of data connector to create. Supported types are:\n            - \"sentinelhub\"\n            - \"nasa_earthdata\"\n            - \"sentinel_aws\"\n\n    Returns:\n        object: An instance of the specified data connector class.\n\n    Raises:\n        ValueError: If an invalid connector type is provided.\n    \"\"\"\n    if connector_type.connector_type == \"sentinelhub\":\n        return SentinelHub()\n    elif connector_type.connector_type == \"nasa_earthdata\":\n        return NASA_EarthData()\n    elif connector_type.connector_type == \"sentinel_aws\":\n        return Sentinel_AWS()\n    elif connector_type.connector_type == \"IBMResearchSTAC\":\n        return IBMResearchSTAC()\n    elif connector_type.connector_type == \"TheWeatherCompany\":\n        return TheWeatherCompany()\n    # -----&gt; Include new connectors here &lt; ------\n    # elif connector_type == \"&lt;new_connector&gt;\"\n    #   return NewConnectorClass()\n    else:\n        raise TerrakitValidationError(\n            f\"Invalid connector type: '{connector_type.connector_type}'\"\n        )\n</code></pre>"},{"location":"api/data_connectors/ibmresearchstac/","title":"IBM Research STAC Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.download.data_connectors.ibmresearch_stac</code> Data Connector Modules.</p>"},{"location":"api/data_connectors/ibmresearchstac/#terrakit.download.data_connectors.ibmresearch_stac","title":"<code>terrakit.download.data_connectors.ibmresearch_stac</code>","text":""},{"location":"api/data_connectors/ibmresearchstac/#terrakit.download.data_connectors.ibmresearch_stac.IBMResearchSTAC","title":"<code>IBMResearchSTAC</code>","text":"<p>               Bases: <code>Connector</code></p> <p>Attributes:</p> Name Type Description <code>connector_type</code> <code>str</code> <p>Name of connector</p> <code>collections</code> <code>list</code> <p>A list of available collections.</p> <code>collections_details</code> <code>list</code> <p>Detailed information about the collections.</p> Source code in <code>terrakit/download/data_connectors/ibmresearch_stac.py</code> <pre><code>class IBMResearchSTAC(Connector):\n    \"\"\"\n    Attributes:\n        connector_type (str): Name of connector\n        collections (list): A list of available collections.\n        collections_details (list): Detailed information about the collections.\n    \"\"\"\n\n    DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n    MEDIA_TYPES = [\"application/x-netcdf\", \"application/netcdf\"]\n\n    def __init__(self):\n        \"\"\"\n        Initializes the IBM Research STAC Connector.\n\n        Attributes:\n            _access_token (str): The access token for authentication.\n            stac_url (str): The base URL for the STAC API.\n            connector_type (str): The type of connector, which is \"IBMResearchSTAC\".\n            collections (list[str]): A list of collection IDs available on the STAC server.\n        \"\"\"\n        url = os.getenv(\"IBMRESEARCH_STAC_URL\", IBM_RESEARCH_CE_STAC_URL)\n        self._access_token: str | None = None\n        if url.endswith(\"/\"):\n            url = url[:-1]\n        self.stac_url = url\n        self.connector_type: str = \"IBMResearchSTAC\"\n        collection_details = self._get_all_collections()\n        self.collections: list[str] = [coll[\"id\"] for coll in collection_details]\n\n    @property\n    def headers(self) -&gt; dict[str, str]:\n        headers = {\"Content-Type\": \"application/json\"}\n\n        if self.stac_url == IBM_RESEARCH_CE_STAC_URL:\n            if self.access_token is None:\n                token = self._get_token()\n                self.access_token = token\n\n            headers[\"Authorization\"] = f\"Bearer {self.access_token}\"\n\n        return headers\n\n    @property\n    def access_token(self) -&gt; str | None:\n        # if token has not been obtained, get the access token\n\n        return self._access_token\n\n    @access_token.setter\n    def access_token(self, token: str):\n        if isinstance(token, str):\n            self._access_token = token\n\n    @staticmethod\n    def _validate_dates(start: str, end: str):\n        assert pd.Timestamp(start) &lt; pd.Timestamp(end)\n\n    @staticmethod\n    def _validate_bbox(bbox: tuple[float, float, float, float]):\n        \"\"\"\n        Validate the bounding box coordinates to ensure they are within valid geographic range.\n\n        Parameters:\n            bbox (tuple[float, float, float, float]): A tuple containing (west, south, east, north) coordinates.\n\n        Raises:\n            AssertionError: If the provided coordinates are not within the valid geographic range.\n        \"\"\"\n        west, south, east, north = bbox\n        assert -180 &lt;= west &lt; east &lt;= 180, f\"Error! invalid values: {west=} {east=}\"\n        assert -90 &lt;= south &lt; north &lt;= 90, f\"Error! invalid values: {south=} {north=}\"\n\n    def _get_token(self) -&gt; str:\n        \"\"\"this function is designed to retrieve an access token for authentication using the\n        IBM AppID service\n\n        Returns:\n            str: access token\n        \"\"\"\n        for var in [\n            \"APPID_ISSUER\",\n            \"CLIENT_ID\",\n            \"CLIENT_SECRET\",\n            \"APPID_USERNAME\",\n            \"APPID_PASSWORD\",\n        ]:\n            if var not in os.environ:\n                link = \"https://github.com/terrastackai/terrakit?tab=readme-ov-file#ibm-research-stac\"\n                msg = f\"Error! {var} is not set. Please check {link}\"\n                logger.error(msg)\n                raise TerrakitMissingEnvironmentVariable(message=msg)\n        appid_issuer: str = os.environ[\"APPID_ISSUER\"]\n        assert isinstance(appid_issuer, str), f\"Error! Invalid type: {appid_issuer=}\"\n        if appid_issuer.endswith(\"/\"):\n            token_url = f\"{appid_issuer}token\"\n        else:\n            token_url = f\"{appid_issuer}/token\"\n        client_id = os.environ[\"CLIENT_ID\"]\n        client_secret = os.environ[\"CLIENT_SECRET\"]\n        username = os.environ[\"APPID_USERNAME\"]\n        password = os.environ[\"APPID_PASSWORD\"]\n        token_url = f\"{appid_issuer}/token\"\n\n        payload = {\n            \"grant_type\": \"password\",\n            \"username\": username,\n            \"password\": password,\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n        }\n        headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n        response = post(\n            url=token_url,\n            headers=headers,\n            payload=payload,\n        )\n        response.raise_for_status()\n        token: str = response.json()[\"access_token\"]\n        if not isinstance(token, str):\n            raise ValueError(f\"Error! Unexpected value: {token=}\")\n        return token\n\n    def _search_items(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        bbox: tuple,\n        bands: list[str],\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Searches for items within a specified date range and bounding box.\n\n        This method constructs a search query for items in a given data collection,\n        within a specified bounding box and date range. It uses the client's search\n        functionality to retrieve the results.\n\n        Parameters:\n            data_collection_name (str): The name of the data collection to search.\n            date_start (str): The start date for the search in 'YYYY-MM-DD' format.\n            date_end (str): The end date for the search in 'YYYY-MM-DD' format.\n            bbox (tuple): A tuple representing the bounding box (minx, miny, maxx, maxy).\n            bands (list[str]): list of band names\n        Returns:\n            list[dict[str, Any]]: The search results containing matching items.\n        \"\"\"\n        logger.info(\n            f\"Search items: {data_collection_name=} {date_start=} {date_end=} {bbox=} {bands=}\"\n        )\n        start_dt = pd.Timestamp(date_start).strftime(IBMResearchSTAC.DATETIME_FORMAT)\n        end_dt = pd.Timestamp(date_end).strftime(IBMResearchSTAC.DATETIME_FORMAT)\n        dt = f\"{start_dt}/{end_dt}\"\n\n        search_url = f\"{self.stac_url}/search\"\n        data = {\n            \"collections\": [data_collection_name],\n            \"bbox\": list(bbox),\n            \"datetime\": dt,\n        }\n        logger.info(f\"Querying STAC: {search_url} {data=}\")\n        resp: requests.Response = post(\n            url=search_url, headers=self.headers, payload=data\n        )\n        feature_collection = resp.json()\n        all_items: list[dict] = feature_collection[\"features\"]\n        # filter out items that do not have specified band\n        if len(bands) &gt; 0:\n            items = list()\n            for i in all_items:\n                cube_variables: dict[str, Any] = i[\"properties\"][\"cube:variables\"]\n                available_variables = list(cube_variables.keys())\n                if any(band in available_variables for band in bands):\n                    items.append(i)\n\n            return items\n        else:\n            return all_items\n\n    def _get_all_collections(self) -&gt; list[dict]:\n        \"\"\"\n        Fetch all collections from the STAC API.\n\n        Returns:\n            list: A list of dictionaries, each representing a collection.\n        \"\"\"\n        url = f\"{self.stac_url}/collections\"\n        resp = get(url=url, headers=self.headers)\n        resp.raise_for_status()\n        collections_dict = resp.json()\n        collections: list = collections_dict[\"collections\"]\n        if not isinstance(collections, list):\n            raise ValueError(f\"Error! not a list: {collections=}\")\n        supported_collections = [c for c in collections if self._is_supported(c[\"id\"])]\n        return supported_collections\n\n    def _is_supported(self, collection_name: str) -&gt; bool:\n        \"\"\"\n        Check if the given collection supports any of the predefined media types.\n\n        This method sends a GET request to the STAC API to fetch items from the specified collection.\n        It then checks if any of the assets in the first item have a 'data' role and if the asset type\n        is one of the predefined media types.\n\n        Parameters:\n            collection_name (str): The name of the collection to check.\n\n        Returns:\n            bool: 'True' if the collection supports any of the predefined media types, 'False' otherwise.\n        \"\"\"\n        url = f\"{self.stac_url}/collections/{collection_name}/items\"\n        resp = get(url=url, headers=self.headers, params={\"limit\": 1})\n        resp.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n        response = resp.json()\n        features = response.get(\n            \"features\", []\n        )  # Use .get() to avoid KeyError if \"features\" is not present\n        if (\n            features and isinstance(features, list) and features\n        ):  # Check if features is not empty\n            item = features[0]\n            assets: dict = item.get(\n                \"assets\", {}\n            )  # Use .get() to avoid KeyError if \"assets\" is not present\n            for asset in assets.values():\n                if \"roles\" in asset and \"data\" in asset[\"roles\"]:\n                    media_type: str = asset.get(\"type\")\n                    if (\n                        media_type.lower() in IBMResearchSTAC.MEDIA_TYPES\n                    ):  # Assuming MEDIA_TYPES is a class attribute\n                        return True\n        return False\n\n    def list_collections(self) -&gt; list[Any]:\n        \"\"\"\n        Lists the available collections.\n\n        Returns:\n            list: A list of collection names.\n        \"\"\"\n        logger.info(\"Listing available collections\")\n        return self.collections\n\n    def find_data(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n        Parameters:\n            data_collection_name (str): The name of the Sentinel Hub data collection to search.\n            date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n            date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n            area_polygon (Polygon, optional): A polygon defining the area of interest.\n            bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n            bands (list, optional): A list of bands to retrieve. Defaults to [].\n            maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n            data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n        Returns:\n            tuple: A tuple containing a sorted list of unique dates and a list of data results.\n        \"\"\"\n        # validate user's input\n        IBMResearchSTAC._validate_dates(start=date_start, end=date_end)\n        # convert bbox to tuple if necessary\n        if bbox is not None and not isinstance(bbox, tuple):\n            bbox = tuple(bbox)\n            # validate bbox\n            IBMResearchSTAC._validate_bbox(bbox)\n        # check if collection is supported\n        if data_collection_name not in [c for c in self.collections]:\n            msg = f\"Error! {data_collection_name=} is not supported\"\n            logger.error(msg)\n            raise ValueError(msg)\n\n        # search items using STAC API\n        items_as_dicts = self._search_items(\n            data_collection_name=data_collection_name,\n            date_start=date_start,\n            date_end=date_end,\n            bbox=bbox,\n            bands=bands,\n        )\n        unique_dates: set = set()\n        results: list[dict[str, Any]] = list()\n        item_dict: dict\n        for item_dict in items_as_dicts:\n            item_properties: dict = item_dict[\"properties\"]\n            if item_properties.get(\"datetime\") is not None:\n                unique_dates.add(item_properties[\"datetime\"])\n            else:\n                unique_dates.add(item_properties[\"start_datetime\"])\n\n            results.append(item_dict)\n        return list(unique_dates), results\n\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ):\n        \"\"\"\n        Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n        Parameters:\n            data_collection_name (str): Name of the data collection to fetch data from.\n            date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n            bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n            bands (list, optional): List of bands to retrieve. Defaults to all bands.\n            maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n            data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n            save_file (str, optional): Path to save the output file. Defaults to None.\n            working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n        Returns:\n            xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n        \"\"\"\n        if bbox is not None and not isinstance(bbox, tuple):\n            bbox = tuple(bbox)\n            IBMResearchSTAC._validate_bbox(bbox)\n        logger.debug(\n            f\"Fetching data from {data_collection_name} {bbox=} {date_start=} {date_end=}\"\n        )\n        items_as_dicts = self._search_items(\n            data_collection_name=data_collection_name,\n            date_start=date_start,\n            date_end=date_end,\n            bbox=bbox,\n            bands=bands,\n        )\n\n        start_dt = pd.Timestamp(date_start).to_pydatetime()\n        end_dt = pd.Timestamp(date_end).to_pydatetime()\n        temporal_extent = (start_dt, end_dt)\n        file_reader = NetCDFFileReader(\n            items=items_as_dicts,\n            bbox=bbox,\n            temporal_extent=temporal_extent,\n            bands=bands,\n            properties=None,\n        )\n        data = file_reader.load_items()\n        # persist data\n        if save_file is not None:\n            extension = Path(save_file).suffix\n            match extension:\n                case \".tif\":\n                    save_data_array_to_file(da=data, save_file=save_file)\n                case \".nc\":\n                    save_data_array_as_netcdf(da=data, save_file=save_file)\n                case _:  # Default case (wildcard)\n                    # Code to execute if no other pattern matches\n                    raise ValueError(f\"Error! Invalid extension: {extension}\")\n        return data\n</code></pre>"},{"location":"api/data_connectors/ibmresearchstac/#terrakit.download.data_connectors.ibmresearch_stac.IBMResearchSTAC.list_collections","title":"<code>list_collections</code>","text":"<p>Lists the available collections.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Any]</code> <p>A list of collection names.</p> Source code in <code>terrakit/download/data_connectors/ibmresearch_stac.py</code> <pre><code>def list_collections(self) -&gt; list[Any]:\n    \"\"\"\n    Lists the available collections.\n\n    Returns:\n        list: A list of collection names.\n    \"\"\"\n    logger.info(\"Listing available collections\")\n    return self.collections\n</code></pre>"},{"location":"api/data_connectors/ibmresearchstac/#terrakit.download.data_connectors.ibmresearch_stac.IBMResearchSTAC.find_data","title":"<code>find_data</code>","text":"<p>This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the Sentinel Hub data collection to search.</p> required <code>date_start</code> <code>str</code> <p>The start date for the time interval in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>The end date for the time interval in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>Polygon</code> <p>A polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>tuple</code> <p>A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).</p> <code>None</code> <code>bands</code> <code>list</code> <p>A list of bands to retrieve. Defaults to [].</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).</p> <code>100</code> <code>data_connector_spec</code> <code>list</code> <p>A dictionary containing the data connector specification.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>A tuple containing a sorted list of unique dates and a list of data results.</p> Source code in <code>terrakit/download/data_connectors/ibmresearch_stac.py</code> <pre><code>def find_data(\n    self,\n    data_collection_name: str,\n    date_start: str,\n    date_end: str,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n    Parameters:\n        data_collection_name (str): The name of the Sentinel Hub data collection to search.\n        date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n        date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n        area_polygon (Polygon, optional): A polygon defining the area of interest.\n        bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n        bands (list, optional): A list of bands to retrieve. Defaults to [].\n        maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n        data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n    Returns:\n        tuple: A tuple containing a sorted list of unique dates and a list of data results.\n    \"\"\"\n    # validate user's input\n    IBMResearchSTAC._validate_dates(start=date_start, end=date_end)\n    # convert bbox to tuple if necessary\n    if bbox is not None and not isinstance(bbox, tuple):\n        bbox = tuple(bbox)\n        # validate bbox\n        IBMResearchSTAC._validate_bbox(bbox)\n    # check if collection is supported\n    if data_collection_name not in [c for c in self.collections]:\n        msg = f\"Error! {data_collection_name=} is not supported\"\n        logger.error(msg)\n        raise ValueError(msg)\n\n    # search items using STAC API\n    items_as_dicts = self._search_items(\n        data_collection_name=data_collection_name,\n        date_start=date_start,\n        date_end=date_end,\n        bbox=bbox,\n        bands=bands,\n    )\n    unique_dates: set = set()\n    results: list[dict[str, Any]] = list()\n    item_dict: dict\n    for item_dict in items_as_dicts:\n        item_properties: dict = item_dict[\"properties\"]\n        if item_properties.get(\"datetime\") is not None:\n            unique_dates.add(item_properties[\"datetime\"])\n        else:\n            unique_dates.add(item_properties[\"start_datetime\"])\n\n        results.append(item_dict)\n    return list(unique_dates), results\n</code></pre>"},{"location":"api/data_connectors/ibmresearchstac/#terrakit.download.data_connectors.ibmresearch_stac.IBMResearchSTAC.get_data","title":"<code>get_data</code>","text":"<p>Fetches data from SentinelHub for the specified collection, date range, area, and bands.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>Name of the data collection to fetch data from.</p> required <code>date_start</code> <code>str</code> <p>Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to retrieve. Defaults to all bands.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover threshold (0-100). Defaults to 100.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Data connector specification. Defaults to None.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the output file. Defaults to None.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files. Defaults to '.'.</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>xarray</code> <p>An xarray Datasets containing the fetched data with dimensions (time, band, y, x).</p> Source code in <code>terrakit/download/data_connectors/ibmresearch_stac.py</code> <pre><code>def get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n):\n    \"\"\"\n    Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n    Parameters:\n        data_collection_name (str): Name of the data collection to fetch data from.\n        date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n        bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n        bands (list, optional): List of bands to retrieve. Defaults to all bands.\n        maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n        data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n        save_file (str, optional): Path to save the output file. Defaults to None.\n        working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n    Returns:\n        xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n    \"\"\"\n    if bbox is not None and not isinstance(bbox, tuple):\n        bbox = tuple(bbox)\n        IBMResearchSTAC._validate_bbox(bbox)\n    logger.debug(\n        f\"Fetching data from {data_collection_name} {bbox=} {date_start=} {date_end=}\"\n    )\n    items_as_dicts = self._search_items(\n        data_collection_name=data_collection_name,\n        date_start=date_start,\n        date_end=date_end,\n        bbox=bbox,\n        bands=bands,\n    )\n\n    start_dt = pd.Timestamp(date_start).to_pydatetime()\n    end_dt = pd.Timestamp(date_end).to_pydatetime()\n    temporal_extent = (start_dt, end_dt)\n    file_reader = NetCDFFileReader(\n        items=items_as_dicts,\n        bbox=bbox,\n        temporal_extent=temporal_extent,\n        bands=bands,\n        properties=None,\n    )\n    data = file_reader.load_items()\n    # persist data\n    if save_file is not None:\n        extension = Path(save_file).suffix\n        match extension:\n            case \".tif\":\n                save_data_array_to_file(da=data, save_file=save_file)\n            case \".nc\":\n                save_data_array_as_netcdf(da=data, save_file=save_file)\n            case _:  # Default case (wildcard)\n                # Code to execute if no other pattern matches\n                raise ValueError(f\"Error! Invalid extension: {extension}\")\n    return data\n</code></pre>"},{"location":"api/data_connectors/nasa_earthdata/","title":"NASA Earthdata Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.download.data_connectors.nasa_earthdata</code> data connector module.</p>"},{"location":"api/data_connectors/nasa_earthdata/#terrakit.download.data_connectors.nasa_earthdata","title":"<code>terrakit.download.data_connectors.nasa_earthdata</code>","text":""},{"location":"api/data_connectors/nasa_earthdata/#terrakit.download.data_connectors.nasa_earthdata.NASA_EarthData","title":"<code>NASA_EarthData</code>","text":"<p>               Bases: <code>Connector</code></p> <p>Class to interact with NASA EarthData connector for listing collections and fetching data.</p> Source code in <code>terrakit/download/data_connectors/nasa_earthdata.py</code> <pre><code>class NASA_EarthData(Connector):\n    \"\"\"\n    Class to interact with NASA EarthData connector for listing collections and fetching data.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        List available collections.\n        \"\"\"\n        self.connector_type = \"nasa_earthdata\"\n        self.collections: list[Any] = load_and_list_collections(\n            connector_type=\"nasa_earthdata\"\n        )\n        self.collections_details = load_and_list_collections(\n            as_json=True, connector_type=\"nasa_earthdata\"\n        )\n        self.lp_search = connect_to_stac(\n            stac_url=\"https://cmr.earthdata.nasa.gov/stac/\", subcatalog_name=\"LPCLOUD\"\n        )\n\n    def list_collections(self) -&gt; list:\n        \"\"\"\n        Returns the current list of collections for th NASA EarthData connector.\n\n        Returns:\n            list: The list of collections managed by the class.\n        \"\"\"\n        logger.info(\"Listing available collections\")\n        return self.collections\n\n    def find_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        Finds data items in the specified collection, date range, and area.\n\n        Args:\n            data_collection_name (str): The name of the data collection to search.\n            date_start (str): The start date for the search (YYYY-MM-DD).\n            date_end (str): The end date for the search (YYYY-MM-DD).\n            area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n            bbox (list, optional): Bounding box defining the area of interest [west, south, east, north]. Defaults to None.\n            bands (list, optional): List of bands to retrieve. Defaults to [].\n            maxcc (int, optional): Maximum cloud cover percentage. Defaults to 100.\n            data_connector_spec (dict, optional): Additional data connector specifications. Defaults to None.\n\n        Returns:\n            tuple: A tuple containing unique dates and the list of data items.\n        \"\"\"\n\n        # Check credentials have been set correctly.\n        if \"NASA_EARTH_BEARER_TOKEN\" not in os.environ:\n            raise TerrakitValidationError(\n                message=\"Error: Missing credentials 'NASA_EARTH_BEARER_TOKEN'. Please update .env with correct credentials.\"\n            )\n\n        # Check data_collection_name exists in self.collections.\n        check_collection_exists(data_collection_name, self.collections)\n\n        logger.info(\"Listing NASA Earthdata data\")\n\n        items = find_items(\n            self.lp_search,\n            bbox,\n            date_start,\n            date_end,\n            collections=[data_collection_name],\n            limit=250,\n        )\n\n        if maxcc:\n            items = [\n                item\n                for item in items\n                if item[\"properties\"].get(\"eo:cloud_cover\") &lt; maxcc\n            ]\n        items = [\n            {\n                \"id\": item[\"id\"],\n                \"properties\": {\n                    \"datetime\": item[\"properties\"][\"datetime\"],\n                    \"eo:cloud_cover\": item[\"properties\"][\"eo:cloud_cover\"],\n                },\n            }\n            for item in items\n        ]\n        unique_dates = sorted(\n            set(([X[\"properties\"][\"datetime\"].split(\"T\")[0] for X in items]))\n        )\n\n        logger.info(f\"Found {len(unique_dates)} unique dates:  {unique_dates}\")\n\n        return unique_dates, items\n\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ) -&gt; Union[xr.DataArray, None]:\n        \"\"\"\n        Fetches data from NASA EarthData connector for the specified collection, date range, area, and bands.\n\n        Args:\n            data_collection_name (str): The name of the data collection to fetch.\n            date_start (str): The start date for the search (YYYY-MM-DD).\n            date_end (str): The end date for the search (YYYY-MM-DD).\n            area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n            bbox (list, optional): Bounding box defining the area of interest [west, south, east, north]. Defaults to None.\n            bands (list): List of bands to fetch. Defaults to [].\n            maxcc (int, optional): Maximum cloud cover percentage. Defaults to 100.\n            data_connector_spec (dict, optional): Additional data connector specifications. Defaults to None.\n            save_file (str, optional): Path to save the fetched data. Defaults to None.\n            working_dir (str, optional): Working directory for temporary files. Defaults to \".\".\n\n        Returns:\n            xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n        \"\"\"\n        # Check credentials have been set correctly.\n        if \"NASA_EARTH_BEARER_TOKEN\" not in os.environ:\n            raise TerrakitValidationError(\n                message=\"Error: Missing credentials 'NASA_EARTH_BEARER_TOKEN'. Please update .env with correct credentials.\"\n            )\n\n        # Check data_collection_name exists in self.collections.\n        if data_collection_name not in self.collections:\n            raise TerrakitValueError(\n                message=f\"Invalid collection '{data_collection_name}'. Please choose from one of the following collection {self.collections}\"\n            )\n\n        check_area_polygon(\n            area_polygon=area_polygon, connector_type=self.connector_type\n        )\n        temp_creds_req = get_temp_creds()\n\n        session = boto3.Session(\n            aws_access_key_id=temp_creds_req[\"accessKeyId\"],\n            aws_secret_access_key=temp_creds_req[\"secretAccessKey\"],\n            aws_session_token=temp_creds_req[\"sessionToken\"],\n            region_name=\"us-west-2\",\n        )\n\n        if NASA_EARTH_BEARER_TOKEN:\n            rio_env = rio.Env(\n                AWSSession(session),\n                GDAL_HTTP_AUTH=\"BEARER\",  # pragma: allowlist secret\n                GDAL_HTTP_BEARER=NASA_EARTH_BEARER_TOKEN,\n                GDAL_DISABLE_READDIR_ON_OPEN=\"TRUE\",\n                GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"~/cookies.txt\"),\n                GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"~/cookies.txt\"),\n            )\n        else:\n            rio_env = rio.Env(\n                AWSSession(session),\n                GDAL_DISABLE_READDIR_ON_OPEN=\"TRUE\",\n                GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"~/cookies.txt\"),\n                GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"~/cookies.txt\"),\n            )\n\n        with rio_env:\n            unique_dates: Union[list[str], None]\n            results: Union[list[dict[str, Any]], None]\n\n            results = find_items(\n                self.lp_search,\n                bbox,\n                date_start,\n                date_end,\n                collections=[data_collection_name],\n                limit=250,\n            )\n            if maxcc:\n                results = [\n                    item\n                    for item in results\n                    if item[\"properties\"].get(\"eo:cloud_cover\") &lt; maxcc\n                ]\n            unique_dates = sorted(\n                set(([X[\"properties\"][\"datetime\"].split(\"T\")[0] for X in results]))\n            )\n            # Check that unique dates and find_data results are not None.\n            if unique_dates is None and results is None:\n                logger.warning(\"Warning: Unique dates and find_data results are None\")\n                return None\n\n            ds: xr.DataArray\n            ds_list: list[Any] = []\n            for udate in unique_dates:  # type: ignore[union-attr]\n                date_items: list[Any] = []\n                for X in results:  # type: ignore[union-attr]\n                    if X[\"properties\"][\"datetime\"].split(\"T\")[0] == udate:\n                        date_items.append(X)\n                num_threads = len(bands)\n                ans = Parallel(n_jobs=num_threads, prefer=\"threads\")(\n                    delayed(get_band)(date_items, b, bbox, temp_creds_req, working_dir)\n                    for b in tqdm(bands)\n                )\n                da = xr.concat(ans, dim=\"band\")\n\n                data_date_datetime = datetime.strptime(udate, \"%Y-%m-%d\")\n                da = da.assign_coords({\"band\": bands, \"time\": data_date_datetime})\n\n                ds_list.append(da)\n            ds = xr.concat(ds_list, dim=\"time\")\n\n            save_data_array_to_file(ds, save_file)\n            deleteList = glob.glob(f\"{working_dir}/links_*.vrt\", recursive=True)\n            for file_to_delete in deleteList:\n                try:\n                    os.remove(file_to_delete)\n                except OSError as err:\n                    logger.error(\"Error while deleting file\", err)\n\n            return ds\n</code></pre>"},{"location":"api/data_connectors/nasa_earthdata/#terrakit.download.data_connectors.nasa_earthdata.NASA_EarthData.list_collections","title":"<code>list_collections</code>","text":"<p>Returns the current list of collections for th NASA EarthData connector.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of collections managed by the class.</p> Source code in <code>terrakit/download/data_connectors/nasa_earthdata.py</code> <pre><code>def list_collections(self) -&gt; list:\n    \"\"\"\n    Returns the current list of collections for th NASA EarthData connector.\n\n    Returns:\n        list: The list of collections managed by the class.\n    \"\"\"\n    logger.info(\"Listing available collections\")\n    return self.collections\n</code></pre>"},{"location":"api/data_connectors/nasa_earthdata/#terrakit.download.data_connectors.nasa_earthdata.NASA_EarthData.find_data","title":"<code>find_data</code>","text":"<p>Finds data items in the specified collection, date range, and area.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the data collection to search.</p> required <code>date_start</code> <code>str</code> <p>The start date for the search (YYYY-MM-DD).</p> required <code>date_end</code> <code>str</code> <p>The end date for the search (YYYY-MM-DD).</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest [west, south, east, north]. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to retrieve. Defaults to [].</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage. Defaults to 100.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Additional data connector specifications. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>A tuple containing unique dates and the list of data items.</p> Source code in <code>terrakit/download/data_connectors/nasa_earthdata.py</code> <pre><code>def find_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    Finds data items in the specified collection, date range, and area.\n\n    Args:\n        data_collection_name (str): The name of the data collection to search.\n        date_start (str): The start date for the search (YYYY-MM-DD).\n        date_end (str): The end date for the search (YYYY-MM-DD).\n        area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n        bbox (list, optional): Bounding box defining the area of interest [west, south, east, north]. Defaults to None.\n        bands (list, optional): List of bands to retrieve. Defaults to [].\n        maxcc (int, optional): Maximum cloud cover percentage. Defaults to 100.\n        data_connector_spec (dict, optional): Additional data connector specifications. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing unique dates and the list of data items.\n    \"\"\"\n\n    # Check credentials have been set correctly.\n    if \"NASA_EARTH_BEARER_TOKEN\" not in os.environ:\n        raise TerrakitValidationError(\n            message=\"Error: Missing credentials 'NASA_EARTH_BEARER_TOKEN'. Please update .env with correct credentials.\"\n        )\n\n    # Check data_collection_name exists in self.collections.\n    check_collection_exists(data_collection_name, self.collections)\n\n    logger.info(\"Listing NASA Earthdata data\")\n\n    items = find_items(\n        self.lp_search,\n        bbox,\n        date_start,\n        date_end,\n        collections=[data_collection_name],\n        limit=250,\n    )\n\n    if maxcc:\n        items = [\n            item\n            for item in items\n            if item[\"properties\"].get(\"eo:cloud_cover\") &lt; maxcc\n        ]\n    items = [\n        {\n            \"id\": item[\"id\"],\n            \"properties\": {\n                \"datetime\": item[\"properties\"][\"datetime\"],\n                \"eo:cloud_cover\": item[\"properties\"][\"eo:cloud_cover\"],\n            },\n        }\n        for item in items\n    ]\n    unique_dates = sorted(\n        set(([X[\"properties\"][\"datetime\"].split(\"T\")[0] for X in items]))\n    )\n\n    logger.info(f\"Found {len(unique_dates)} unique dates:  {unique_dates}\")\n\n    return unique_dates, items\n</code></pre>"},{"location":"api/data_connectors/nasa_earthdata/#terrakit.download.data_connectors.nasa_earthdata.NASA_EarthData.get_data","title":"<code>get_data</code>","text":"<p>Fetches data from NASA EarthData connector for the specified collection, date range, area, and bands.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the data collection to fetch.</p> required <code>date_start</code> <code>str</code> <p>The start date for the search (YYYY-MM-DD).</p> required <code>date_end</code> <code>str</code> <p>The end date for the search (YYYY-MM-DD).</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest [west, south, east, north]. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to fetch. Defaults to [].</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage. Defaults to 100.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Additional data connector specifications. Defaults to None.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the fetched data. Defaults to None.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files. Defaults to \".\".</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>xarray</code> <code>Union[DataArray, None]</code> <p>An xarray Datasets containing the fetched data with dimensions (time, band, y, x).</p> Source code in <code>terrakit/download/data_connectors/nasa_earthdata.py</code> <pre><code>def get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n) -&gt; Union[xr.DataArray, None]:\n    \"\"\"\n    Fetches data from NASA EarthData connector for the specified collection, date range, area, and bands.\n\n    Args:\n        data_collection_name (str): The name of the data collection to fetch.\n        date_start (str): The start date for the search (YYYY-MM-DD).\n        date_end (str): The end date for the search (YYYY-MM-DD).\n        area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n        bbox (list, optional): Bounding box defining the area of interest [west, south, east, north]. Defaults to None.\n        bands (list): List of bands to fetch. Defaults to [].\n        maxcc (int, optional): Maximum cloud cover percentage. Defaults to 100.\n        data_connector_spec (dict, optional): Additional data connector specifications. Defaults to None.\n        save_file (str, optional): Path to save the fetched data. Defaults to None.\n        working_dir (str, optional): Working directory for temporary files. Defaults to \".\".\n\n    Returns:\n        xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n    \"\"\"\n    # Check credentials have been set correctly.\n    if \"NASA_EARTH_BEARER_TOKEN\" not in os.environ:\n        raise TerrakitValidationError(\n            message=\"Error: Missing credentials 'NASA_EARTH_BEARER_TOKEN'. Please update .env with correct credentials.\"\n        )\n\n    # Check data_collection_name exists in self.collections.\n    if data_collection_name not in self.collections:\n        raise TerrakitValueError(\n            message=f\"Invalid collection '{data_collection_name}'. Please choose from one of the following collection {self.collections}\"\n        )\n\n    check_area_polygon(\n        area_polygon=area_polygon, connector_type=self.connector_type\n    )\n    temp_creds_req = get_temp_creds()\n\n    session = boto3.Session(\n        aws_access_key_id=temp_creds_req[\"accessKeyId\"],\n        aws_secret_access_key=temp_creds_req[\"secretAccessKey\"],\n        aws_session_token=temp_creds_req[\"sessionToken\"],\n        region_name=\"us-west-2\",\n    )\n\n    if NASA_EARTH_BEARER_TOKEN:\n        rio_env = rio.Env(\n            AWSSession(session),\n            GDAL_HTTP_AUTH=\"BEARER\",  # pragma: allowlist secret\n            GDAL_HTTP_BEARER=NASA_EARTH_BEARER_TOKEN,\n            GDAL_DISABLE_READDIR_ON_OPEN=\"TRUE\",\n            GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"~/cookies.txt\"),\n            GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"~/cookies.txt\"),\n        )\n    else:\n        rio_env = rio.Env(\n            AWSSession(session),\n            GDAL_DISABLE_READDIR_ON_OPEN=\"TRUE\",\n            GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"~/cookies.txt\"),\n            GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"~/cookies.txt\"),\n        )\n\n    with rio_env:\n        unique_dates: Union[list[str], None]\n        results: Union[list[dict[str, Any]], None]\n\n        results = find_items(\n            self.lp_search,\n            bbox,\n            date_start,\n            date_end,\n            collections=[data_collection_name],\n            limit=250,\n        )\n        if maxcc:\n            results = [\n                item\n                for item in results\n                if item[\"properties\"].get(\"eo:cloud_cover\") &lt; maxcc\n            ]\n        unique_dates = sorted(\n            set(([X[\"properties\"][\"datetime\"].split(\"T\")[0] for X in results]))\n        )\n        # Check that unique dates and find_data results are not None.\n        if unique_dates is None and results is None:\n            logger.warning(\"Warning: Unique dates and find_data results are None\")\n            return None\n\n        ds: xr.DataArray\n        ds_list: list[Any] = []\n        for udate in unique_dates:  # type: ignore[union-attr]\n            date_items: list[Any] = []\n            for X in results:  # type: ignore[union-attr]\n                if X[\"properties\"][\"datetime\"].split(\"T\")[0] == udate:\n                    date_items.append(X)\n            num_threads = len(bands)\n            ans = Parallel(n_jobs=num_threads, prefer=\"threads\")(\n                delayed(get_band)(date_items, b, bbox, temp_creds_req, working_dir)\n                for b in tqdm(bands)\n            )\n            da = xr.concat(ans, dim=\"band\")\n\n            data_date_datetime = datetime.strptime(udate, \"%Y-%m-%d\")\n            da = da.assign_coords({\"band\": bands, \"time\": data_date_datetime})\n\n            ds_list.append(da)\n        ds = xr.concat(ds_list, dim=\"time\")\n\n        save_data_array_to_file(ds, save_file)\n        deleteList = glob.glob(f\"{working_dir}/links_*.vrt\", recursive=True)\n        for file_to_delete in deleteList:\n            try:\n                os.remove(file_to_delete)\n            except OSError as err:\n                logger.error(\"Error while deleting file\", err)\n\n        return ds\n</code></pre>"},{"location":"api/data_connectors/sentinel_hub/","title":"Sentinel Hub Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.download.data_connectors.sentinelhub</code> data connector module.</p>"},{"location":"api/data_connectors/sentinel_hub/#terrakit.download.data_connectors.sentinelhub","title":"<code>terrakit.download.data_connectors.sentinelhub</code>","text":""},{"location":"api/data_connectors/sentinel_hub/#terrakit.download.data_connectors.sentinelhub.SentinelHub","title":"<code>SentinelHub</code>","text":"<p>               Bases: <code>Connector</code></p> <p>A class to interact with Sentinel Hub data services.</p> <p>Attributes:</p> Name Type Description <code>collections</code> <code>list</code> <p>A list of available collections.</p> <code>collections_details</code> <code>list</code> <p>Detailed information about the collections.</p> <code>sh_config</code> <code>SHConfig</code> <p>Configuration settings for Sentinel Hub.</p> Source code in <code>terrakit/download/data_connectors/sentinelhub.py</code> <pre><code>class SentinelHub(Connector):\n    \"\"\"\n    A class to interact with Sentinel Hub data services.\n\n    Attributes:\n        collections (list): A list of available collections.\n        collections_details (list): Detailed information about the collections.\n        sh_config (SHConfig): Configuration settings for Sentinel Hub.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize SentinelHub with collections and configuration.\n        \"\"\"\n        self.connector_type = \"sentinelhub\"\n        self.collections: list[Any] = load_and_list_collections(\n            connector_type=\"sentinelhub\"\n        )\n        self.collections_details = load_and_list_collections(\n            as_json=True, connector_type=\"sentinelhub\"\n        )\n        self.sh_config = SHConfig()\n\n    def list_collections(self) -&gt; list[Any]:\n        \"\"\"\n        Lists the available collections.\n\n        Returns:\n            list: A list of collection names.\n        \"\"\"\n        logger.info(\"Listing available collections\")\n        return self.collections\n\n    def find_data(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n        Parameters:\n            data_collection_name (str): The name of the Sentinel Hub data collection to search.\n            date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n            date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n            area_polygon (Polygon, optional): A polygon defining the area of interest.\n            bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n            bands (list, optional): A list of bands to retrieve. Defaults to [].\n            maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n            data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n        Returns:\n            tuple: A tuple containing a sorted list of unique dates and a list of data results.\n\n        Raises:\n            TerrakitValidationError: If a validation error occurs.\n            TerrakitValueError: If a value error occurs.\n        \"\"\"\n        # Check credentials have been set correctly.\n        if \"SH_CLIENT_ID\" not in os.environ and \"SH_CLIENT_SECRET\" not in os.environ:\n            raise TerrakitValidationError(\n                message=\"Error: Missing credentials 'SH_CLIENT_ID' and 'SH_CLIENT_SECRET'. Please update .env with correct credentials.\"\n            )\n\n        # Check data_collection_name exists in self.collections.\n        check_collection_exists(data_collection_name, self.collections)\n\n        # Check date_start and date_end are in the correct format.\n        check_start_end_date(date_start=date_start, date_end=date_end)\n        check_area_polygon(\n            area_polygon=area_polygon, connector_type=self.connector_type\n        )\n        check_bbox(bbox=bbox, connector_type=self.connector_type)\n\n        if data_connector_spec is None:\n            data_connector_spec_list = [\n                X\n                for X in self.collections_details\n                if X[\"collection_name\"] == data_collection_name\n            ]\n            if len(data_connector_spec_list) == 0:\n                error_msg = (\n                    f\"Unable to find collection details for '{data_collection_name}'\"\n                )\n                logger.error(error_msg)\n                raise ValueError(error_msg)\n            data_connector_spec = data_connector_spec_list[0]\n\n        data_collection = eval(data_connector_spec[\"data_collection\"])\n\n        self.sh_config.sh_base_url = data_collection.service_url\n        logger.info(self.sh_config.sh_base_url)\n        dataset_catalog = SentinelHubCatalog(config=self.sh_config)\n\n        if \"filter\" in data_connector_spec[\"search\"]:\n            filter_string = data_connector_spec[\"search\"][\"filter\"]\n\n            for X in data_connector_spec[\"request_input_data\"]:\n                if X == \"maxcc\":\n                    filter_string = filter_string.replace(X, str(maxcc))\n                else:\n                    filter_string = filter_string.replace(\n                        X, str(data_connector_spec[\"request_input_data\"][X])\n                    )\n        else:\n            filter_string = \"\"\n\n        if \"fields\" in data_connector_spec[\"search\"]:\n            fields_dict = eval(data_connector_spec[\"search\"][\"fields\"])\n\n        else:\n            fields_dict = {\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []}\n\n        time_interval = date_start, date_end\n\n        if bbox is not None:\n            aoi_bbox = BBox(bbox=bbox, crs=CRS.WGS84)\n\n            search_iterator = dataset_catalog.search(\n                data_collection,\n                bbox=aoi_bbox,\n                time=time_interval,\n                filter=filter_string,\n                fields=fields_dict,\n            )\n        elif area_polygon is not None:\n            search_iterator = dataset_catalog.search(\n                data_collection,\n                intersects=area_polygon,\n                time=time_interval,\n                filter=filter_string,\n                fields=fields_dict,\n            )\n        else:\n            error_msg = f\"Error: Issue finding data from {self.connector_type}. Please specify at least one of 'bbox' and 'area_polygon'\"\n            logger.error(error_msg)\n            raise TerrakitValueError(error_msg)\n\n        try:\n            results = list(search_iterator)\n        except InvalidClientError as e:\n            error_msg = (\n                f\"Error: Issue authenticating. Check credentials are up to date.{e}\"\n            )\n            logger.error(error_msg)\n            return None, None\n\n        unique_dates = sorted(set([X[\"properties\"][\"datetime\"][0:10] for X in results]))\n        return unique_dates, results\n\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ) -&gt; Union[xr.DataArray, None]:\n        \"\"\"\n        Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n        Parameters:\n            data_collection_name (str): Name of the data collection to fetch data from.\n            date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n            area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n            bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n            bands (list, optional): List of bands to retrieve. Defaults to all bands.\n            maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n            data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n            save_file (str, optional): Path to save the output file. Defaults to None.\n            working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n        Returns:\n            xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n\n        Raises:\n            TerrakitValidationError: If a validation error occurs.\n            TerrakitValueError: If a value error occurs.\n        \"\"\"\n        # Check credentials have been set correctly.\n        if \"SH_CLIENT_ID\" not in os.environ and \"SH_CLIENT_SECRET\" not in os.environ:\n            error_msg = \"Error: Missing credentials 'SH_CLIENT_ID' and 'SH_CLIENT_SECRET'. Please update .env with correct credentials.\"\n            logger.warning(error_msg)\n            raise TerrakitValidationError(error_msg)\n\n        # Check data_collection_name exists in self.collections.\n        if data_collection_name not in self.collections:\n            error_msg = f\"Invalid collection '{data_collection_name}'. Please choose from one of the following collection {self.collections}\"\n            logger.warning(error_msg)\n            raise TerrakitValueError(error_msg)\n\n        logger.info(bands)\n        collection_details = [\n            X\n            for X in self.collections_details\n            if X[\"collection_name\"] == data_collection_name\n        ][0]\n\n        unique_dates, res = self.find_data(\n            data_collection_name, date_start, date_end, bbox=bbox, maxcc=maxcc\n        )\n\n        # Check that unique dates and find_data results are not None.\n        if unique_dates is None and res is None:\n            logger.warning(\"Warning: Unique dates and find_data results are None\")\n            return None\n\n        if unique_dates == []:\n            logger.warning(\n                f\"No data found for the specified date range {date_start}:{date_end}. Unique dates: {unique_dates}\"\n            )\n            return None\n        da_list = []\n        logger.info(f\"The following unique dates were found: {unique_dates}\")\n        for udate in unique_dates:  # type: ignore[union-attr]\n            usave_file = (\n                save_file.replace(\".tif\", f\"_{udate}.tif\")\n                if save_file is not None\n                else None\n            )\n\n            da: xr.DataArray = sh_get_data(\n                self.sh_config,\n                collection_details,\n                bbox,\n                udate,\n                bands,\n                usave_file,\n                sh_data_dir=f\"{working_dir}/sh_data\",\n            )\n\n            da_list.append(da)\n\n        logger.info(\"Concatenating data...\")\n        da = xr.concat(da_list, dim=\"time\")\n\n        # Save to file\n        save_data_array_to_file(da, save_file)\n\n        sh_data_dir = f\"{working_dir}/sh_data\"\n        logging.info(f\"Removing dir {sh_data_dir}\")\n        shutil.rmtree(sh_data_dir, ignore_errors=True)\n\n        return da\n</code></pre>"},{"location":"api/data_connectors/sentinel_hub/#terrakit.download.data_connectors.sentinelhub.SentinelHub.list_collections","title":"<code>list_collections</code>","text":"<p>Lists the available collections.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Any]</code> <p>A list of collection names.</p> Source code in <code>terrakit/download/data_connectors/sentinelhub.py</code> <pre><code>def list_collections(self) -&gt; list[Any]:\n    \"\"\"\n    Lists the available collections.\n\n    Returns:\n        list: A list of collection names.\n    \"\"\"\n    logger.info(\"Listing available collections\")\n    return self.collections\n</code></pre>"},{"location":"api/data_connectors/sentinel_hub/#terrakit.download.data_connectors.sentinelhub.SentinelHub.find_data","title":"<code>find_data</code>","text":"<p>This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>The name of the Sentinel Hub data collection to search.</p> required <code>date_start</code> <code>str</code> <p>The start date for the time interval in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>The end date for the time interval in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>Polygon</code> <p>A polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>tuple</code> <p>A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).</p> <code>None</code> <code>bands</code> <code>list</code> <p>A list of bands to retrieve. Defaults to [].</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).</p> <code>100</code> <code>data_connector_spec</code> <code>list</code> <p>A dictionary containing the data connector specification.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>A tuple containing a sorted list of unique dates and a list of data results.</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If a validation error occurs.</p> <code>TerrakitValueError</code> <p>If a value error occurs.</p> Source code in <code>terrakit/download/data_connectors/sentinelhub.py</code> <pre><code>def find_data(\n    self,\n    data_collection_name: str,\n    date_start: str,\n    date_end: str,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    This function retrieves unique dates and corresponding data results from a specified Sentinel Hub data collection.\n\n    Parameters:\n        data_collection_name (str): The name of the Sentinel Hub data collection to search.\n        date_start (str): The start date for the time interval in 'YYYY-MM-DD' format.\n        date_end (str): The end date for the time interval in 'YYYY-MM-DD' format.\n        area_polygon (Polygon, optional): A polygon defining the area of interest.\n        bbox (tuple, optional): A bounding box defining the area of interest in the format (minx, miny, maxx, maxy).\n        bands (list, optional): A list of bands to retrieve. Defaults to [].\n        maxcc (int, optional): The maximum cloud cover percentage for the data. Default is 100 (no cloud cover filter).\n        data_connector_spec (list, optional): A dictionary containing the data connector specification.\n\n    Returns:\n        tuple: A tuple containing a sorted list of unique dates and a list of data results.\n\n    Raises:\n        TerrakitValidationError: If a validation error occurs.\n        TerrakitValueError: If a value error occurs.\n    \"\"\"\n    # Check credentials have been set correctly.\n    if \"SH_CLIENT_ID\" not in os.environ and \"SH_CLIENT_SECRET\" not in os.environ:\n        raise TerrakitValidationError(\n            message=\"Error: Missing credentials 'SH_CLIENT_ID' and 'SH_CLIENT_SECRET'. Please update .env with correct credentials.\"\n        )\n\n    # Check data_collection_name exists in self.collections.\n    check_collection_exists(data_collection_name, self.collections)\n\n    # Check date_start and date_end are in the correct format.\n    check_start_end_date(date_start=date_start, date_end=date_end)\n    check_area_polygon(\n        area_polygon=area_polygon, connector_type=self.connector_type\n    )\n    check_bbox(bbox=bbox, connector_type=self.connector_type)\n\n    if data_connector_spec is None:\n        data_connector_spec_list = [\n            X\n            for X in self.collections_details\n            if X[\"collection_name\"] == data_collection_name\n        ]\n        if len(data_connector_spec_list) == 0:\n            error_msg = (\n                f\"Unable to find collection details for '{data_collection_name}'\"\n            )\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        data_connector_spec = data_connector_spec_list[0]\n\n    data_collection = eval(data_connector_spec[\"data_collection\"])\n\n    self.sh_config.sh_base_url = data_collection.service_url\n    logger.info(self.sh_config.sh_base_url)\n    dataset_catalog = SentinelHubCatalog(config=self.sh_config)\n\n    if \"filter\" in data_connector_spec[\"search\"]:\n        filter_string = data_connector_spec[\"search\"][\"filter\"]\n\n        for X in data_connector_spec[\"request_input_data\"]:\n            if X == \"maxcc\":\n                filter_string = filter_string.replace(X, str(maxcc))\n            else:\n                filter_string = filter_string.replace(\n                    X, str(data_connector_spec[\"request_input_data\"][X])\n                )\n    else:\n        filter_string = \"\"\n\n    if \"fields\" in data_connector_spec[\"search\"]:\n        fields_dict = eval(data_connector_spec[\"search\"][\"fields\"])\n\n    else:\n        fields_dict = {\"include\": [\"id\", \"properties.datetime\"], \"exclude\": []}\n\n    time_interval = date_start, date_end\n\n    if bbox is not None:\n        aoi_bbox = BBox(bbox=bbox, crs=CRS.WGS84)\n\n        search_iterator = dataset_catalog.search(\n            data_collection,\n            bbox=aoi_bbox,\n            time=time_interval,\n            filter=filter_string,\n            fields=fields_dict,\n        )\n    elif area_polygon is not None:\n        search_iterator = dataset_catalog.search(\n            data_collection,\n            intersects=area_polygon,\n            time=time_interval,\n            filter=filter_string,\n            fields=fields_dict,\n        )\n    else:\n        error_msg = f\"Error: Issue finding data from {self.connector_type}. Please specify at least one of 'bbox' and 'area_polygon'\"\n        logger.error(error_msg)\n        raise TerrakitValueError(error_msg)\n\n    try:\n        results = list(search_iterator)\n    except InvalidClientError as e:\n        error_msg = (\n            f\"Error: Issue authenticating. Check credentials are up to date.{e}\"\n        )\n        logger.error(error_msg)\n        return None, None\n\n    unique_dates = sorted(set([X[\"properties\"][\"datetime\"][0:10] for X in results]))\n    return unique_dates, results\n</code></pre>"},{"location":"api/data_connectors/sentinel_hub/#terrakit.download.data_connectors.sentinelhub.SentinelHub.get_data","title":"<code>get_data</code>","text":"<p>Fetches data from SentinelHub for the specified collection, date range, area, and bands.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>Name of the data collection to fetch data from.</p> required <code>date_start</code> <code>str</code> <p>Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest. Defaults to None.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to retrieve. Defaults to all bands.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover threshold (0-100). Defaults to 100.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Data connector specification. Defaults to None.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the output file. Defaults to None.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for temporary files. Defaults to '.'.</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>xarray</code> <code>Union[DataArray, None]</code> <p>An xarray Datasets containing the fetched data with dimensions (time, band, y, x).</p> <p>Raises:</p> Type Description <code>TerrakitValidationError</code> <p>If a validation error occurs.</p> <code>TerrakitValueError</code> <p>If a value error occurs.</p> Source code in <code>terrakit/download/data_connectors/sentinelhub.py</code> <pre><code>def get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n) -&gt; Union[xr.DataArray, None]:\n    \"\"\"\n    Fetches data from SentinelHub for the specified collection, date range, area, and bands.\n\n    Parameters:\n        data_collection_name (str): Name of the data collection to fetch data from.\n        date_start (str): Start date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        date_end (str): End date for the data retrieval (inclusive), in 'YYYY-MM-DD' format.\n        area_polygon (list, optional): Polygon defining the area of interest. Defaults to None.\n        bbox (list, optional): Bounding box defining the area of interest. Defaults to None.\n        bands (list, optional): List of bands to retrieve. Defaults to all bands.\n        maxcc (int, optional): Maximum cloud cover threshold (0-100). Defaults to 100.\n        data_connector_spec (dict, optional): Data connector specification. Defaults to None.\n        save_file (str, optional): Path to save the output file. Defaults to None.\n        working_dir (str, optional): Working directory for temporary files. Defaults to '.'.\n\n    Returns:\n        xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n\n    Raises:\n        TerrakitValidationError: If a validation error occurs.\n        TerrakitValueError: If a value error occurs.\n    \"\"\"\n    # Check credentials have been set correctly.\n    if \"SH_CLIENT_ID\" not in os.environ and \"SH_CLIENT_SECRET\" not in os.environ:\n        error_msg = \"Error: Missing credentials 'SH_CLIENT_ID' and 'SH_CLIENT_SECRET'. Please update .env with correct credentials.\"\n        logger.warning(error_msg)\n        raise TerrakitValidationError(error_msg)\n\n    # Check data_collection_name exists in self.collections.\n    if data_collection_name not in self.collections:\n        error_msg = f\"Invalid collection '{data_collection_name}'. Please choose from one of the following collection {self.collections}\"\n        logger.warning(error_msg)\n        raise TerrakitValueError(error_msg)\n\n    logger.info(bands)\n    collection_details = [\n        X\n        for X in self.collections_details\n        if X[\"collection_name\"] == data_collection_name\n    ][0]\n\n    unique_dates, res = self.find_data(\n        data_collection_name, date_start, date_end, bbox=bbox, maxcc=maxcc\n    )\n\n    # Check that unique dates and find_data results are not None.\n    if unique_dates is None and res is None:\n        logger.warning(\"Warning: Unique dates and find_data results are None\")\n        return None\n\n    if unique_dates == []:\n        logger.warning(\n            f\"No data found for the specified date range {date_start}:{date_end}. Unique dates: {unique_dates}\"\n        )\n        return None\n    da_list = []\n    logger.info(f\"The following unique dates were found: {unique_dates}\")\n    for udate in unique_dates:  # type: ignore[union-attr]\n        usave_file = (\n            save_file.replace(\".tif\", f\"_{udate}.tif\")\n            if save_file is not None\n            else None\n        )\n\n        da: xr.DataArray = sh_get_data(\n            self.sh_config,\n            collection_details,\n            bbox,\n            udate,\n            bands,\n            usave_file,\n            sh_data_dir=f\"{working_dir}/sh_data\",\n        )\n\n        da_list.append(da)\n\n    logger.info(\"Concatenating data...\")\n    da = xr.concat(da_list, dim=\"time\")\n\n    # Save to file\n    save_data_array_to_file(da, save_file)\n\n    sh_data_dir = f\"{working_dir}/sh_data\"\n    logging.info(f\"Removing dir {sh_data_dir}\")\n    shutil.rmtree(sh_data_dir, ignore_errors=True)\n\n    return da\n</code></pre>"},{"location":"api/data_connectors/sentinelaws/","title":"Sentinel AWS Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.download.data_connectors.sentinel_aws</code> data connector module.</p>"},{"location":"api/data_connectors/sentinelaws/#terrakit.download.data_connectors.sentinel_aws","title":"<code>terrakit.download.data_connectors.sentinel_aws</code>","text":""},{"location":"api/data_connectors/sentinelaws/#terrakit.download.data_connectors.sentinel_aws.Sentinel_AWS","title":"<code>Sentinel_AWS</code>","text":"<p>               Bases: <code>Connector</code></p> <p>Class for interacting with Sentinel AWS data via STAC API.</p> <p>Attributes:</p> Name Type Description <code>connector_type</code> <code>str</code> <p>Type of data connector, always \"sentinel_aws\".</p> <code>stac_url</code> <code>str</code> <p>Base URL for the STAC API.</p> <code>collections</code> <code>list</code> <p>List of available collections.</p> <code>collections_details</code> <code>dict</code> <p>Detailed information about collections.</p> Source code in <code>terrakit/download/data_connectors/sentinel_aws.py</code> <pre><code>class Sentinel_AWS(Connector):\n    \"\"\"\n    Class for interacting with Sentinel AWS data via STAC API.\n\n    Attributes:\n        connector_type (str): Type of data connector, always \"sentinel_aws\".\n        stac_url (str): Base URL for the STAC API.\n        collections (list): List of available collections.\n        collections_details (dict): Detailed information about collections.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize Sentinel_AWS class with default attributes.\n        \"\"\"\n        self.connector_type = \"sentinel_aws\"\n        self.stac_url = \"https://earth-search.aws.element84.com/v1/\"\n        self.collections: list[Any] = load_and_list_collections(\n            connector_type=\"sentinel_aws\"\n        )\n        self.collections_details = load_and_list_collections(\n            as_json=True, connector_type=\"sentinel_aws\"\n        )\n\n    def list_collections(self) -&gt; list[Any]:\n        \"\"\"\n        List available collections.\n\n        Returns:\n            list: List of available collections.\n        \"\"\"\n\n        logger.info(\"Listing available collections\")\n        return self.collections\n\n    def find_data(\n        self,\n        data_collection_name: str,\n        date_start: str,\n        date_end: str,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n    ) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n        \"\"\"\n        Find Sentinel AWS data based on given parameters.\n\n        Args:\n            data_collection_name (str): Name of the data collection.\n            date_start (str): Start date in 'YYYY-MM-DD' format.\n            date_end (str): End date in 'YYYY-MM-DD' format.\n            bands (list): List of bands to fetch.\n            area_polygon (list, optional): Polygon defining the area of interest.\n            bbox (list, optional): Bounding box defining the area of interest.\n            maxcc (int, optional): Maximum cloud cover percentage.\n            data_connector_spec (dict, optional): Additional data connector specifications.\n\n        Returns:\n            tuple: A tuple containing unique dates and STAC items.\n        \"\"\"\n        logger.info(\"Listing Sentinel AWS data\")\n\n        check_collection_exists(data_collection_name, self.collections)\n\n        check_start_end_date(date_start=date_start, date_end=date_end)\n        check_area_polygon(\n            area_polygon=area_polygon, connector_type=self.connector_type\n        )\n        check_bbox(bbox=bbox, connector_type=self.connector_type)\n\n        collection_detials = self._get_collection_info(data_collection_name)\n        fields = self._get_search_fields(collection_detials)\n\n        try:\n            unique_dates, stac_items = find_items(\n                self.stac_url,\n                bbox,\n                date_start,\n                date_end,\n                bands=bands,\n                collections=[data_collection_name],\n                limit=250,\n                maxcc=maxcc,\n                data_connector_spec=data_connector_spec,\n                fields=fields,\n            )\n\n        except ValueError as e:\n            error_msg = f\"Unable to find data for collection '{data_collection_name}. This could be due to the parameters set:\\n\\t bbox={bbox}, start_date={date_start}, end_date={date_end}, collection={data_collection_name}, fields={fields}, max_cc={maxcc}.\"\n            logger.exception(error_msg)\n            raise TerrakitValueError(error_msg) from e\n\n        stac_items = [\n            {\"id\": item.id, \"properties\": item.properties} for item in stac_items\n        ]\n\n        return unique_dates, stac_items\n\n    def get_data(\n        self,\n        data_collection_name,\n        date_start,\n        date_end,\n        area_polygon=None,\n        bbox=None,\n        bands=[],\n        maxcc=100,\n        data_connector_spec=None,\n        save_file=None,\n        working_dir=\".\",\n    ) -&gt; Union[xr.DataArray, None]:\n        \"\"\"\n        Get Sentinel AWS data based on given parameters.\n\n        Args:\n            data_collection_name (str): Name of the data collection.\n            date_start (str): Start date in 'YYYY-MM-DD' format.\n            date_end (str): End date in 'YYYY-MM-DD' format.\n            area_polygon (list, optional): Polygon defining the area of interest.\n            bbox (list, optional): Bounding box defining the area of interest.\n            bands (list, optional): List of bands to retrieve.\n            maxcc (int, optional): Maximum cloud cover percentage.\n            data_connector_spec (dict, optional): Additional data connector specifications.\n            save_file (str, optional): Path to save the data.\n            working_dir (str, optional): Working directory for saving files.\n\n        Returns:\n            xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n        \"\"\"\n        check_collection_exists(data_collection_name, self.collections)\n        # Check that the bands the user has requested exist in the data collection\n        check_bands(\n            connector_type=self.connector_type,\n            collection_name=data_collection_name,\n            bands=bands,\n        )\n\n        if data_connector_spec is None:\n            data_connector_spec_list = [\n                X\n                for X in self.collections_details\n                if X[\"collection_name\"] == data_collection_name\n            ]\n            if len(data_connector_spec_list) == 0:\n                error_msg = (\n                    f\"Unable to find collection details for '{data_collection_name}'\"\n                )\n                logger.error(error_msg)\n                raise TerrakitValueError(error_msg)\n            data_connector_spec = data_connector_spec_list[0]\n\n        try:\n            unique_dates, results = self.find_data(\n                data_collection_name=data_collection_name,\n                date_start=date_start,\n                date_end=date_end,\n                bbox=bbox,\n                bands=bands,\n                maxcc=maxcc,\n                data_connector_spec=data_connector_spec,\n            )\n        except TerrakitValueError as e:\n            raise e\n\n        da_list: list[Any] = []\n        for date in unique_dates:  # type: ignore[union-attr]\n            da: xr.DataArray = get_sh_aws_data(\n                self.stac_url,\n                bbox,\n                date_start,\n                date_end,\n                bands=bands,\n                collections=[data_collection_name],\n                limit=250,\n                maxcc=maxcc,\n                data_connector_spec=data_connector_spec,\n            )\n            date_time_stamp = datetime.strptime(date, \"%Y-%m-%d\")\n            da = da.assign_coords({\"band\": bands, \"time\": date_time_stamp})\n            da_list.append(da)\n\n        da = xr.concat(da_list, dim=\"time\")\n        save_data_array_to_file(da, save_file)\n\n        return da\n\n    def _get_collection_info(self, collection_name) -&gt; dict[str, Any]:\n        collection_info = {}\n        for i, collections_details in enumerate(self.collections_details):\n            if collections_details[\"collection_name\"] == collection_name:\n                collection_info = self.collections_details[i]\n        return collection_info\n\n    def _get_search_fields(self, collection_info: dict[str, Any]) -&gt; str:\n        fields = \"{}\"\n        if \"search\" in collection_info:\n            if \"fields\" in collection_info[\"search\"]:\n                fields = collection_info[\"search\"][\"fields\"]\n        if type(fields) is not str:\n            err_msg = f\"'fields' value in collections.json must be a str, not {type(fields)}: {fields}\"\n            raise TerrakitValueError(err_msg)\n        return fields\n</code></pre>"},{"location":"api/data_connectors/sentinelaws/#terrakit.download.data_connectors.sentinel_aws.Sentinel_AWS.list_collections","title":"<code>list_collections</code>","text":"<p>List available collections.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[Any]</code> <p>List of available collections.</p> Source code in <code>terrakit/download/data_connectors/sentinel_aws.py</code> <pre><code>def list_collections(self) -&gt; list[Any]:\n    \"\"\"\n    List available collections.\n\n    Returns:\n        list: List of available collections.\n    \"\"\"\n\n    logger.info(\"Listing available collections\")\n    return self.collections\n</code></pre>"},{"location":"api/data_connectors/sentinelaws/#terrakit.download.data_connectors.sentinel_aws.Sentinel_AWS.find_data","title":"<code>find_data</code>","text":"<p>Find Sentinel AWS data based on given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>Name of the data collection.</p> required <code>date_start</code> <code>str</code> <p>Start date in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>End date in 'YYYY-MM-DD' format.</p> required <code>bands</code> <code>list</code> <p>List of bands to fetch.</p> <code>[]</code> <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest.</p> <code>None</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Additional data connector specifications.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]</code> <p>A tuple containing unique dates and STAC items.</p> Source code in <code>terrakit/download/data_connectors/sentinel_aws.py</code> <pre><code>def find_data(\n    self,\n    data_collection_name: str,\n    date_start: str,\n    date_end: str,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n) -&gt; Union[tuple[list[Any], list[dict[str, Any]]], tuple[None, None]]:\n    \"\"\"\n    Find Sentinel AWS data based on given parameters.\n\n    Args:\n        data_collection_name (str): Name of the data collection.\n        date_start (str): Start date in 'YYYY-MM-DD' format.\n        date_end (str): End date in 'YYYY-MM-DD' format.\n        bands (list): List of bands to fetch.\n        area_polygon (list, optional): Polygon defining the area of interest.\n        bbox (list, optional): Bounding box defining the area of interest.\n        maxcc (int, optional): Maximum cloud cover percentage.\n        data_connector_spec (dict, optional): Additional data connector specifications.\n\n    Returns:\n        tuple: A tuple containing unique dates and STAC items.\n    \"\"\"\n    logger.info(\"Listing Sentinel AWS data\")\n\n    check_collection_exists(data_collection_name, self.collections)\n\n    check_start_end_date(date_start=date_start, date_end=date_end)\n    check_area_polygon(\n        area_polygon=area_polygon, connector_type=self.connector_type\n    )\n    check_bbox(bbox=bbox, connector_type=self.connector_type)\n\n    collection_detials = self._get_collection_info(data_collection_name)\n    fields = self._get_search_fields(collection_detials)\n\n    try:\n        unique_dates, stac_items = find_items(\n            self.stac_url,\n            bbox,\n            date_start,\n            date_end,\n            bands=bands,\n            collections=[data_collection_name],\n            limit=250,\n            maxcc=maxcc,\n            data_connector_spec=data_connector_spec,\n            fields=fields,\n        )\n\n    except ValueError as e:\n        error_msg = f\"Unable to find data for collection '{data_collection_name}. This could be due to the parameters set:\\n\\t bbox={bbox}, start_date={date_start}, end_date={date_end}, collection={data_collection_name}, fields={fields}, max_cc={maxcc}.\"\n        logger.exception(error_msg)\n        raise TerrakitValueError(error_msg) from e\n\n    stac_items = [\n        {\"id\": item.id, \"properties\": item.properties} for item in stac_items\n    ]\n\n    return unique_dates, stac_items\n</code></pre>"},{"location":"api/data_connectors/sentinelaws/#terrakit.download.data_connectors.sentinel_aws.Sentinel_AWS.get_data","title":"<code>get_data</code>","text":"<p>Get Sentinel AWS data based on given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>data_collection_name</code> <code>str</code> <p>Name of the data collection.</p> required <code>date_start</code> <code>str</code> <p>Start date in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str</code> <p>End date in 'YYYY-MM-DD' format.</p> required <code>area_polygon</code> <code>list</code> <p>Polygon defining the area of interest.</p> <code>None</code> <code>bbox</code> <code>list</code> <p>Bounding box defining the area of interest.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of bands to retrieve.</p> <code>[]</code> <code>maxcc</code> <code>int</code> <p>Maximum cloud cover percentage.</p> <code>100</code> <code>data_connector_spec</code> <code>dict</code> <p>Additional data connector specifications.</p> <code>None</code> <code>save_file</code> <code>str</code> <p>Path to save the data.</p> <code>None</code> <code>working_dir</code> <code>str</code> <p>Working directory for saving files.</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>xarray</code> <code>Union[DataArray, None]</code> <p>An xarray Datasets containing the fetched data with dimensions (time, band, y, x).</p> Source code in <code>terrakit/download/data_connectors/sentinel_aws.py</code> <pre><code>def get_data(\n    self,\n    data_collection_name,\n    date_start,\n    date_end,\n    area_polygon=None,\n    bbox=None,\n    bands=[],\n    maxcc=100,\n    data_connector_spec=None,\n    save_file=None,\n    working_dir=\".\",\n) -&gt; Union[xr.DataArray, None]:\n    \"\"\"\n    Get Sentinel AWS data based on given parameters.\n\n    Args:\n        data_collection_name (str): Name of the data collection.\n        date_start (str): Start date in 'YYYY-MM-DD' format.\n        date_end (str): End date in 'YYYY-MM-DD' format.\n        area_polygon (list, optional): Polygon defining the area of interest.\n        bbox (list, optional): Bounding box defining the area of interest.\n        bands (list, optional): List of bands to retrieve.\n        maxcc (int, optional): Maximum cloud cover percentage.\n        data_connector_spec (dict, optional): Additional data connector specifications.\n        save_file (str, optional): Path to save the data.\n        working_dir (str, optional): Working directory for saving files.\n\n    Returns:\n        xarray: An xarray Datasets containing the fetched data with dimensions (time, band, y, x).\n    \"\"\"\n    check_collection_exists(data_collection_name, self.collections)\n    # Check that the bands the user has requested exist in the data collection\n    check_bands(\n        connector_type=self.connector_type,\n        collection_name=data_collection_name,\n        bands=bands,\n    )\n\n    if data_connector_spec is None:\n        data_connector_spec_list = [\n            X\n            for X in self.collections_details\n            if X[\"collection_name\"] == data_collection_name\n        ]\n        if len(data_connector_spec_list) == 0:\n            error_msg = (\n                f\"Unable to find collection details for '{data_collection_name}'\"\n            )\n            logger.error(error_msg)\n            raise TerrakitValueError(error_msg)\n        data_connector_spec = data_connector_spec_list[0]\n\n    try:\n        unique_dates, results = self.find_data(\n            data_collection_name=data_collection_name,\n            date_start=date_start,\n            date_end=date_end,\n            bbox=bbox,\n            bands=bands,\n            maxcc=maxcc,\n            data_connector_spec=data_connector_spec,\n        )\n    except TerrakitValueError as e:\n        raise e\n\n    da_list: list[Any] = []\n    for date in unique_dates:  # type: ignore[union-attr]\n        da: xr.DataArray = get_sh_aws_data(\n            self.stac_url,\n            bbox,\n            date_start,\n            date_end,\n            bands=bands,\n            collections=[data_collection_name],\n            limit=250,\n            maxcc=maxcc,\n            data_connector_spec=data_connector_spec,\n        )\n        date_time_stamp = datetime.strptime(date, \"%Y-%m-%d\")\n        da = da.assign_coords({\"band\": bands, \"time\": date_time_stamp})\n        da_list.append(da)\n\n    da = xr.concat(da_list, dim=\"time\")\n    save_data_array_to_file(da, save_file)\n\n    return da\n</code></pre>"},{"location":"api/data_connectors/theweathercompany/","title":"The Weather Company Data Connector Documentation","text":"<p>Documentation for the <code>terrakit.download.data_connectors.theweathercompany</code> data connector module</p>"},{"location":"api/data_connectors/theweathercompany/#terrakit.download.data_connectors.theweathercompany","title":"<code>terrakit.download.data_connectors.theweathercompany</code>","text":""},{"location":"api/download_transformations/transformations/","title":"Download Transformations Documentation","text":"<p>Documentation for the <code>terrakit.download.transformations.*</code> functions</p>"},{"location":"api/download_transformations/transformations/#terrakit.download.transformations.impute_nans_xarray.impute_nans_xarray","title":"<code>impute_nans_xarray(da: DataArray, nodata_value=-9999) -&gt; DataArray</code>","text":"<p>Impute NaN values in an xarray DataArray using nearest neighbor interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>nodata_value</code> <code>int</code> <p>The value representing missing data.</p> <code>-9999</code> <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If an error occurs during transformation.</p> <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: The imputed DataArray.</p> Source code in <code>terrakit/download/transformations/impute_nans_xarray.py</code> <pre><code>def impute_nans_xarray(da: DataArray, nodata_value=-9999) -&gt; DataArray:\n    \"\"\"\n    Impute NaN values in an xarray DataArray using nearest neighbor interpolation.\n\n    Parameters:\n        da (xarray.DataArray): The input DataArray.\n        nodata_value (int): The value representing missing data.\n\n    Raises:\n        TerrakitBaseException: If an error occurs during transformation.\n\n    Returns:\n        xarray.DataArray: The imputed DataArray.\n    \"\"\"\n    logger.info(\"Imputing NaNs in xarray data array.\")\n    try:\n        total_nodata_pixels = np.count_nonzero(da.isin([nodata_value]))\n        if total_nodata_pixels &gt; 0:\n            for d in range(0, da.shape[0]):\n                for i in range(0, len(da[\"band\"])):\n                    slice_dims = da[d, i] if da.ndim == 4 else da[i]\n                    interpolated = slice_dims.rio.interpolate_na(\"nearest\")\n                    if da.ndim == 4:\n                        da[d, i, :, :] = interpolated\n                    else:\n                        da[i, :, :] = interpolated\n        else:\n            logger.info(\"Skipping imputation as no nodata pixels found\")\n    except Exception as e:\n        err_msg = f\"An error occurred running 'impute_nans_xarray' with '{nodata_value=}': {e}\"\n        logger.error(err_msg)\n        raise TerrakitBaseException(err_msg)\n\n    return da\n</code></pre>"},{"location":"api/download_transformations/transformations/#terrakit.download.transformations.scale_data_xarray.scale_data_xarray","title":"<code>scale_data_xarray(da: DataArray, scaling_factors: list) -&gt; DataArray</code>","text":"<p>Scale the values in an xarray DataArray by given scaling factors.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>scaling_factors</code> <code>list</code> <p>A list of scaling factors corresponding to each band.</p> required <p>Raises:</p> Type Description <code>TerrakitBaseException</code> <p>If an error occurs during transformation.</p> <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: The scaled DataArray.</p> Source code in <code>terrakit/download/transformations/scale_data_xarray.py</code> <pre><code>def scale_data_xarray(da: DataArray, scaling_factors: list) -&gt; DataArray:\n    \"\"\"\n    Scale the values in an xarray DataArray by given scaling factors.\n\n    Parameters:\n        da (xarray.DataArray): The input DataArray.\n        scaling_factors (list): A list of scaling factors corresponding to each band.\n\n    Raises:\n        TerrakitBaseException: If an error occurs during transformation.\n\n    Returns:\n        xarray.DataArray: The scaled DataArray.\n    \"\"\"\n    try:\n        for b in range(0, len(scaling_factors)):\n            da[:, b] = da[:, b] * scaling_factors[b]\n    except Exception as e:\n        err_msg = f\"An error occuring running 'scale_data_xarray' with '{scaling_factors=}': {e}\"\n        logger.error(err_msg)\n        raise TerrakitBaseException(err_msg)\n    return da\n</code></pre>"},{"location":"api/utils/geodata_utils/","title":"Download Utils Documentation","text":"<p>Documentation for the <code>terrakit.download.geodata_utils</code> module.</p>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils","title":"<code>terrakit.download.geodata_utils</code>","text":""},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.list_data_connectors","title":"<code>list_data_connectors(as_json: bool = False) -&gt; Union[list, Dict[str, Any], Any]</code>","text":"<p>List available data connectors.</p> <p>Parameters:</p> Name Type Description Default <code>as_json</code> <code>bool</code> <p>If True, return data connectors as a JSON object, otherwise return a list of connector names.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[list, Dict[str, Any], Any]</code> <p>Union[list, Dict[str, Any], Any]: List of connector names or JSON object containing all connector specifications.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def list_data_connectors(as_json: bool = False) -&gt; Union[list, Dict[str, Any], Any]:\n    \"\"\"\n    List available data connectors.\n\n    Parameters:\n        as_json (bool): If True, return data connectors as a JSON object, otherwise return a list of connector names.\n\n    Returns:\n        Union[list, Dict[str, Any], Any]: List of connector names or JSON object containing all connector specifications.\n    \"\"\"\n    location = os.path.dirname(os.path.realpath(__file__))\n    file_path = os.path.join(location, \"collections.json\")\n    with open(file_path, \"r\") as file:\n        data_connector_spec_all = json.load(file)\n\n    if as_json is True:\n        return data_connector_spec_all\n    else:\n        return list(set([X[\"connector\"] for X in data_connector_spec_all]))\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.load_and_list_collections","title":"<code>load_and_list_collections(as_json: bool = False, connector_type: Union[str, None] = None) -&gt; Union[list, Dict[str, Any]]</code>","text":"<p>Load and list collections for a given data connector type.</p> <p>Parameters:</p> Name Type Description Default <code>as_json</code> <code>bool</code> <p>If True, return collection details as a JSON object, otherwise return a list of collection names.</p> <code>False</code> <code>connector_type</code> <code>str</code> <p>The type of data connector to filter collections by.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[list, Dict[str, Any]]</code> <p>Union[list, Dict[str, Any]]: List of collection names or JSON object containing collection specifications.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def load_and_list_collections(\n    as_json: bool = False, connector_type: Union[str, None] = None\n) -&gt; Union[list, Dict[str, Any]]:\n    \"\"\"\n    Load and list collections for a given data connector type.\n\n    Parameters:\n        as_json (bool): If True, return collection details as a JSON object, otherwise return a list of collection names.\n        connector_type (str): The type of data connector to filter collections by.\n\n    Returns:\n        Union[list, Dict[str, Any]]: List of collection names or JSON object containing collection specifications.\n    \"\"\"\n    logger.info(connector_type)\n    location = os.path.dirname(os.path.realpath(__file__))\n    file_path = os.path.join(location, \"collections.json\")\n    with open(file_path, \"r\") as file:\n        data_connector_spec_all = json.load(file)\n    if connector_type is not None:\n        connector_collections = [\n            X for X in data_connector_spec_all if X[\"connector\"] == connector_type\n        ]\n    else:\n        connector_collections = data_connector_spec_all\n    if as_json is True:\n        return connector_collections\n    else:\n        return [X[\"collection_name\"] for X in connector_collections]\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.check_bands","title":"<code>check_bands(connector_type: str, collection_name: str, bands: list)</code>","text":"<p>Check if the specified bands are available for a given collection and connector type.</p> <p>Parameters:</p> Name Type Description Default <code>connector_type</code> <code>str</code> <p>The type of data connector.</p> required <code>collection_name</code> <code>str</code> <p>The name of the collection.</p> required <code>bands</code> <code>list</code> <p>List of band names to check.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>Modified list of bands with any unavailable bands replaced by their alternative names if possible.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def check_bands(connector_type: str, collection_name: str, bands: list):\n    \"\"\"\n    Check if the specified bands are available for a given collection and connector type.\n\n    Parameters:\n        connector_type (str): The type of data connector.\n        collection_name (str): The name of the collection.\n        bands (list): List of band names to check.\n\n    Returns:\n        list: Modified list of bands with any unavailable bands replaced by their alternative names if possible.\n    \"\"\"\n    all_collections = load_and_list_collections(\n        as_json=True, connector_type=connector_type\n    )\n\n    collection_details_list: list[Any] = []\n    for X in all_collections:\n        if X[\"collection_name\"] == collection_name:  # type: ignore\n            collection_details_list.append(X)\n\n    if len(collection_details_list) == 0:\n        error_msg = f\"Unable to find collection details for '{connector_type}'\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n    collection_details = collection_details_list[0]\n    available_bands = [X[\"band_name\"] for X in collection_details[\"bands\"]]\n    alternative_bands = [X[\"alt_names\"] for X in collection_details[\"bands\"]]\n    alternative_bands = [item for sublist in alternative_bands for item in sublist]\n\n    for i, b in enumerate(bands):\n        if b in available_bands:\n            logger.info(f\"Band {b} available \\u2714\")\n            new_band = [\n                X[\"band_name\"]\n                for X in collection_details[\"bands\"]\n                if b in X[\"alt_names\"]\n            ]\n            logger.info(new_band)\n        else:\n            logger.info(f\"Band {b} unavailable \\u274c\")\n            new_band = [\n                X[\"band_name\"]\n                for X in collection_details[\"bands\"]\n                if b in X[\"alt_names\"]\n            ]\n            if len(new_band) &gt; 0:\n                logger.info(f\"Alterative name found and used: {new_band[0]}\")\n                bands[i] = new_band[0]\n            else:\n                logger.info(f\"Bands to choose from: {available_bands}\")\n                break\n\n    return bands\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.polygon_to_bbox","title":"<code>polygon_to_bbox(polygon, buffer_size)</code>","text":"<p>Convert a Shapely Polygon to a bounding box with a buffer zone.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>Polygon</code> <p>The input polygon.</p> required <code>buffer_size</code> <code>float</code> <p>The size of the buffer zone in the same units as the polygon's CRS.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>float</code> <p>Bounding box [min_lon, min_lat, max_lon, max_lat] with buffer zone.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def polygon_to_bbox(polygon, buffer_size):\n    \"\"\"\n    Convert a Shapely Polygon to a bounding box with a buffer zone.\n\n    Parameters:\n        polygon (shapely.geometry.Polygon): The input polygon.\n        buffer_size (float): The size of the buffer zone in the same units as the polygon's CRS.\n\n    Returns:\n        list(float): Bounding box [min_lon, min_lat, max_lon, max_lat] with buffer zone.\n    \"\"\"\n    bbox = shape(polygon).bounds\n    bbox = list(bbox)\n    bbox[0] = bbox[0] - buffer_size\n    bbox[1] = bbox[1] - buffer_size\n    bbox[2] = bbox[2] + buffer_size\n    bbox[3] = bbox[3] + buffer_size\n    return bbox\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.calculate_resolution","title":"<code>calculate_resolution(meter_resolution, lat)</code>","text":"<p>Calculate the spatial resolution in latitude and longitude for a given meter resolution at a specific latitude.</p> <p>Parameters:</p> Name Type Description Default <code>meter_resolution</code> <code>float</code> <p>The desired resolution in meters.</p> required <code>lat</code> <code>float</code> <p>The latitude for which to calculate the resolution.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>(float, float)</code> <p>Resolution in latitude and longitude.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def calculate_resolution(meter_resolution, lat):\n    \"\"\"\n    Calculate the spatial resolution in latitude and longitude for a given meter resolution at a specific latitude.\n\n    Parameters:\n        meter_resolution (float): The desired resolution in meters.\n        lat (float): The latitude for which to calculate the resolution.\n\n    Returns:\n        tuple(float, float): Resolution in latitude and longitude.\n    \"\"\"\n    # Get length of degrees\n    lat_rad = math.radians(lat)\n    # Calculate the length of one degree in latitude considering the ellipticity of the earth\n    lat_degree_length = (\n        111132.954 - 559.822 * math.cos(2 * lat_rad) + 1.175 * math.cos(4 * lat_rad)\n    )\n    # Calculate the length of one degree in longitude based on the latitude and the earth radius\n    lon_degree_length = (math.pi / 180) * math.cos(lat_rad) * 6378137.0\n    # Get resolution\n    resolution_lat = meter_resolution / lat_degree_length\n    resolution_lon = meter_resolution / lon_degree_length\n\n    return resolution_lat, resolution_lon\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.verify_input_image","title":"<code>verify_input_image(image, standard_dimensions=224) -&gt; typing.Tuple[int, str]</code>","text":"<p>Verify input dimensions for supplied image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>bytes</code> <p>image</p> required <code>standard_dimensions</code> <code>int</code> <p>expected size of image</p> <code>224</code> Return <p>tuple[int, str]: [verification_status_code, verification_msg]</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def verify_input_image(image, standard_dimensions=224) -&gt; typing.Tuple[int, str]:\n    \"\"\"\n    Verify input dimensions for supplied image\n\n    Args:\n        image (bytes): image\n        standard_dimensions (int): expected size of image\n\n    Return:\n        tuple[int, str]: [verification_status_code, verification_msg]\n    \"\"\"\n    res = os.popen(f\"gdalinfo {image} -json\").read()\n    res_json = json.loads(res)\n    dims = res_json[\"size\"]\n\n    # Check if image is geotiff\n    if res_json[\"driverShortName\"] != \"GTiff\":\n        return 1007, f\"Input {image} is not a GeoTiff.\"\n\n    # Check image dimensions\n    image_input_dimensions = np.min(dims)\n\n    if image_input_dimensions &lt; standard_dimensions:\n        return (\n            1002,\n            f\"Input image too small for image {image} with dimensions {image_input_dimensions}. Both dimensions must be &gt;= 224.\",\n        )\n    else:\n        # Log/Show dimensions of the input image\n        logger.debug(f\"Input image {image} has dimensions {image_input_dimensions}\")\n        return 200, str(image_input_dimensions)\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.check_projection","title":"<code>check_projection(file)</code>","text":"<p>Check the projection is correct, if not reproject to EPSG:4326</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the input file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def check_projection(file):\n    \"\"\"\n    Check the projection is correct, if not reproject to EPSG:4326\n\n    Parameters:\n        file (str): The path to the input file.\n\n    Returns:\n        None\n    \"\"\"\n    res = os.popen(f\"gdalinfo {file} -proj4 -json\").read()\n    res_json = json.loads(res)\n    # WGS84 is the same as EPSG:4326\n    if res_json[\"stac\"][\"proj:epsg\"] != 4326:\n        os.system(f\"gdalwarp {file} -t_srs EPSG:4326 {file}_reprojected.tif\")\n        os.system(f\"mv {file}_reprojected.tif {file} \")\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.pad_bbox","title":"<code>pad_bbox(padding_degrees, bbox)</code>","text":"<p>Add padding to bounding box to help with edge artifacts. Args:     padding_degrees (float): number of degrees to add as border to bbox     bbox (list(float)): original bounding box [min_lon, min_lat, max_lon, max_lat] Return:     padded_bbox (list(float)): bouning box with border of padding [min_lon, min_lat, max_lon, max_lat]</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def pad_bbox(padding_degrees, bbox):\n    \"\"\"\n    Add padding to bounding box to help with edge artifacts.\n    Args:\n        padding_degrees (float): number of degrees to add as border to bbox\n        bbox (list(float)): original bounding box [min_lon, min_lat, max_lon, max_lat]\n    Return:\n        padded_bbox (list(float)): bouning box with border of padding [min_lon, min_lat, max_lon, max_lat]\n    \"\"\"\n    return [\n        bbox[0] - padding_degrees,\n        bbox[1] - padding_degrees,\n        bbox[2] + padding_degrees,\n        bbox[3] + padding_degrees,\n    ]\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.tile_bbox","title":"<code>tile_bbox(aoi_size, bbox, resolution, tile_size_x=2200.0, tile_size_y=2200.0)</code>","text":"<p>Tile a bounding box if it exceeds 2400 pixels in any dimension.</p> <p>Parameters:</p> Name Type Description Default <code>aoi_size</code> <code>tuple(float</code> <p>The size of the area of interest [width, height].</p> required <code>bbox</code> <code>list(float</code> <p>The original bounding box [min_lon, min_lat, max_lon, max_lat].</p> required <code>resolution</code> <code>float</code> <p>The spatial resolution.</p> required <code>tile_size_x</code> <code>float</code> <p>The desired width of each tile.</p> <code>2200.0</code> <code>tile_size_y</code> <code>float</code> <p>The desired height of each tile.</p> <code>2200.0</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing lists of tiled bounding boxes and their respective sizes.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def tile_bbox(aoi_size, bbox, resolution, tile_size_x=2200.0, tile_size_y=2200.0):\n    \"\"\"\n    Tile a bounding box if it exceeds 2400 pixels in any dimension.\n\n    Parameters:\n        aoi_size (tuple(float)): The size of the area of interest [width, height].\n        bbox (list(float)): The original bounding box [min_lon, min_lat, max_lon, max_lat].\n        resolution (float): The spatial resolution.\n        tile_size_x (float): The desired width of each tile.\n        tile_size_y (float): The desired height of each tile.\n\n    Returns:\n        tuple: A tuple containing lists of tiled bounding boxes and their respective sizes.\n    \"\"\"\n    numLon = math.floor(aoi_size[0] / tile_size_x)\n    numLat = math.floor(aoi_size[1] / tile_size_y)\n\n    lonStep = (bbox[2] - bbox[0]) * (tile_size_x / aoi_size[0])\n    latStep = (bbox[3] - bbox[1]) * (tile_size_y / aoi_size[1])\n\n    lons = [bbox[0] + (lonStep * X) for X in list(range(0, numLon + 1))] + [bbox[2]]\n    lats = [bbox[1] + (latStep * X) for X in list(range(0, numLat + 1))] + [bbox[3]]\n\n    aoi_bboxes = []\n    aoi_sizes = []\n\n    for x in range(0, numLon + 1):\n        for y in range(0, numLat + 1):\n            aoi_bbox = BBox(\n                bbox=[lons[x], lats[y], lons[x + 1], lats[y + 1]], crs=CRS.WGS84\n            )\n            aoi_bboxes = aoi_bboxes + [aoi_bbox]\n            aoi_sizes = aoi_sizes + [\n                bbox_to_dimensions(aoi_bbox, resolution=resolution)\n            ]\n\n    return aoi_bboxes, aoi_sizes\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.check_and_crop_bbox","title":"<code>check_and_crop_bbox(bbox, resolution)</code>","text":"<p>Check and crop a bounding box to ensure it fits within Sentinel Hub's processing limits.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list(float</code> <p>The original bounding box [min_lon, min_lat, max_lon, max_lat].</p> required <code>resolution</code> <code>float</code> <p>The spatial resolution.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the cropped bounding box and its size.</p> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def check_and_crop_bbox(bbox, resolution):\n    \"\"\"\n    Check and crop a bounding box to ensure it fits within Sentinel Hub's processing limits.\n\n    Parameters:\n        bbox (list(float)): The original bounding box [min_lon, min_lat, max_lon, max_lat].\n        resolution (float): The spatial resolution.\n\n    Returns:\n        tuple: A tuple containing the cropped bounding box and its size.\n    \"\"\"\n    # Check expected pixel size (Sentinel Hub is limited to 2500 pixel)\n    aoi_bbox = [BBox(bbox=bbox, crs=CRS.WGS84)]\n    aoi_size = [bbox_to_dimensions(aoi_bbox[0], resolution=resolution)]\n    if any(s &gt; 2400 for s in aoi_size[0]):\n        aoi_bbox, aoi_size = tile_bbox(aoi_size[0], bbox, resolution)\n\n    for i, b in enumerate(aoi_size):\n        if any(s &lt; 244 for s in b):\n            logger.info(f\"Dimension less than 244, will pad - {aoi_size[i]}\")\n            center_lon = aoi_bbox[i].middle[0]\n            center_lat = aoi_bbox[i].middle[1]\n            resolution_lat, resolution_lon = calculate_resolution(\n                meter_resolution=resolution, lat=center_lat\n            )\n            padding = int(224 / 2) + 50\n\n            new_bbox = list(aoi_bbox[i])\n\n            if aoi_size[i][0] &lt; 224:\n                # Add padding to the image\n                new_bbox[0] = center_lon - padding * resolution_lon\n                new_bbox[2] = center_lon + padding * resolution_lon\n            if aoi_size[i][1] &lt; 224:\n                new_bbox[1] = center_lat - padding * resolution_lat\n                new_bbox[3] = center_lat + padding * resolution_lat\n            aoi_bbox[i] = BBox(bbox=new_bbox, crs=CRS.WGS84)\n            aoi_size[i] = bbox_to_dimensions(aoi_bbox[i], resolution=resolution)\n            logger.info(f\"New dimensions are {aoi_size[i]}\")\n\n    return aoi_bbox, aoi_size\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.save_data_array_to_file","title":"<code>save_data_array_to_file(da, save_file, imputed=False) -&gt; None</code>","text":"<p>Save an xarray DataArray to a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>save_file</code> <code>str</code> <p>The path to save the DataArray.</p> required <code>imputed</code> <code>bool</code> <p>Whether the DataArray has been imputed.</p> <code>False</code> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def save_data_array_to_file(da, save_file, imputed=False) -&gt; None:\n    \"\"\"\n    Save an xarray DataArray to a GeoTIFF file.\n\n    Parameters:\n        da (xarray.DataArray): The input DataArray.\n        save_file (str): The path to save the DataArray.\n        imputed (bool): Whether the DataArray has been imputed.\n    \"\"\"\n    if save_file is not None:\n        if da.time is not None:\n            for i, t in enumerate(da.time.values):\n                date = t.astype(str)[:10]\n                if save_file.find(date) == -1:\n                    file_path = save_file.replace(\".tif\", f\"_{date}.tif\")\n                else:\n                    file_path = save_file\n                if imputed is True and \"imputed\" not in file_path:\n                    file_path = file_path.replace(\".tif\", \"_imputed.tif\")\n                save_cog(da.isel(time=i), file_path)\n        else:\n            logger.warning(\n                f\"Error saving file. Missing time dimension. Dimensions are: {da.dims}\"\n            )\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.save_cog","title":"<code>save_cog(ds, filename='cogeo.tif') -&gt; None</code>","text":"<p>Save an xarray Dataset as a Cloud Optimized GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input Dataset.</p> required <code>filename</code> <code>str</code> <p>The path and filename for the output GeoTIFF.</p> <code>'cogeo.tif'</code> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def save_cog(ds, filename=\"cogeo.tif\") -&gt; None:\n    \"\"\"\n    Save an xarray Dataset as a Cloud Optimized GeoTIFF.\n\n    Parameters:\n        ds (xarray.Dataset): The input Dataset.\n        filename (str): The path and filename for the output GeoTIFF.\n    \"\"\"\n    logger.info(f\"Saving cloud optimized geotiff to {filename}\")\n    ds.rio.to_raster(raster_path=filename, driver=\"COG\")\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.save_data_array_as_netcdf","title":"<code>save_data_array_as_netcdf(da: xr.DataArray, save_file: str | bool, **kwargs) -&gt; None</code>","text":"<p>Save an xarray DataArray as a NetCDF file.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The input DataArray.</p> required <code>save_file</code> <code>str</code> <p>The path to save the DataArray.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the to_netcdf method.</p> <code>{}</code> Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def save_data_array_as_netcdf(\n    da: xr.DataArray, save_file: str | bool, **kwargs\n) -&gt; None:\n    \"\"\"\n    Save an xarray DataArray as a NetCDF file.\n\n    Parameters:\n        da (xarray.DataArray): The input DataArray.\n        save_file (str): The path to save the DataArray.\n        **kwargs: Additional keyword arguments for the to_netcdf method.\n    \"\"\"\n    if not isinstance(save_file, str):\n        raise TypeError(\"Error! save_file should be a string\")\n    else:\n        da.to_netcdf(path=save_file, **kwargs)\n</code></pre>"},{"location":"api/utils/geodata_utils/#terrakit.download.geodata_utils.validate_input_params","title":"<code>validate_input_params(bbox: tuple | None, date_start: str | None, date_end: str | None) -&gt; None</code>","text":"<p>Validate input parameters for bounding box and date range.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>tuple | None</code> <p>The bounding box as a tuple (min_lon, min_lat, max_lon, max_lat).</p> required <code>date_start</code> <code>str | None</code> <p>The start date in 'YYYY-MM-DD' format.</p> required <code>date_end</code> <code>str | None</code> <p>The end date in 'YYYY-MM-DD' format.</p> required Source code in <code>terrakit/download/geodata_utils.py</code> <pre><code>def validate_input_params(\n    bbox: tuple | None, date_start: str | None, date_end: str | None\n) -&gt; None:\n    \"\"\"\n    Validate input parameters for bounding box and date range.\n\n    Parameters:\n        bbox (tuple | None): The bounding box as a tuple (min_lon, min_lat, max_lon, max_lat).\n        date_start (str | None): The start date in 'YYYY-MM-DD' format.\n        date_end (str | None): The end date in 'YYYY-MM-DD' format.\n    \"\"\"\n    if bbox is not None:\n        if not isinstance(bbox, (tuple, list)):\n            raise ValueError(\"Error! bbox should be either a tuple or list\")\n        else:\n            if len(bbox) != 4:\n                raise ValueError(\"Error! bbox must have 4 items\")\n            else:\n                west, south, east, north = bbox\n                if not (-180 &lt;= west &lt; east &lt;= 180 and -90 &lt;= south &lt; north &lt;= 90):\n                    raise ValueError(f\"Error! Invalid values in {bbox=}\")\n    if date_start is not None and date_end is not None:\n        if not isinstance(date_start, str) or not isinstance(date_end, str):\n            raise ValueError(\"Error! date_start and date_end should be a string\")\n        else:\n            start = pd.Timestamp(date_start)\n            end = pd.Timestamp(date_end)\n            if start &gt; end:\n                raise ValueError(f\"Error! {start=} cannot be after {end=}\")\n</code></pre>"},{"location":"api/utils/geospatial_utils/","title":"Documentation for Geospatial Utils","text":"<p>Documentation for the <code>terrakit.general_utils.geospatial_util</code> module.</p>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util","title":"<code>terrakit.general_utils.geospatial_util</code>","text":""},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.get_credentials_by_bucket","title":"<code>get_credentials_by_bucket</code>","text":"<p>get the credentials to access the specified bucket. This method maps the bucket to a cos instance, then it gets the credentials to access this instance</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>input bucket name</p> required <p>Returns:</p> Type Description <code>dict[str, Optional[str]]</code> <p>dict[str, str]: a dict that contains endpoint, access_key_id, secret_access_key, region, endpoint</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def get_credentials_by_bucket(bucket: str) -&gt; dict[str, Optional[str]]:\n    \"\"\"get the credentials to access the specified bucket. This method maps the bucket to a\n    cos instance, then it gets the credentials to access this instance\n\n    Parameters:\n        bucket (str): input bucket name\n\n    Returns:\n        dict[str, str]: a dict that contains endpoint, access_key_id, secret_access_key, region,\n            endpoint\n    \"\"\"\n    # make sure the bucket variable is valid\n    assert bucket is not None\n    assert isinstance(bucket, str)\n    # create the environment variable name, which is based on the bucket name\n    envvar = remove_invalid_characters(name=bucket)\n    endpoint_env_var_name = f\"{envvar}_ENDPOINT\".upper()\n    cos_instance_env_var_name = f\"{envvar}_INSTANCE\".upper()\n    # if these env variables are set, it means that credentials are required\n    if cos_instance_env_var_name in os.environ and endpoint_env_var_name in os.environ:\n        # get COS instance name\n        cos_instance = os.environ[cos_instance_env_var_name].upper()\n        cos_instance = remove_invalid_characters(name=cos_instance)\n        # get endpoint\n        endpoint = os.environ[endpoint_env_var_name]\n        # create env variable names based on COS instance name\n        access_key_id_env_var = f\"{cos_instance}_ACCESS_KEY_ID\"\n        secret_access_key_env_var = f\"{cos_instance}_SECRET_ACCESS_KEY\"\n        logger.info(\n            f\"Accessing env variables: {access_key_id_env_var=} {secret_access_key_env_var=}\"\n        )\n        try:\n            # get the credential values\n            access_key_id = os.getenv(access_key_id_env_var)\n            secret_access_key = os.getenv(secret_access_key_env_var)\n        except KeyError as e:\n            msg = f\"KeyError! At lmaximum_longitude one of these variables ({access_key_id_env_var=}, {secret_access_key_env_var=}), which grant access to the {bucket} bucket,  has not been set. Message={e}\"\n            logger.info(msg=msg)\n            raise KeyError(msg)\n        # get endpoint value\n    else:\n        # if the dataset does not require credentials\n        access_key_id = secret_access_key = endpoint = None\n    # grouping credentials as dict\n    credentials = {\n        \"access_key_id\": access_key_id,\n        \"secret_access_key\": secret_access_key,\n        \"endpoint\": endpoint,\n    }\n    return credentials\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.parse_region","title":"<code>parse_region</code>","text":"<p>extract region from endpoint</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>e.g., s3.us-minimum_latitude.cloud-object-storage.appdomain.cloud</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>region, e.g., us-minimum_latitude</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def parse_region(endpoint: str) -&gt; str:\n    \"\"\"extract region from endpoint\n\n    Parameters:\n        endpoint (str): e.g., s3.us-minimum_latitude.cloud-object-storage.appdomain.cloud\n\n    Returns:\n        str: region, e.g., us-minimum_latitude\n    \"\"\"\n    fields = endpoint.split(\".\")\n    assert len(fields) &gt; 0, f\"Error! Unexpected endpoint: {endpoint}\"\n    region = fields[1]\n    assert isinstance(region, str), f\"Error! Unexpected region type: {region=}\"\n    return region\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.get_xarray_coord","title":"<code>get_xarray_coord</code>","text":"<p>Retrieves the coordinate name associated with the given dimension from an xarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataArray</code> <p>The input xarray DataArray.</p> required <code>dimension</code> <code>str</code> <p>The dimension to search for in the coordinates.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The name of the coordinate associated with the given dimension, or None if not found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified dimension is not found in any of the coordinates.</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def get_xarray_coord(data: xr.DataArray, dimension: str) -&gt; str | None:\n    \"\"\"\n    Retrieves the coordinate name associated with the given dimension from an xarray DataArray.\n\n    Parameters:\n        data (xr.DataArray): The input xarray DataArray.\n        dimension (str): The dimension to search for in the coordinates.\n\n    Returns:\n        str | None: The name of the coordinate associated with the given dimension, or None if not found.\n\n    Raises:\n        ValueError: If the specified dimension is not found in any of the coordinates.\n    \"\"\"\n    # initialize variable\n    coord_name = None\n    # hardcoded values of longitude and latitude\n    longitude_list = [DEFAULT_X_DIMENSION, \"longitude\", \"lon\", \"long\"]\n    latitude_list = [DEFAULT_Y_DIMENSION, \"latitude\", \"lat\"]\n    # assumption: dimension must one of the hardcoded values\n    if dimension in longitude_list:\n        possible_values = longitude_list\n    elif dimension in latitude_list:\n        possible_values = latitude_list\n    else:\n        raise ValueError(f\"Error! Unable to find a coord that has {dimension=}\")\n\n    coordinates = list(data.coords.keys())\n    found = False\n    i = 0\n    while i &lt; len(coordinates) and not found:\n        coord = coordinates[i]\n        i += 1\n        coord_dims = list(data.coords[coord].dims)\n\n        if len(coord_dims) == 1 and dimension in coord_dims:\n            coord_name = str(coord)\n            found = True\n            break\n        elif (\n            len(coord_dims) &gt; 1 and dimension in coord_dims and coord in possible_values\n        ):\n            coord_name = str(coord)\n            found = True\n    return coord_name\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.clip_box","title":"<code>clip_box</code>","text":"<p>Clips an xarray DataArray to a bounding box.</p> <p>This function clips an xarray DataArray to a user-defined bounding box. It handles both linear and curvilinear coordinate systems.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataArray</code> <p>The input xarray DataArray to be clipped.</p> required <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>A tuple containing the bounding box coordinates (minx, miny, maxx, maxy).</p> required <code>x_dim</code> <code>str</code> <p>The name of the x-coordinate dimension.</p> required <code>y_dim</code> <code>str</code> <p>The name of the y-coordinate dimension.</p> required <code>crs</code> <code>Optional[int]</code> <p>The CRS (Coordinate Reference System) of the input data. Default is EPSG:4326.</p> <code>4326</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: The clipped xarray DataArray.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the bounding box coordinates are invalid (minx &gt; maxx or miny &gt; maxy).</p> <code>OneDimensionalRaster</code> <p>If the resulting DataArray has either x or y dimension of size 1.</p> <code>TypeError</code> <p>If the coordinates have more than one dimension and share a name with one of their dimensions.</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def clip_box(\n    data: xr.DataArray,\n    bbox: Tuple[float, float, float, float],\n    x_dim: str,\n    y_dim: str,\n    crs: Optional[int] = 4326,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Clips an xarray DataArray to a bounding box.\n\n    This function clips an xarray DataArray to a user-defined bounding box. It handles both linear and curvilinear coordinate systems.\n\n    Parameters:\n        data (xr.DataArray): The input xarray DataArray to be clipped.\n        bbox (Tuple[float, float, float, float]): A tuple containing the bounding box coordinates (minx, miny, maxx, maxy).\n        x_dim (str): The name of the x-coordinate dimension.\n        y_dim (str): The name of the y-coordinate dimension.\n        crs (Optional[int]): The CRS (Coordinate Reference System) of the input data. Default is EPSG:4326.\n\n    Returns:\n        xr.DataArray: The clipped xarray DataArray.\n\n    Raises:\n        ValueError: If the bounding box coordinates are invalid (minx &gt; maxx or miny &gt; maxy).\n        OneDimensionalRaster: If the resulting DataArray has either x or y dimension of size 1.\n        TypeError: If the coordinates have more than one dimension and share a name with one of their dimensions.\n    \"\"\"\n\n    # set CRS\n    if data.rio.crs is None:\n        input_crs = CRS.from_epsg(crs)\n        data.rio.write_crs(input_crs, inplace=True)\n    # area selected by the end-user\n    minx, miny, maxx, maxy = bbox\n    # get coords\n    x_coord = get_xarray_coord(data=data, dimension=x_dim)\n    assert x_coord is not None\n    y_coord = get_xarray_coord(data=data, dimension=y_dim)\n    assert y_coord is not None\n    # \"xarray disallows variables with more than 1 dimension that share a name with one of their\n    # dimensions to avoid conflicts and ambiguity when accessing data\". Thus, when coordinates\n    # have two dimensions, we rely on \"where()\" to clip the data\n    if any(c is not None and len(data.coords[c].dims) &gt; 1 for c in [x_coord, y_coord]):\n        data = _clip_curvilinear_raster(\n            data=data,\n            bbox=bbox,\n            x_coord=x_coord,\n            y_coord=y_coord,\n        )\n    else:\n        data = _rename_coords(\n            data=data, x_coord=x_coord, x_dim=x_dim, y_coord=y_coord, y_dim=y_dim\n        )\n        # clip_box works if coords and dims have the same name\n        rename_dict = dict()\n        if y_dim != DEFAULT_Y_DIMENSION:\n            rename_dict[y_dim] = DEFAULT_Y_DIMENSION\n        if x_dim != DEFAULT_X_DIMENSION:\n            rename_dict[x_dim] = DEFAULT_X_DIMENSION\n        if len(rename_dict) &gt; 0:\n            data = data.rename(rename_dict)\n\n        # adjust user input based on the limits of the data coordinates\n        minx = max(minx, min(data[DEFAULT_X_DIMENSION].values.flatten()))\n        maxx = min(maxx, max(data[DEFAULT_X_DIMENSION].values.flatten()))\n        if minx &gt; maxx:\n            msg = f\"Error! {minx=} &gt;= {maxx=}\"\n            raise ValueError(msg)\n        miny = max(miny, min(data[DEFAULT_Y_DIMENSION].values.flatten()))\n        maxy = min(maxy, max(data[DEFAULT_Y_DIMENSION].values.flatten()))\n        if miny &gt; maxy:\n            msg = f\"Error! {miny=} &gt;= {maxy=}\"\n            raise ValueError(msg)\n\n        try:\n            data = data.rio.clip_box(\n                minx=minx, miny=miny, maxx=maxx, maxy=maxy, crs=crs\n            )\n            # restore original dimension names\n            reversed_dict = {v: k for k, v in rename_dict.items()}\n            if len(reversed_dict) &gt; 0:\n                data = data.rename(reversed_dict)\n        except TypeError:\n            # handling the case when a given coord has multiple dimensions (curvilinear)\n            data = data.where(\n                (data.x &lt;= maxx)\n                &amp; (data.x &gt;= minx)\n                &amp; (data.y &lt;= maxy)\n                &amp; (data.y &gt;= miny),\n                drop=True,\n            )\n        except OneDimensionalRaster:\n            # handling exception when resulting dataarray has either x or y 1-size dimension\n\n            # assumption: coordinates are sorted\n            # get index of x that is smaller than minx\n            minx_index = bisect.bisect_left(a=data.x.values.flatten(), x=minx)\n            # get index of x that is greater than maxx\n            maxx_index = bisect.bisect_right(a=data.x.values.flatten(), x=maxx)\n            if minx_index == maxx_index:\n                if minx_index &gt; 0:\n                    minx_index -= 1\n                else:\n                    maxx_index += 1\n\n            # get index of y that is smaller than miny\n            miny_index = bisect.bisect_left(a=data.y.values.flatten(), x=miny)\n            # get index of y that is smaller than maxy\n            maxy_index = bisect.bisect_right(a=data.y.values.flatten(), x=maxy)\n            if miny_index == maxy_index:\n                if miny_index &gt; 0:\n                    miny_index -= 1\n                else:\n                    maxy_index += 1\n            selector = {\n                \"x\": slice(minx_index, maxx_index),\n                \"y\": slice(miny_index, maxy_index),\n            }\n\n            data = data.isel(selector)\n        # rename dimensions back to original\n        if not isinstance(data, xr.DataArray):\n            msg = f\"Error! Invalid data type: {type(data)}\"\n            raise ValueError(msg)\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.rename_vars","title":"<code>rename_vars</code>","text":"<p>Rename DEFAULT_TIME_DIMENSION to \"temp\"</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset</code> <p>data set to check variables</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: Returns the updated dataset</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def rename_vars(data: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Rename DEFAULT_TIME_DIMENSION to \"temp\"\n\n    Parameters:\n        data (xr.Dataset): data set to check variables\n\n    Returns:\n        xr.Dataset: Returns the updated dataset\n    \"\"\"\n    for var in data.variables:\n        if var == DEFAULT_TIME_DIMENSION:\n            data = data.rename_vars({var: \"temp\"})\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.expand_time_dimension","title":"<code>expand_time_dimension</code>","text":"<p>Expands the time dimension in the given xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset</code> <p>The input xarray Dataset.</p> required <code>time_dim</code> <code>str | None</code> <p>The name of the time dimension to expand. If None, no expansion is performed.</p> required <code>dt</code> <code>str | None</code> <p>A string representing a date-time in the format 'YYYY-MM-DD HH:MM:SS'. If provided, the time dimension is expanded with this date-time.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The xarray Dataset with the time dimension expanded.</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def expand_time_dimension(\n    data: xr.Dataset, time_dim: str | None, dt: str | None\n) -&gt; xr.Dataset:\n    \"\"\"\n    Expands the time dimension in the given xarray Dataset.\n\n    Parameters:\n        data (xr.Dataset): The input xarray Dataset.\n        time_dim (str | None): The name of the time dimension to expand. If None, no expansion is performed.\n        dt (str | None): A string representing a date-time in the format 'YYYY-MM-DD HH:MM:SS'. If provided, the time dimension is expanded with this date-time.\n\n    Returns:\n        xr.Dataset: The xarray Dataset with the time dimension expanded.\n    \"\"\"\n    if (\n        # if time_dim is None then it is not one of the dimensions\n        (time_dim is None or time_dim not in data.dims)\n        # the default time dimension must not be one of the dimensions\n        and DEFAULT_TIME_DIMENSION not in data.dims\n        # if dt is none we cannot use it\n        and dt is not None\n    ):\n        ts = pd.Timestamp(dt)\n        pydt = ts.to_pydatetime()\n        if time_dim is None:\n            time_dim = DEFAULT_TIME_DIMENSION\n        data = data.expand_dims({time_dim: [pydt]})\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.create_missing_coords","title":"<code>create_missing_coords</code>","text":"<p>Create a new coordinate to be attached to an existing dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset</code> <p>Dataset</p> required <code>time_dim</code> <code>str</code> <p>time dimension</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: Dataset</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def create_missing_coords(data: xr.Dataset, time_dim: str | None) -&gt; xr.Dataset:\n    \"\"\"Create a new coordinate to be attached to an existing dimension.\n\n    Parameters:\n        data (xr.Dataset): Dataset\n        time_dim (str): time dimension\n\n    Returns:\n        xr.Dataset: Dataset\n    \"\"\"\n    if DEFAULT_TIME_DIMENSION in list(data.dims) and not any(\n        t in list(data.coords) for t in [time_dim, DEFAULT_TIME_DIMENSION]\n    ):\n        time_values = data[DEFAULT_TIME_DIMENSION].values\n        data = data.assign_coords(\n            {DEFAULT_TIME_DIMENSION: (DEFAULT_TIME_DIMENSION, time_values)}\n        )\n\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.rename_dimensions","title":"<code>rename_dimensions</code>","text":"<p>Renames dimensions in an xarray Dataset.</p> <p>This function renames the dimensions of an xarray Dataset based on the provided parameters. If a dimension name is provided and it exists in the Dataset, it will be renamed to the corresponding default dimension name. If no dimension name is provided or it matches the default dimension name, the dimension remains unchanged.</p> <p>The function returns the modified Dataset with the renamed dimensions. If any of the provided dimension names do not exist in the input Dataset, a ValueError is raised.</p> <p>The default dimension names are defined as constants: - DEFAULT_X_DIMENSION - DEFAULT_TIME_DIMENSION - DEFAULT_Y_DIMENSION</p> <p>These constants should be defined elsewhere in the codebase.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataset</code> <p>The input xarray Dataset to rename dimensions in.</p> required <code>x_dim</code> <code>str</code> <p>The current name of the x-dimension. Defaults to DEFAULT_X_DIMENSION.</p> <code>DEFAULT_X_DIMENSION</code> <code>time_dim</code> <code>str</code> <p>The current name of the time dimension. Defaults to DEFAULT_TIME_DIMENSION.</p> <code>DEFAULT_TIME_DIMENSION</code> <code>y_dim</code> <code>str</code> <p>The current name of the y-dimension. Defaults to DEFAULT_Y_DIMENSION.</p> <code>DEFAULT_Y_DIMENSION</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The xarray Dataset with renamed dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the provided dimension names do not exist in the input Dataset.</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def rename_dimensions(\n    data: xr.Dataset,\n    x_dim: str | None = DEFAULT_X_DIMENSION,\n    time_dim: str | None = DEFAULT_TIME_DIMENSION,\n    y_dim: str | None = DEFAULT_Y_DIMENSION,\n) -&gt; xr.Dataset:\n    # Docstrings assisted by watsonx Code Assistant\n    \"\"\"\n    Renames dimensions in an xarray Dataset.\n\n    This function renames the dimensions of an xarray Dataset based on the provided parameters.\n    If a dimension name is provided and it exists in the Dataset, it will be renamed to the corresponding default dimension name.\n    If no dimension name is provided or it matches the default dimension name, the dimension remains unchanged.\n\n    The function returns the modified Dataset with the renamed dimensions.\n    If any of the provided dimension names do not exist in the input Dataset, a ValueError is raised.\n\n    The default dimension names are defined as constants:\n    - DEFAULT_X_DIMENSION\n    - DEFAULT_TIME_DIMENSION\n    - DEFAULT_Y_DIMENSION\n\n    These constants should be defined elsewhere in the codebase.\n\n    Parameters:\n        data (xr.Dataset): The input xarray Dataset to rename dimensions in.\n        x_dim (str, optional): The current name of the x-dimension. Defaults to DEFAULT_X_DIMENSION.\n        time_dim (str, optional): The current name of the time dimension. Defaults to DEFAULT_TIME_DIMENSION.\n        y_dim (str, optional): The current name of the y-dimension. Defaults to DEFAULT_Y_DIMENSION.\n\n    Returns:\n        xr.Dataset: The xarray Dataset with renamed dimensions.\n\n    Raises:\n        ValueError: If any of the provided dimension names do not exist in the input Dataset.\n    \"\"\"\n    rename_dict = dict()\n    if x_dim is not None and x_dim != DEFAULT_X_DIMENSION and x_dim in data.dims.keys():\n        rename_dict[x_dim] = DEFAULT_X_DIMENSION\n    if y_dim is not None and y_dim != DEFAULT_Y_DIMENSION and y_dim in data.dims.keys():\n        rename_dict[y_dim] = DEFAULT_Y_DIMENSION\n    if (\n        time_dim is not None\n        and time_dim != DEFAULT_TIME_DIMENSION\n        and time_dim in data.dims.keys()\n    ):\n        rename_dict[time_dim] = DEFAULT_TIME_DIMENSION\n    if len(rename_dict) &gt; 0:\n        data = data.rename_dims(rename_dict)\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.filter_by_time","title":"<code>filter_by_time</code>","text":"<p>Filter data by timestamp</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataArray</code> <p>datacube</p> required <code>temporal_extent</code> <code>Tuple[datetime, datetime]</code> <p>start and end datetime</p> required <code>temporal_dim</code> <code>str</code> <p>name of the temporal dimension</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: datacube</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def filter_by_time(\n    data: Union[xr.DataArray, xr.Dataset],\n    temporal_extent: Tuple[datetime, Optional[datetime]],\n    temporal_dim: str,\n) -&gt; xr.DataArray:\n    \"\"\"Filter data by timestamp\n\n    Parameters:\n        data (xr.DataArray): datacube\n        temporal_extent (Tuple[datetime, datetime]): start and end datetime\n        temporal_dim (str): name of the temporal dimension\n\n    Returns:\n        xr.DataArray: datacube\n    \"\"\"\n\n    if isinstance(data, xr.Dataset):\n        data = data.to_array()\n\n    start_date = temporal_extent[0]\n    end_date = temporal_extent[1]\n    ts = data[temporal_dim].values\n    assert len(ts) &gt; 0, \"Error! temporal dimension is empty\"\n    # if end_date is None it is a open ended interval\n    if end_date is None:\n        end_date = sorted(ts)[-1]\n    if start_date.tzinfo is None:\n        start_date = pytz.UTC.localize(start_date)\n\n    if end_date.tzinfo is None:\n        end_date = pytz.UTC.localize(end_date)\n\n    # convert temporal index to datetime timezone-aware\n    timestamps = _convert_to_datetime(datetime_index=ts)\n    # if length of timestamps equals 2, timestamsps have been converted\n    if len(timestamps) &gt; 0:\n        start_index = bisect.bisect_left(timestamps, start_date)\n        end_index = bisect.bisect_right(timestamps, end_date)\n        if start_index == end_index:\n            data = data.isel({temporal_dim: [start_index]})\n        else:\n            data = data.isel({temporal_dim: slice(start_index, end_index)})\n    return data\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.remove_repeated_time_coords","title":"<code>remove_repeated_time_coords</code>","text":"<p>Squeeze duplicate timestamps into unique timestamps.</p> <p>This function keeps the time dimension but merges duplicate timestamps by backward filling nan values.</p> <p>Parameters:</p> Name Type Description Default <code>data_array</code> <code>DataArray</code> <p>data array</p> required <code>time_dim</code> <code>str</code> <p>time dimension</p> <code>DEFAULT_TIME_DIMENSION</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: data array</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def remove_repeated_time_coords(\n    data_array: xr.DataArray, time_dim: str = DEFAULT_TIME_DIMENSION\n) -&gt; xr.DataArray:\n    \"\"\"Squeeze duplicate timestamps into unique timestamps.\n\n    This function keeps the time dimension but merges duplicate timestamps by backward filling nan values.\n\n    Parameters:\n        data_array (xr.DataArray): data array\n        time_dim (str): time dimension\n\n    Returns:\n        xr.DataArray: data array\n    \"\"\"\n    assert time_dim in data_array.dims, f\"Error! {time_dim} is not in {data_array.dims}\"\n    # if there is no repeated timestamp, return same array\n    if len(set(data_array[time_dim].values)) == len(data_array[time_dim].values):\n        return data_array\n    else:\n        array_by_time: DefaultDict = defaultdict(list)\n        for index, t in enumerate(data_array[time_dim].values):\n            slice_array = data_array.isel({time_dim: index})\n            if t in array_by_time.keys():\n                array_by_time[t] = array_by_time[t].combine_first(slice_array)\n            else:\n                array_by_time[t] = slice_array\n        # logger.info('length of concat list', len(arr_timestamp_lst))\n        arr: xr.DataArray = xr.concat(\n            array_by_time.values(), dim=time_dim, compat=\"override\", coords=\"minimal\"\n        )\n\n        return arr\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.reproject_bbox","title":"<code>reproject_bbox</code>","text":"<p>reproject bounding box to specified dst_crs</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>minimum_longitude, minimum_latitude, maximum_longitude, maximum_latitude</p> required <code>dst_crs</code> <code>Union[int, str]</code> <p>destination CRS</p> required <code>src_crs</code> <code>Union[int, str]</code> <p>source CRS. Defaults to 4326.</p> <code>4326</code> <p>Returns:</p> Type Description <code>Tuple[float, float, float, float]</code> <p>Tuple[float, float, float, float]: reprojected bbox</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def reproject_bbox(\n    bbox: Tuple[float, float, float, float],\n    dst_crs: Union[int, str],\n    src_crs: Union[int, str] = 4326,\n    is_360_degree_system: bool = True,\n) -&gt; Tuple[float, float, float, float]:\n    \"\"\"reproject bounding box to specified dst_crs\n\n    Parameters:\n        bbox (Tuple[float, float, float, float]): minimum_longitude, minimum_latitude, maximum_longitude, maximum_latitude\n        dst_crs (Union[int, str]): destination CRS\n        src_crs (Union[int, str], optional): source CRS. Defaults to 4326.\n\n    Returns:\n        Tuple[float, float, float, float]: reprojected bbox\n    \"\"\"\n    crs_from: CRS = _get_epsg(crs_code=src_crs)\n    crs_to: CRS = _get_epsg(crs_code=dst_crs)\n    if crs_from.to_epsg() == crs_to.to_epsg() and not is_360_degree_system:\n        return bbox\n\n    transformer = pyproj.Transformer.from_crs(\n        crs_from=crs_from, crs_to=crs_to, always_xy=True\n    )\n    minx, miny, maxx, maxy = bbox\n    assert minx &lt;= maxx, f\"Error! {minx=} &lt;= {maxx=} is false\"\n    assert miny &lt;= maxy, f\"Error! {miny=} &lt;= {maxy=} is false\"\n    repr_minx, repr_miny = transformer.transform(minx, miny)\n    repr_maxx, repr_maxy = transformer.transform(maxx, maxy)\n    assert repr_minx &lt;= repr_maxx, f\"Error! {repr_minx=} &lt;= {repr_maxx=}\"\n    assert repr_miny &lt;= repr_maxy, f\"Error! {repr_miny=} &lt;= {repr_maxy=}\"\n    if dst_crs == 4326 and is_360_degree_system:\n        repr_minx, repr_maxx = _convert_to_360_degree_system(\n            values=[repr_minx, repr_maxx]\n        )\n\n    return (repr_minx, repr_miny, repr_maxx, repr_maxy)\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.convert_bbox_to_polygon","title":"<code>convert_bbox_to_polygon</code>","text":"<p>Converts a bounding box to a Shapely Polygon.</p> <p>This function takes a bounding box represented as a tuple of four floats (min_longitude, min_latitude, max_longitude, max_latitude) and converts it into a Shapely Polygon.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>A tuple containing the minimum and maximum longitude and latitude defining the bounding box.</p> required <p>Returns:</p> Name Type Description <code>Polygon</code> <code>Polygon</code> <p>A Shapely Polygon object representing the bounding box.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the created Polygon is not valid.</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def convert_bbox_to_polygon(bbox: Tuple[float, float, float, float]) -&gt; Polygon:\n    \"\"\"\n    Converts a bounding box to a Shapely Polygon.\n\n    This function takes a bounding box represented as a tuple of four floats\n    (min_longitude, min_latitude, max_longitude, max_latitude) and converts it\n    into a Shapely Polygon.\n\n    Parameters:\n        bbox (Tuple[float, float, float, float]): A tuple containing the\n            minimum and maximum longitude and latitude defining the bounding box.\n\n    Returns:\n        Polygon: A Shapely Polygon object representing the bounding box.\n\n    Raises:\n        AssertionError: If the created Polygon is not valid.\n    \"\"\"\n    minimum_longitude, minimum_latitude, maximum_longitude, maximum_latitude = bbox\n    p = Polygon(\n        [\n            [minimum_longitude, minimum_latitude],\n            [maximum_longitude, minimum_latitude],\n            [maximum_longitude, maximum_latitude],\n            [minimum_longitude, maximum_latitude],\n        ]\n    )\n    assert p.is_valid\n    return p\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.to_geojson","title":"<code>to_geojson</code>","text":"<p>convert shapely Polygon to either dict or str</p> <p>Parameters:</p> Name Type Description Default <code>geom</code> <code>Polygon</code> <p>geometry</p> required <code>output_format</code> <code>str</code> <p>description. Defaults to \"dict\".</p> <code>'dict'</code> <p>Returns:</p> Type Description <code>Union[dict, str]</code> <p>Union[dict, str]: geojson</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def to_geojson(geom: Polygon, output_format: str = \"dict\") -&gt; Union[dict, str]:\n    \"\"\"convert shapely Polygon to either dict or str\n\n    Parameters:\n        geom (Polygon): geometry\n        output_format (str, optional): _description_. Defaults to \"dict\".\n\n    Returns:\n        Union[dict, str]: geojson\n    \"\"\"\n    assert isinstance(geom, Polygon), f\"Error! not a polygon: {type(geom)}\"\n    poly = geojson.Polygon(list(geom.exterior.coords))\n    if output_format == \"dict\":\n        output = dict(poly)\n        assert isinstance(output, dict)\n    else:\n        output = geojson.dumps(poly)\n        assert isinstance(output, str)\n    return output\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.from_bbox_to_polygon","title":"<code>from_bbox_to_polygon</code>","text":"<p>generates a polygon from a bounding box</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>right, bottom, left, top</p> required <p>Returns:</p> Name Type Description <code>Polygon</code> <code>Polygon</code> <p>description</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def from_bbox_to_polygon(bbox: Tuple[float, float, float, float]) -&gt; Polygon:\n    \"\"\"generates a polygon from a bounding box\n\n    Parameters:\n        bbox (Tuple[float, float, float, float]): right, bottom, left, top\n\n    Returns:\n        Polygon: _description_\n    \"\"\"\n    minimum_longitude, minimum_latitude, maximum_longitude, maximum_latitude = bbox\n    assert minimum_longitude &lt;= maximum_longitude, (\n        f\"Error! Invalid values: {minimum_longitude=} {maximum_longitude=}\"\n    )\n    assert minimum_latitude &lt;= maximum_latitude, (\n        f\"Error! Invalid values: {minimum_latitude=} {maximum_latitude=}\"\n    )\n    p = Polygon(\n        [\n            [minimum_longitude, minimum_latitude],\n            [minimum_longitude, maximum_latitude],\n            [maximum_longitude, maximum_latitude],\n            [maximum_longitude, minimum_latitude],\n        ]\n    )\n    assert p.is_valid, f\"Error! Invalid polygon {p=}\"\n    return p\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.remove_invalid_characters","title":"<code>remove_invalid_characters</code>","text":"<p>environment variables must have alpha-numeric characters and underscore. This function remove what is invalid</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the bucket or instance</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>core part of env var</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def remove_invalid_characters(name: str) -&gt; str:\n    \"\"\"environment variables must have alpha-numeric characters and underscore. This function\n    remove what is invalid\n\n    Parameters:\n        name (str): name of the bucket or instance\n\n    Returns:\n        str: core part of env var\n    \"\"\"\n    assert isinstance(name, str), f\"Error! {name=} is not a str\"\n    env_var = \"\".join([i if str.isalnum(i) or i == \"_\" else \"\" for i in name])\n    return env_var\n</code></pre>"},{"location":"api/utils/geospatial_utils/#terrakit.general_utils.geospatial_util.extract_date_from_filename","title":"<code>extract_date_from_filename</code>","text":"<p>Extract a date from filename. Supports: YYYYDDD (7), YYYYMMDD (8), YYMMDD (6 -&gt; 20YYMMDD).</p> Source code in <code>terrakit/general_utils/geospatial_util.py</code> <pre><code>def extract_date_from_filename(\n    path: str,\n    prefer: str = \"first\",\n    warn_on_multiple: bool = True,\n) -&gt; datetime:\n    \"\"\"\n    Extract a date from filename.\n    Supports: YYYYDDD (7), YYYYMMDD (8), YYMMDD (6 -&gt; 20YYMMDD).\n    \"\"\"\n    name = os.path.basename(path)\n    tokens: list[str] = DATE_TOKEN_RE.findall(name)\n    if not tokens:\n        raise ValueError(f\"No 6/7/8-digit date found in: {name}\")\n\n    parsed: list[tuple[str, datetime]] = []\n    for t in tokens:\n        try:\n            parsed.append((t, _parse_date_token(t)))\n        except Exception:\n            continue\n\n    if not parsed:\n        raise ValueError(\n            f\"Found date-like tokens, but none parsed as valid dates: {tokens}\"\n        )\n\n    if len(parsed) &gt; 1 and warn_on_multiple:\n        logger.warning(\n            f\"Multiple date tokens found: {', '.join(t for t, _ in parsed)}. Using the {prefer} occurrence.\",\n            UserWarning,\n        )\n\n    prefer = prefer.lower()\n    if prefer == \"first\":\n        chosen = parsed[0]\n    elif prefer == \"last\":\n        chosen = parsed[-1]\n    elif prefer == \"max\":\n        chosen = max(parsed, key=lambda x: x[1])\n    elif prefer == \"min\":\n        chosen = min(parsed, key=lambda x: x[1])\n    else:\n        raise ValueError(\"`prefer` must be one of: 'first', 'last', 'max', 'min'\")\n\n    return chosen[1]\n</code></pre>"},{"location":"api/utils/helpers/","title":"Documentation for Helper Modules","text":"<p>Documentation for the <code>terrakit.general_utils</code> helper modules.</p>"},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader","title":"<code>terrakit.general_utils.labels_downloader</code>","text":""},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader.rapid_mapping_event_lookup","title":"<code>rapid_mapping_event_lookup(event_id) -&gt; dict</code>","text":"<p>Event look up for a given event from Copernicus Rapid Mapping Service.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>str</code> <p>event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict[str: any]: json response containing full details available for a given event.</p> Source code in <code>terrakit/general_utils/labels_downloader.py</code> <pre><code>def rapid_mapping_event_lookup(event_id) -&gt; dict:\n    \"\"\"\n    Event look up for a given event from Copernicus Rapid Mapping Service.\n\n    Parameters:\n        event_id (str): event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".\n\n    Returns:\n        dict[str: any]: json response containing full details available for a given event.\n    \"\"\"\n    event_id = event_id.upper().strip(\"EMSR\")\n    url = f\"{COPERNICUS_URL}/dashboard-api/public-activations/?code=EMSR{event_id}\"\n    resp = get(url)\n    resp.raise_for_status()\n    resp_json: dict = resp.json()\n    return resp_json\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader.rapid_mapping_acquisition_time_lookup","title":"<code>rapid_mapping_acquisition_time_lookup(event_id, monitoring_number) -&gt; str</code>","text":"<p>Look up acquisition time for a given event ID from Copernicus Rapid Mapping Service.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>str</code> <p>event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".</p> required <code>monitoring_number</code> <code>str</code> <p>monitoring number given by a two digit number. Provide either as \"MONIT00\" or \"monit00\" or \"00\".</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>acquisition time with format '%Y-%m-%dT%H:%M:%S'</p> Source code in <code>terrakit/general_utils/labels_downloader.py</code> <pre><code>def rapid_mapping_acquisition_time_lookup(event_id, monitoring_number) -&gt; str:\n    \"\"\"\n    Look up acquisition time for a given event ID from Copernicus Rapid Mapping Service.\n\n    Parameters:\n        event_id (str): event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".\n        monitoring_number (str): monitoring number given by a two digit number. Provide either as \"MONIT00\" or \"monit00\" or \"00\".\n\n    Returns:\n        str: acquisition time with format '%Y-%m-%dT%H:%M:%S'\n    \"\"\"\n    event_id = event_id.upper().strip(\"EMSR\")\n    monitoring_number = monitoring_number.upper().strip(\"MONIT\")\n    resp_json = rapid_mapping_event_lookup(event_id)\n    products = resp_json[\"results\"][0][\"aois\"][0][\"products\"]\n    for product in products:\n        if int(monitoring_number) == product[\"monitoringNumber\"]:\n            acquisitionTime: str = product[\"images\"][0][\"acquisitionTime\"]\n    return acquisitionTime\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader.rapid_mapping_event_date_time_lookup","title":"<code>rapid_mapping_event_date_time_lookup(event_id) -&gt; str</code>","text":"<p>Look up event date and time for a given event ID from Copernicus Rapid Mapping Service.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>str</code> <p>event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>event time with format '%Y-%m-%dT%H:%M:%S'</p> Source code in <code>terrakit/general_utils/labels_downloader.py</code> <pre><code>def rapid_mapping_event_date_time_lookup(event_id) -&gt; str:\n    \"\"\"\n    Look up event date and time for a given event ID from Copernicus Rapid Mapping Service.\n\n    Parameters:\n        event_id (str): event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"000\".\n\n    Returns:\n        str: event time with format '%Y-%m-%dT%H:%M:%S'\n    \"\"\"\n    event_id = event_id.upper().strip(\"EMSR\")\n    url = f\"{COPERNICUS_URL}/dashboard-api/public-activations/?code=EMSR{event_id}\"\n    resp = get(url)\n    resp.raise_for_status()\n    event_time: str = resp.json()[\"results\"][0][\"eventTime\"]\n    return event_time\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader.rapid_mapping_geojson_downloader","title":"<code>rapid_mapping_geojson_downloader(event_id, aoi, monitoring_number, version, dest) -&gt; str</code>","text":"<p>Download GeoJSON labels from Copernicus Rapid Mapping Service.</p> <p>Parameters:</p> Name Type Description Default <code>event_id</code> <code>str</code> <p>event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"emsr000\" or \"000\".</p> required <code>aoi</code> <code>str</code> <p>The area of interest is a two digit code for the aoi of the given event. Provide either as \"AOI00\" or \"aoi00\" or \"00\".</p> required <code>monitoring_number</code> <code>str</code> <p>The monitoring number for the event. Provide either as \"MONIT00\" or \"monit00\" or \"00\".</p> required <code>version</code> <code>str</code> <p>The event version number. Provide either as \"V1\" or \"v1\" or \"1\".</p> required <code>dest</code> <code>str</code> <p>The destination directory to save the downloaded GeoJSON files.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>downloaded GeoJSON path name</p> Example <pre><code>rapid_mapping_geojson_downloader(\n    event_id=\"748\",\n    aoi=\"01\",\n    monitoring_number=\"05\",\n    version=\"v1\",\n    dest=LABELS_FOLDER,\n)\n</code></pre> Source code in <code>terrakit/general_utils/labels_downloader.py</code> <pre><code>def rapid_mapping_geojson_downloader(\n    event_id, aoi, monitoring_number, version, dest\n) -&gt; str:\n    \"\"\"\n    Download GeoJSON labels from Copernicus Rapid Mapping Service.\n\n    Parameters:\n        event_id (str): event id is a three digit code unique to each event. Provide either as \"EMSR000\" or \"emsr000\" or \"000\".\n        aoi (str): The area of interest is a two digit code for the aoi of the given event. Provide either as \"AOI00\" or \"aoi00\" or \"00\".\n        monitoring_number (str): The monitoring number for the event. Provide either as \"MONIT00\" or \"monit00\" or \"00\".\n        version (str): The event version number. Provide either as \"V1\" or \"v1\" or \"1\".\n        dest (str): The destination directory to save the downloaded GeoJSON files.\n\n    Returns:\n        str: downloaded GeoJSON path name\n\n    Example:\n        ```python\n        rapid_mapping_geojson_downloader(\n            event_id=\"748\",\n            aoi=\"01\",\n            monitoring_number=\"05\",\n            version=\"v1\",\n            dest=LABELS_FOLDER,\n        )\n        ```\n    \"\"\"\n    dest = Path(dest)\n    dest = Path.absolute(dest)\n    event_id = event_id.upper().strip(\"EMSR\")\n    aoi = aoi.upper().strip(\"AOI\")\n    monitoring_number = monitoring_number.upper().strip(\"MONIT\")\n    version = version.upper().strip(\"V\")\n    zip_id = f\"EMSR{event_id}_AOI{aoi}_DEL_MONIT{monitoring_number}_v{version}.zip\"\n    zip_file = f\"{dest}/{zip_id}\"\n    geojson_file = f\"EMSR{event_id}_AOI{aoi}_DEL_MONIT{monitoring_number}_observedEventA_v{version}.json\"\n\n    # Check if labels already exist\n    acquisition_time = rapid_mapping_acquisition_time_lookup(\n        event_id, monitoring_number\n    )\n    acquisition_date = acquisition_time.split(\"T\")[0]\n\n    # update label geojson to include date\n    geojson_file_with_date = geojson_file.replace(\".json\", f\"_{acquisition_date}.json\")\n    if Path(f\"{dest}/{geojson_file_with_date}\").is_file():\n        print(\n            f\".\\n..\\n...\\n&gt;&gt;&gt; Skipping download.\\n\\t&gt;&gt;&gt; File already exists: {dest}/{geojson_file_with_date} already exists.\"\n        )\n    else:\n        print(\n            f\".\\n..\\n...\\n&gt;&gt;&gt; Downloading labels from Copernicus Emergency Management Service for: \\n\\t&gt;&gt;&gt; EMSR{event_id} &lt;&lt;&lt;\\n\\t&gt;&gt;&gt; AOI{aoi} &lt;&lt;&lt;\\n\\t&gt;&gt;&gt; MONIT{monitoring_number} &lt;&lt;&lt;\\n\\t&gt;&gt;&gt; observedEventA &lt;&lt;&lt;\\n\\t&gt;&gt;&gt; v{version} &lt;&lt;&lt;\"\n        )\n        # Create directory to download results to\n        try:\n            dest = Path(dest)\n            dest.mkdir(parents=True, exist_ok=True)\n        except Exception as e:\n            raise TerrakitBaseException(\n                f\"An issue occurred created {dest}. Please check this is a valid dir: {e}\"\n            )\n\n        # Download zip\n        url = f\"{COPERNICUS_URL}/EMSR{event_id}/AOI{aoi}/DEL_MONIT{monitoring_number}/{zip_id}\"\n        print(f\".\\n..\\n...\\n&gt;&gt;&gt; Requesting event data from:\\n\\t&gt;&gt;&gt; {url} ... &lt;&lt;&lt;\")\n        resp = get(url)\n        resp.raise_for_status()\n\n        # Extract zip\n        try:\n            with open(zip_file, \"wb\") as f:\n                f.write(resp.content)\n        except Exception as e:\n            raise TerrakitBaseException(\n                f\"An issue occurred while writting the contents to {zip_file}: {e}\"\n            )\n        print(f\".\\n..\\n...\\n&gt;&gt;&gt; Extracting event geojson to:\\n\\t&gt;&gt;&gt; {dest} ... &lt;&lt;&lt;\")\n        try:\n            with ZipFile(zip_file, \"r\") as z_file:\n                if geojson_file in z_file.namelist():\n                    z_file.extract(geojson_file, dest)\n                    os.remove(zip_file)\n                else:\n                    print(\n                        f\"{geojson_file} not found in zip. Zip contents includes: {z_file.filelist}\"\n                    )\n        except Exception as e:\n            raise TerrakitBaseException(\n                f\"An issue occurred while extracting {geojson_file} from {zip_file}: {e}\"\n            )\n\n        # update label geojson to include date\n        geojson_file_with_date = geojson_file.replace(\n            \".json\", f\"_{acquisition_date}.json\"\n        )\n        try:\n            os.rename(f\"{dest}/{geojson_file}\", f\"{dest}/{geojson_file_with_date}\")\n            print(\n                f\".\\n..\\n...\\n&gt;&gt;&gt; Label geojson successfully saved:\\n\\t&gt;&gt;&gt; acquisition date: {acquisition_date} &lt;&lt;&lt;\\n\\t&gt;&gt;&gt; {dest}/{geojson_file_with_date} &lt;&lt;&lt;\"\n            )\n        except FileNotFoundError:\n            raise TerrakitBaseException(f\"Error: {dest}/{geojson_file} not found.\")\n        except PermissionError:\n            raise TerrakitBaseException(\n                f\"Error: Check permission to rename {dest}/{geojson_file}.\"\n            )\n        except OSError as e:\n            raise TerrakitBaseException(\n                f\"An error occurred append date to {dest}/{geojson_file}: {e}\"\n            )\n        print(\".\\n..\\n...\\n&gt;&gt;&gt; Downloaded completed successfully &lt;&lt;&lt;\")\n    return f\"{dest}/{geojson_file_with_date}\"\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.labels_downloader.hugging_face_file_downloader","title":"<code>hugging_face_file_downloader(repo_id: str, filename: str, revision: str = 'main', subfolder: str | None = None, dest: str = '.')</code>","text":"<p>Downloads a label file from Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The Hugging Face Hub repository ID.</p> required <code>filename</code> <code>str</code> <p>The name of the file to download.</p> required <code>revision</code> <code>str</code> <p>The revision or commit to download. Defaults to \"main\".</p> <code>'main'</code> <code>subfolder</code> <code>str</code> <p>The subfolder within the repository to download from. Defaults to None.</p> <code>None</code> <code>dest</code> <code>str</code> <p>The destination directory to save the downloaded file. Defaults to the current directory (.).</p> <code>'.'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the downloaded file.</p> Example <pre><code>hugging_face_file_downloader(\n    repo_id=\"ibm-nasa-geospatial/hls_burn_scars\",\n    filename=\"subsetted_512x512_HLS.S30.T10SGD.2021306.v1.4.mask.tif\",\n    revision=\"e48662b31288f1d5f1fd5cf5ebb0e454092a19ce\",\n    subfolder=\"training\",\n    dest=\"./docs/exmamples/test_wildfire\",\n)\n</code></pre> Source code in <code>terrakit/general_utils/labels_downloader.py</code> <pre><code>def hugging_face_file_downloader(\n    repo_id: str,\n    filename: str,\n    revision: str = \"main\",\n    subfolder: str | None = None,\n    dest: str = \".\",\n):\n    \"\"\"\n    Downloads a label file from Hugging Face Hub.\n\n    Parameters:\n        repo_id (str): The Hugging Face Hub repository ID.\n        filename (str): The name of the file to download.\n        revision (str, optional): The revision or commit to download. Defaults to \"main\".\n        subfolder (str, optional): The subfolder within the repository to download from. Defaults to None.\n        dest (str, optional): The destination directory to save the downloaded file. Defaults to the current directory (.).\n\n    Returns:\n        str: The path to the downloaded file.\n\n    Example:\n        ```python\n        hugging_face_file_downloader(\n            repo_id=\"ibm-nasa-geospatial/hls_burn_scars\",\n            filename=\"subsetted_512x512_HLS.S30.T10SGD.2021306.v1.4.mask.tif\",\n            revision=\"e48662b31288f1d5f1fd5cf5ebb0e454092a19ce\",\n            subfolder=\"training\",\n            dest=\"./docs/exmamples/test_wildfire\",\n        )\n        ```\n    \"\"\"\n    # Create directory to download results to\n    try:\n        dest = Path(dest)  # type: ignore[assignment]\n        dest.mkdir(parents=True, exist_ok=True)  # type: ignore[attr-defined]\n    except Exception as e:\n        raise TerrakitBaseException(\n            f\"An issue occurred created {dest}. Please check this is a valid dir: {e}\"\n        )\n\n    tmp_download_dir = \"tmp_hf_download\"\n    hf_hub_download(\n        repo_id=repo_id,\n        repo_type=\"dataset\",\n        subfolder=subfolder,\n        filename=filename,\n        revision=revision,\n        local_dir=\"./tmp_hf_download\",\n    )\n    try:\n        if subfolder:\n            os.rename(\n                f\"./{tmp_download_dir}/{subfolder}/{filename}\", f\"{dest}/{filename}\"\n            )\n        else:\n            os.rename(f\"{tmp_download_dir}/{filename}\", f\"{dest}/{filename}\")\n        print(\".\\n..\\n...\\n&gt;&gt;&gt; Label successfully saved&lt;&lt;&lt;\")\n    except FileNotFoundError:\n        raise TerrakitBaseException(f\"Error: {tmp_download_dir}/{filename} not found.\")\n    except PermissionError:\n        raise TerrakitBaseException(\n            f\"Error: Check permission to rename {dest}/{filename}.\"\n        )\n    except OSError as e:\n        raise TerrakitBaseException(\n            f\"An error occurred append date to {dest}/{filename}: {e}\"\n        )\n    print(\".\\n..\\n...\\n&gt;&gt;&gt; Downloaded completed successfully &lt;&lt;&lt;\")\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.statistics","title":"<code>terrakit.general_utils.statistics</code>","text":""},{"location":"api/utils/helpers/#terrakit.general_utils.statistics.compute_stats","title":"<code>compute_stats(dataset)</code>","text":"<p>Compute descriptive statistics for a given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ndarray</code> <p>The dataset for which to compute statistics.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing mean, median, minimum, maximum, standard deviation, and count of the dataset.</p> Source code in <code>terrakit/general_utils/statistics.py</code> <pre><code>def compute_stats(dataset):\n    \"\"\"\n    Compute descriptive statistics for a given dataset.\n\n    Parameters:\n        dataset (numpy.ndarray): The dataset for which to compute statistics.\n\n    Returns:\n        tuple: A tuple containing mean, median, minimum, maximum, standard deviation, and count of the dataset.\n    \"\"\"\n    mean_val = np.mean(dataset)\n    median_val = np.median(dataset)\n    min_val = np.min(dataset)\n    max_val = np.max(dataset)\n    std_dev = np.std(dataset)\n    count = dataset.size\n    print(f\"Mean pixel value: {mean_val}\")\n    print(f\"Median pixel value: {median_val}\")\n    print(f\"Minimum pixel value: {min_val}\")\n    print(f\"Maximum pixel value: {max_val}\")\n    print(f\"Standard deviation: {std_dev}\")\n    print(f\"Number of masked pixels: {count}\\n--------\")\n    return mean_val, median_val, min_val, max_val, std_dev, count\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.statistics.compute_stats_for_masked_pixels","title":"<code>compute_stats_for_masked_pixels(image, mask)</code>","text":"<p>Compute descriptive statistics for masked pixels in the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image data.</p> required <code>mask</code> <code>ndarray</code> <p>The mask to filter the image data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing mean, median, minimum, maximum, standard deviation, and count of the masked pixels.</p> Source code in <code>terrakit/general_utils/statistics.py</code> <pre><code>def compute_stats_for_masked_pixels(image, mask):\n    \"\"\"\n    Compute descriptive statistics for masked pixels in the given image.\n\n    Parameters:\n        image (numpy.ndarray): The image data.\n        mask (numpy.ndarray): The mask to filter the image data.\n\n    Returns:\n        tuple: A tuple containing mean, median, minimum, maximum, standard deviation, and count of the masked pixels.\n    \"\"\"\n    masked_data = image[mask &gt; 0]\n    return compute_stats(masked_data)\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.statistics.load_verified_stats","title":"<code>load_verified_stats()</code>","text":"<p>Load precomputed statistics for verified data calculated from target_tif = \"sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_20\" generated using EMSR748</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing verified label statistics, verified data statistics, and verified masked statistics.</p> Source code in <code>terrakit/general_utils/statistics.py</code> <pre><code>def load_verified_stats():\n    \"\"\"\n    Load precomputed statistics for verified data calculated from target_tif = \"sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_20\" generated using EMSR748\n\n    Returns:\n        tuple: A tuple containing verified label statistics, verified data statistics, and verified masked statistics.\n    \"\"\"\n    verified_label_stats = (\n        np.float64(0.532928466796875),\n        np.float64(1.0),\n        np.float64(0.0),\n        np.float64(1.0),\n        np.float64(0.49891453784632),\n        65536,\n    )\n\n    verified_data_stats = (\n        np.float64(6038.482086181641),\n        np.float64(6510.0),\n        np.float64(507.0),\n        np.float64(10984.0),\n        np.float64(1818.4264044731774),\n        65536,\n    )\n\n    verified_mask_stats = (\n        np.float64(6435.794422493272),\n        np.float64(6784.0),\n        np.float64(664.0),\n        np.float64(9424.0),\n        np.float64(1584.2594606115797),\n        34926,\n    )\n    return verified_label_stats, verified_data_stats, verified_mask_stats\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.defaults","title":"<code>terrakit.general_utils.defaults</code>","text":""},{"location":"api/utils/helpers/#terrakit.general_utils.defaults.get_default_class_args_and_values","title":"<code>get_default_class_args_and_values(cls_name: type) -&gt; dict</code>","text":"<p>Return a dictionary of class arguments and default values.</p> <p>Parameters:</p> Name Type Description Default <code>cls_name</code> <code>type) </code> <p>The class for which to retrieve default arguments.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing class arguments and their default values.</p> Source code in <code>terrakit/general_utils/defaults.py</code> <pre><code>def get_default_class_args_and_values(cls_name: type) -&gt; dict:\n    \"\"\"\n    Return a dictionary of class arguments and default values.\n\n    Parameters:\n        cls_name (type) : The class for which to retrieve default arguments.\n\n    Returns:\n        dict: A dictionary containing class arguments and their default values.\n    \"\"\"\n    default_args = {}\n    signature = inspect.signature(cls_name.__init__)  # type: ignore[misc]\n    parameters = signature.parameters\n    for name, param in parameters.items():\n        if name == \"self\":\n            continue  # Skip the 'self' parameter\n        default_value = param.default\n        if default_value is inspect.Parameter.empty:\n            default_value = None\n        default_args[name] = default_value\n    return default_args\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.defaults.get_pipeline_defaults","title":"<code>get_pipeline_defaults() -&gt; dict</code>","text":"<p>Return a dictionary of arguments and default values for all pipeline steps.</p> <p>Returns     dict: A dictionary containing class arguments and their default values for pipeline steps.</p> Example <pre><code>from terrakit.general_utils.defautls import get_pipeline_default\n\noptions = get_pipeline_defaults()\n</code></pre> Source code in <code>terrakit/general_utils/defaults.py</code> <pre><code>def get_pipeline_defaults() -&gt; dict:\n    \"\"\"\n    Return a dictionary of arguments and default values for all pipeline steps.\n\n    Returns\n        dict: A dictionary containing class arguments and their default values for pipeline steps.\n\n    Example:\n        ```python\n        from terrakit.general_utils.defautls import get_pipeline_default\n\n        options = get_pipeline_defaults()\n        ```\n    \"\"\"\n    onboarding_defaults = {}\n    onboarding_defaults[\"labels\"] = get_default_class_args_and_values(LabelsCls)\n    onboarding_defaults[\"download\"] = get_default_class_args_and_values(DownloadCls)\n    onboarding_defaults[\"chip\"] = get_default_class_args_and_values(ChipAndLabelCls)\n    return onboarding_defaults\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.defaults.update_pipeline_args","title":"<code>update_pipeline_args(pipeline_options: dict) -&gt; dict</code>","text":"<p>Update default values for any pipeline steps specified in pipelines_options.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_options</code> <code>dict</code> <p>Dictionary of all pipeline options.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of class arguments and either default values or 'pipeline_options' values.</p> Example <pre><code>from terrakit.general_utils.defautls import update_pipeline_args\n\nmy_options = {\n    \"chip\": {\n        \"sample_dim\": 124,\n    }\n}\nonboarding_options = update_pipeline_args(my_options)\n</code></pre> Source code in <code>terrakit/general_utils/defaults.py</code> <pre><code>def update_pipeline_args(pipeline_options: dict) -&gt; dict:\n    \"\"\"\n    Update default values for any pipeline steps specified in pipelines_options.\n\n    Parameters:\n        pipeline_options (dict): Dictionary of all pipeline options.\n\n    Returns:\n        dict: Dictionary of class arguments and either default values or 'pipeline_options' values.\n\n    Example:\n        ```python\n        from terrakit.general_utils.defautls import update_pipeline_args\n\n        my_options = {\n            \"chip\": {\n                \"sample_dim\": 124,\n            }\n        }\n        onboarding_options = update_pipeline_args(my_options)\n        ```\n    \"\"\"\n    default_onboarding_options = get_pipeline_defaults()\n    for step in pipeline_options:\n        if step in default_onboarding_options.keys():\n            for parms in pipeline_options[step]:\n                if parms in default_onboarding_options[step].keys():\n                    default_onboarding_options[step][parms] = pipeline_options[step][\n                        parms\n                    ]\n    return default_onboarding_options\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.exceptions","title":"<code>terrakit.general_utils.exceptions</code>","text":""},{"location":"api/utils/helpers/#terrakit.general_utils.exceptions.TerrakitBaseException","title":"<code>TerrakitBaseException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all custom exceptions in the project.</p> Source code in <code>terrakit/general_utils/exceptions.py</code> <pre><code>class TerrakitBaseException(Exception):\n    \"\"\"Base exception for all custom exceptions in the project.\"\"\"\n\n    def __init__(self, message: str, details: Union[None, str, Dict[Any, Any]] = None):\n        self.message = message\n        self.details = details or {}\n        super().__init__(self.message)\n\n    def __str__(self) -&gt; str:\n        if self.details:\n            return f\"{self.message} - Additional details: {self.details}\"\n        return self.message\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.exceptions.TerrakitValidationError","title":"<code>TerrakitValidationError</code>","text":"<p>               Bases: <code>TerrakitBaseException</code></p> <p>Raised when there is an validation error.</p> Source code in <code>terrakit/general_utils/exceptions.py</code> <pre><code>class TerrakitValidationError(TerrakitBaseException):\n    \"\"\"Raised when there is an validation error.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/utils/helpers/#terrakit.general_utils.exceptions.TerrakitValueError","title":"<code>TerrakitValueError</code>","text":"<p>               Bases: <code>TerrakitBaseException</code></p> <p>Raised when there is invalid input.</p> Source code in <code>terrakit/general_utils/exceptions.py</code> <pre><code>class TerrakitValueError(TerrakitBaseException):\n    \"\"\"Raised when there is invalid input.\"\"\"\n\n    pass\n</code></pre>"},{"location":"examples/labels_to_data/","title":"Terrakit: Labels to dataset pipeline","text":"<p>This notebook demonstrates generating a ML-ready dataset from a collection of labels. The labels used in this example are in GeoJSON format from two wildfire events reported on Copernicus Rapid Mapping Service. The two wildfire events are:</p> <ul> <li>Wildfire in Central Madeira, Portugal, 2024/08/26</li> <li>Wildfire in Biebrza National Park, Poland, 2025/04/23</li> </ul> Install Terrakit: For instructions on how to install TerraKit, take a look at the Welcome page.  In\u00a0[\u00a0]: Copied! <pre>import os\n\nfrom glob import glob\nfrom pathlib import Path\n\n# Set LOGLEVEL to info for more verbose logging\nos.environ[\"LOGLEVEL\"] = \"WARNING\"\n\nimport terrakit\n\n# Import TerraKit Utils\nfrom terrakit.general_utils.labels_downloader import (\n    rapid_mapping_geojson_downloader,\n    hugging_face_file_downloader,\n    EXAMPLE_LABEL_FILES,\n    EXAMPLE_RASTER_LABEL_FILES,\n)\nfrom terrakit.general_utils.plotting import (\n    plot_label_dataframes,\n    plot_labels_on_map,\n    plot_tiles_and_label_pair,\n    plot_chip_and_label_pairs,\n)\nfrom terrakit.download.geodata_utils import check_and_crop_bbox\n</pre> import os  from glob import glob from pathlib import Path  # Set LOGLEVEL to info for more verbose logging os.environ[\"LOGLEVEL\"] = \"WARNING\"  import terrakit  # Import TerraKit Utils from terrakit.general_utils.labels_downloader import (     rapid_mapping_geojson_downloader,     hugging_face_file_downloader,     EXAMPLE_LABEL_FILES,     EXAMPLE_RASTER_LABEL_FILES, ) from terrakit.general_utils.plotting import (     plot_label_dataframes,     plot_labels_on_map,     plot_tiles_and_label_pair,     plot_chip_and_label_pairs, ) from terrakit.download.geodata_utils import check_and_crop_bbox In\u00a0[\u00a0]: Copied! <pre># Set a working directory, a dataset name and a directory where some labels can be found\nDATASET_NAME = \"test_dataset\"\nWORKING_DIR = f\"./tmp/{DATASET_NAME}\"\nLABELS_FOLDER = \"./test_wildfire_vector\"\n</pre> # Set a working directory, a dataset name and a directory where some labels can be found DATASET_NAME = \"test_dataset\" WORKING_DIR = f\"./tmp/{DATASET_NAME}\" LABELS_FOLDER = \"./test_wildfire_vector\" Example labels: To download a set of example labels, use the `rapid_mapping_geojson_downloader` module.  In\u00a0[\u00a0]: Copied! <pre># Download some labels if none already exist.\nif (\n    Path(LABELS_FOLDER).is_dir() is False\n    or set(EXAMPLE_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER}/*.json\")) is False\n):\n    rapid_mapping_geojson_downloader(\n        event_id=\"748\",\n        aoi=\"01\",\n        monitoring_number=\"05\",\n        version=\"v1\",\n        dest=LABELS_FOLDER,\n    )\n    rapid_mapping_geojson_downloader(\n        event_id=\"801\",\n        aoi=\"01\",\n        monitoring_number=\"02\",\n        version=\"v1\",\n        dest=LABELS_FOLDER,\n    )\n</pre> # Download some labels if none already exist. if (     Path(LABELS_FOLDER).is_dir() is False     or set(EXAMPLE_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER}/*.json\")) is False ):     rapid_mapping_geojson_downloader(         event_id=\"748\",         aoi=\"01\",         monitoring_number=\"05\",         version=\"v1\",         dest=LABELS_FOLDER,     )     rapid_mapping_geojson_downloader(         event_id=\"801\",         aoi=\"01\",         monitoring_number=\"02\",         version=\"v1\",         dest=LABELS_FOLDER,     ) In\u00a0[\u00a0]: Copied! <pre># Process the labels by providing a labels folder, working directory and dataset name.\nlabels_gdf, grouped_bbox_gdf = terrakit.process_labels(\n    labels_folder=LABELS_FOLDER,\n    dataset_name=DATASET_NAME,\n    working_dir=WORKING_DIR,\n)\nprint(labels_gdf)\nprint(grouped_bbox_gdf)\n</pre> # Process the labels by providing a labels folder, working directory and dataset name. labels_gdf, grouped_bbox_gdf = terrakit.process_labels(     labels_folder=LABELS_FOLDER,     dataset_name=DATASET_NAME,     working_dir=WORKING_DIR, ) print(labels_gdf) print(grouped_bbox_gdf) In\u00a0[15]: Copied! <pre># Plot the process labels and bboxes to confirm they appear as expected.\nplot_label_dataframes(labels_gdf, grouped_bbox_gdf)\n</pre> # Plot the process labels and bboxes to confirm they appear as expected. plot_label_dataframes(labels_gdf, grouped_bbox_gdf) In\u00a0[16]: Copied! <pre># Additionally plot labels and bbox on map.\nmap_collection, title_list = plot_labels_on_map(labels_gdf, grouped_bbox_gdf)\nfor i in range(0, len(map_collection)):\n    print(title_list[i])\n    display(map_collection[i])\n</pre> # Additionally plot labels and bbox on map. map_collection, title_list = plot_labels_on_map(labels_gdf, grouped_bbox_gdf) for i in range(0, len(map_collection)):     print(title_list[i])     display(map_collection[i]) <pre>\n\nDownload tile bounding box and labels for: EMSR748_AOI01_DEL_MONIT05_observedEventA_v1_2024-08-26.json\n\n</pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre>\n\nDownload tile bounding box and labels for: EMSR748_AOI01_DEL_MONIT05_observedEventA_v1_2024-08-26.json\n\n</pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[\u00a0]: Copied! <pre>config = {\n    \"download\": {\n        \"data_sources\": [\n            {\n                \"data_connector\": \"sentinel_aws\",\n                \"collection_name\": \"sentinel-2-l2a\",\n                \"bands\": [\"blue\", \"green\", \"red\"],\n            },\n        ],\n        \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},\n        \"transform\": {\n            \"scale_data_xarray\": True,\n            \"impute_nans\": True,\n            \"reproject\": True,\n        },\n        \"max_cloud_cover\": 80,\n    },\n}\n\nqueried_data = terrakit.download_data(\n    data_sources=config[\"download\"][\"data_sources\"],\n    date_allowance=config[\"download\"][\"date_allowance\"],\n    transform=config[\"download\"][\"transform\"],\n    max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],\n    dataset_name=DATASET_NAME,\n    working_dir=WORKING_DIR,\n    keep_files=False,\n)\n</pre> config = {     \"download\": {         \"data_sources\": [             {                 \"data_connector\": \"sentinel_aws\",                 \"collection_name\": \"sentinel-2-l2a\",                 \"bands\": [\"blue\", \"green\", \"red\"],             },         ],         \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},         \"transform\": {             \"scale_data_xarray\": True,             \"impute_nans\": True,             \"reproject\": True,         },         \"max_cloud_cover\": 80,     }, }  queried_data = terrakit.download_data(     data_sources=config[\"download\"][\"data_sources\"],     date_allowance=config[\"download\"][\"date_allowance\"],     transform=config[\"download\"][\"transform\"],     max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],     dataset_name=DATASET_NAME,     working_dir=WORKING_DIR,     keep_files=False, ) In\u00a0[17]: Copied! <pre>plot_tiles_and_label_pair(\n    queried_data, bands=config[\"download\"][\"data_sources\"][0][\"bands\"]\n)\n</pre> plot_tiles_and_label_pair(     queried_data, bands=config[\"download\"][\"data_sources\"][0][\"bands\"] ) <pre>Legend\nimage_0: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed.tif, label_0: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_labels.tif\n---\n</pre> <pre>Legend\nimage_1: sentinel_aws_sentinel-2-l2a_2025-04-23_imputed.tif, label_1: sentinel_aws_sentinel-2-l2a_2025-04-23_imputed_labels.tif\n---\n</pre> In\u00a0[\u00a0]: Copied! <pre>chip_args = {\n    \"chip\": {\"sample_dim\": 256},\n}\n\nres = terrakit.chip_and_label_data(\n    dataset_name=DATASET_NAME,\n    sample_dim=chip_args[\"chip\"][\"sample_dim\"],\n    queried_data=queried_data,\n    working_dir=WORKING_DIR,\n)\n\n# Note that we can use the working directory to find the list of files to chip.\n# Uncomment to try this out.\n# res = terrakit.chip_and_label_data(\n#     dataset_name=DATASET_NAME,\n#     working_dir=WORKING_DIR,\n#     sample_dim=chip_args[\"chip\"][\"sample_dim\"],\n#     keep_files=True,\n# )\n</pre> chip_args = {     \"chip\": {\"sample_dim\": 256}, }  res = terrakit.chip_and_label_data(     dataset_name=DATASET_NAME,     sample_dim=chip_args[\"chip\"][\"sample_dim\"],     queried_data=queried_data,     working_dir=WORKING_DIR, )  # Note that we can use the working directory to find the list of files to chip. # Uncomment to try this out. # res = terrakit.chip_and_label_data( #     dataset_name=DATASET_NAME, #     working_dir=WORKING_DIR, #     sample_dim=chip_args[\"chip\"][\"sample_dim\"], #     keep_files=True, # ) In\u00a0[18]: Copied! <pre>plot_chip_and_label_pairs(\n    res, bands=config[\"download\"][\"data_sources\"][0][\"bands\"], samples=10\n)\n</pre> plot_chip_and_label_pairs(     res, bands=config[\"download\"][\"data_sources\"][0][\"bands\"], samples=10 ) <pre>Legend\nimage_0: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_9.data.tif\nimage_1: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_4.data.tif\nimage_2: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_16.data.tif\nimage_3: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_8.data.tif\nimage_4: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_17.data.tif\nimage_5: sentinel_aws_sentinel-2-l2a_2025-04-23_imputed_0.data.tif\nimage_6: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_20.data.tif\nimage_7: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_2.data.tif\nimage_8: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_18.data.tif\nimage_9: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_11.data.tif\n---\nlabel_0: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_9.label.tif\nlabel_1: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_4.label.tif\nlabel_2: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_16.label.tif\nlabel_3: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_8.label.tif\nlabel_4: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_17.label.tif\nlabel_5: sentinel_aws_sentinel-2-l2a_2025-04-23_imputed_0.label.tif\nlabel_6: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_20.label.tif\nlabel_7: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_2.label.tif\nlabel_8: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_18.label.tif\nlabel_9: sentinel_aws_sentinel-2-l2a_2024-08-30_imputed_11.label.tif\n</pre> In\u00a0[\u00a0]: Copied! <pre>terrakit.taco_store_data(\n    dataset_name=DATASET_NAME,\n    working_dir=WORKING_DIR,\n    save_dir=WORKING_DIR,\n    tortilla_name=\"terrakit_curated_dataset.tortilla\",\n)\n</pre> terrakit.taco_store_data(     dataset_name=DATASET_NAME,     working_dir=WORKING_DIR,     save_dir=WORKING_DIR,     tortilla_name=\"terrakit_curated_dataset.tortilla\", ) In\u00a0[\u00a0]: Copied! <pre>terrakit.load_tortilla(\"./tmp/test_dataset/terrakit_curated_dataset.tortilla\")\n</pre> terrakit.load_tortilla(\"./tmp/test_dataset/terrakit_curated_dataset.tortilla\") In\u00a0[\u00a0]: Copied! <pre>check_and_crop_bbox(bbox=[-73.645550, 44.233885, -72.539635, 44.278023], resolution=10)\n</pre> check_and_crop_bbox(bbox=[-73.645550, 44.233885, -72.539635, 44.278023], resolution=10) In\u00a0[2]: Copied! <pre># Set a working directory, a dataset name and a directory where some labels can be found\nDATASET_NAME_RASTER = \"test_dataset_raster\"\nWORKING_DIR_RASTER = f\"./tmp/{DATASET_NAME_RASTER}\"\nLABELS_FOLDER_RASTER = \"./test_burn_scar_raster\"\n</pre> # Set a working directory, a dataset name and a directory where some labels can be found DATASET_NAME_RASTER = \"test_dataset_raster\" WORKING_DIR_RASTER = f\"./tmp/{DATASET_NAME_RASTER}\" LABELS_FOLDER_RASTER = \"./test_burn_scar_raster\" Example labels: To download a set of example raster labels, use the `hugging_face_file_downloader` function.  In\u00a0[\u00a0]: Copied! <pre>if (\n    Path(LABELS_FOLDER_RASTER).is_dir() is False\n    or set(EXAMPLE_RASTER_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER_RASTER}/*.tif\"))\n    is False\n):\n    for filename in EXAMPLE_RASTER_LABEL_FILES:\n        hugging_face_file_downloader(\n            repo_id=\"ibm-nasa-geospatial/hls_burn_scars\",\n            filename=filename,\n            revision=\"e48662b31288f1d5f1fd5cf5ebb0e454092a19ce\",\n            subfolder=\"training\",\n            dest=LABELS_FOLDER_RASTER,\n        )\n</pre> if (     Path(LABELS_FOLDER_RASTER).is_dir() is False     or set(EXAMPLE_RASTER_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER_RASTER}/*.tif\"))     is False ):     for filename in EXAMPLE_RASTER_LABEL_FILES:         hugging_face_file_downloader(             repo_id=\"ibm-nasa-geospatial/hls_burn_scars\",             filename=filename,             revision=\"e48662b31288f1d5f1fd5cf5ebb0e454092a19ce\",             subfolder=\"training\",             dest=LABELS_FOLDER_RASTER,         ) In\u00a0[\u00a0]: Copied! <pre>labels_gdf, grouped_bbox_gdf = terrakit.process_labels(\n    labels_folder=LABELS_FOLDER_RASTER,\n    dataset_name=DATASET_NAME_RASTER,\n    working_dir=WORKING_DIR_RASTER,\n    label_type=\"raster\",\n)\nprint(labels_gdf)\nprint(grouped_bbox_gdf)\n</pre> labels_gdf, grouped_bbox_gdf = terrakit.process_labels(     labels_folder=LABELS_FOLDER_RASTER,     dataset_name=DATASET_NAME_RASTER,     working_dir=WORKING_DIR_RASTER,     label_type=\"raster\", ) print(labels_gdf) print(grouped_bbox_gdf) In\u00a0[8]: Copied! <pre>plot_label_dataframes(labels_gdf, grouped_bbox_gdf)\n</pre> plot_label_dataframes(labels_gdf, grouped_bbox_gdf) In\u00a0[\u00a0]: Copied! <pre>config = {\n    \"download\": {\n        \"data_sources\": [\n            {\n                \"data_connector\": \"sentinel_aws\",\n                \"collection_name\": \"sentinel-2-l2a\",\n                \"bands\": [\"blue\", \"green\", \"red\"],\n            },\n        ],\n        \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},\n        \"transform\": {\n            \"scale_data_xarray\": True,\n            \"impute_nans\": True,\n            \"reproject\": True,\n        },\n        \"max_cloud_cover\": 80,\n    },\n}\nqueried_data = terrakit.download_data(\n    data_sources=config[\"download\"][\"data_sources\"],\n    date_allowance=config[\"download\"][\"date_allowance\"],\n    transform=config[\"download\"][\"transform\"],\n    max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],\n    dataset_name=DATASET_NAME_RASTER,\n    working_dir=WORKING_DIR_RASTER,\n    keep_files=False,\n)\n</pre> config = {     \"download\": {         \"data_sources\": [             {                 \"data_connector\": \"sentinel_aws\",                 \"collection_name\": \"sentinel-2-l2a\",                 \"bands\": [\"blue\", \"green\", \"red\"],             },         ],         \"date_allowance\": {\"pre_days\": 0, \"post_days\": 21},         \"transform\": {             \"scale_data_xarray\": True,             \"impute_nans\": True,             \"reproject\": True,         },         \"max_cloud_cover\": 80,     }, } queried_data = terrakit.download_data(     data_sources=config[\"download\"][\"data_sources\"],     date_allowance=config[\"download\"][\"date_allowance\"],     transform=config[\"download\"][\"transform\"],     max_cloud_cover=config[\"download\"][\"max_cloud_cover\"],     dataset_name=DATASET_NAME_RASTER,     working_dir=WORKING_DIR_RASTER,     keep_files=False, ) In\u00a0[10]: Copied! <pre>plot_tiles_and_label_pair(\n    queried_data, bands=config[\"download\"][\"data_sources\"][0][\"bands\"]\n)\n</pre> plot_tiles_and_label_pair(     queried_data, bands=config[\"download\"][\"data_sources\"][0][\"bands\"] ) <pre>Legend\nimage_0: sentinel_aws_sentinel-2-l2a_2018-09-02_imputed.tif, label_0: sentinel_aws_sentinel-2-l2a_2018-09-02_imputed_labels.tif\n---\n</pre> <pre>Legend\nimage_1: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed.tif, label_1: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_labels.tif\n---\n</pre> In\u00a0[\u00a0]: Copied! <pre>chip_args = {\n    \"chip\": {\"sample_dim\": 256},\n}\n\nres = terrakit.chip_and_label_data(\n    dataset_name=DATASET_NAME_RASTER,\n    sample_dim=chip_args[\"chip\"][\"sample_dim\"],\n    queried_data=queried_data,\n    working_dir=WORKING_DIR_RASTER,\n)\n</pre> chip_args = {     \"chip\": {\"sample_dim\": 256}, }  res = terrakit.chip_and_label_data(     dataset_name=DATASET_NAME_RASTER,     sample_dim=chip_args[\"chip\"][\"sample_dim\"],     queried_data=queried_data,     working_dir=WORKING_DIR_RASTER, ) In\u00a0[12]: Copied! <pre>plot_chip_and_label_pairs(\n    res, bands=config[\"download\"][\"data_sources\"][0][\"bands\"], samples=10\n)\n</pre> plot_chip_and_label_pairs(     res, bands=config[\"download\"][\"data_sources\"][0][\"bands\"], samples=10 ) <pre>Legend\nimage_0: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_13.data.tif\nimage_1: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_57.data.tif\nimage_2: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_69.data.tif\nimage_3: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_27.data.tif\nimage_4: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_5.data.tif\nimage_5: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_17.data.tif\nimage_6: sentinel_aws_sentinel-2-l2a_2021-11-02_imputed_17.data.tif\nimage_7: sentinel_aws_sentinel-2-l2a_2018-09-02_imputed_5.data.tif\nimage_8: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_10.data.tif\nimage_9: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_25.data.tif\n---\nlabel_0: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_13.label.tif\nlabel_1: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_57.label.tif\nlabel_2: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_69.label.tif\nlabel_3: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_27.label.tif\nlabel_4: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_5.label.tif\nlabel_5: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_17.label.tif\nlabel_6: sentinel_aws_sentinel-2-l2a_2021-11-02_imputed_17.label.tif\nlabel_7: sentinel_aws_sentinel-2-l2a_2018-09-02_imputed_5.label.tif\nlabel_8: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_10.label.tif\nlabel_9: sentinel_aws_sentinel-2-l2a_2020-09-04_imputed_25.label.tif\n</pre> In\u00a0[\u00a0]: Copied! <pre>terrakit.taco_store_data(\n    dataset_name=DATASET_NAME_RASTER,\n    working_dir=WORKING_DIR_RASTER,\n    save_dir=WORKING_DIR_RASTER,\n    tortilla_name=\"terrakit_curated_dataset.tortilla\",\n)\n</pre> terrakit.taco_store_data(     dataset_name=DATASET_NAME_RASTER,     working_dir=WORKING_DIR_RASTER,     save_dir=WORKING_DIR_RASTER,     tortilla_name=\"terrakit_curated_dataset.tortilla\", )"},{"location":"examples/labels_to_data/#terrakit-labels-to-dataset-pipeline","title":"Terrakit: Labels to dataset pipeline\u00b6","text":""},{"location":"examples/labels_to_data/#1-process-labels","title":"1. Process labels\u00b6","text":"<p>This initial steps takes a directory containing some label files (geojson), and calculates a list of bboxes that encompass the geospatial locations described by the label files. Assuming that the date is included in the label filename, temporal information will also be captured.</p> <p>The function <code>process_geojson_labels</code> returns a Geopandas DataFrame where each row corresponds to a bbox for a given area. The function also saves this information in a shp file. The shp file is output into the working directory (default = <code>\"./tmp\"</code>).</p> <p>Either the shp file or the DataFrame can now be used in the next step to tell Terrakit which time and location to download some data from.</p>"},{"location":"examples/labels_to_data/#2-download-the-data","title":"2. Download the data\u00b6","text":""},{"location":"examples/labels_to_data/#example-21-use-a-shp-file-to-download-data","title":"Example 2.1: Use a shp file to download data\u00b6","text":""},{"location":"examples/labels_to_data/#22-inspect-the-data","title":"2.2: Inspect the data\u00b6","text":"<p>Use the <code>plot_tiles_and_label_pair</code> function to inspect the downloaded tiles and corresponding labels.</p>"},{"location":"examples/labels_to_data/#3-chip-the-data","title":"3. Chip the data\u00b6","text":"<p>Now that the tiled data has been downloaded, let's chip it accordingly.</p>"},{"location":"examples/labels_to_data/#31-example-1-use-the-queried-data-list-returned-from-download_data-to-find-the-files-to-chip","title":"3.1 Example 1: Use the queried data list returned from download_data to find the files to chip.\u00b6","text":""},{"location":"examples/labels_to_data/#check-the-results","title":"Check the results\u00b6","text":"<p>Use the <code>plot_chip_and_label_pairs</code> function to check the chip and label pairs look as expected.</p>"},{"location":"examples/labels_to_data/#4-store","title":"4. Store\u00b6","text":""},{"location":"examples/labels_to_data/#5-upload","title":"5. Upload\u00b6","text":""},{"location":"examples/labels_to_data/#6-additional-utility-functions","title":"6. Additional utility functions\u00b6","text":""},{"location":"examples/labels_to_data/#raster-labels-to-data","title":"Raster Labels to Data\u00b6","text":"<p>Let's take a quick look at how we can also use TerraKit to generating a ML-ready dataset from a collection of raster labels.</p> <p>The label used in this section are in raster format from two burn scar events included in the <code>ibm-nasa-geospatial/hls_burn_scars</code> hugging face dataset.</p>"},{"location":"examples/labels_to_data/#1-process-raster-labels","title":"1. Process raster labels\u00b6","text":"<p>As before, this initial steps takes a directory containing some label files. This time the labels are raster files (.tif). The date is assumed to be contained in the filename again. Supported date types are <code>YYYYDDD (7), YYYYMMDD (8), YYMMDD (6 -&gt; 20YYMMDD).</code></p>"},{"location":"examples/labels_to_data/#2-download-the-data","title":"2. Download the data\u00b6","text":""},{"location":"examples/labels_to_data/#3-chip-the-data","title":"3. Chip the data\u00b6","text":"<p>Now that the tiled data has been downloaded, let's chip it accordingly.</p>"},{"location":"examples/labels_to_data/#4-store","title":"4. Store\u00b6","text":""},{"location":"examples/terrakit_cli/","title":"TerraKit: CLI","text":"<p>We can also run TerraKit via a CLI. This notebook will demonstrate going from a set of vector labels to a taco dataset using a single data connector. To try out the TerraKit CLI, just type <code>terrakit</code>!</p> <p>All parameters specified in this notebook can be put in a yaml, and can be run using the commands below. In this example the configuration file can be found in ./config.yaml.</p> Install Terrakit: For instructions on how to install TerraKit, take a look at the Welcome page.  Example labels: To download a set of example labels, use the `rapid_mapping_geojson_downloader` function. This example expects a set of labels to be found in `\"./docs/examples/test_wildfire_vector\"`.  In\u00a0[\u00a0]: Copied! <pre># Let's take a look at the TerraKit CLI options\n!terrakit -h\n</pre> # Let's take a look at the TerraKit CLI options !terrakit -h <p>We can add some basic imports to get started, including a helper function to download some example labels if they don't already exist.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom glob import glob\nimport warnings\n\nfrom pathlib import Path\nfrom terrakit.general_utils.labels_downloader import (\n    rapid_mapping_geojson_downloader,\n    EXAMPLE_LABEL_FILES,\n)\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import os from glob import glob import warnings  from pathlib import Path from terrakit.general_utils.labels_downloader import (     rapid_mapping_geojson_downloader,     EXAMPLE_LABEL_FILES, )  warnings.filterwarnings(\"ignore\") <p>Download some labels if none already exist. Here we download the labels to <code>./test_wildfire_vector</code>.</p> In\u00a0[\u00a0]: Copied! <pre>LABELS_FOLDER = \"./test_wildfire_vector\"\nif (\n    Path(LABELS_FOLDER).is_dir() is False\n    or set(EXAMPLE_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER}/*.json\")) is False\n):\n    rapid_mapping_geojson_downloader(\n        event_id=\"748\",\n        aoi=\"01\",\n        monitoring_number=\"05\",\n        version=\"v1\",\n        dest=LABELS_FOLDER,\n    )\n    rapid_mapping_geojson_downloader(\n        event_id=\"801\",\n        aoi=\"01\",\n        monitoring_number=\"02\",\n        version=\"v1\",\n        dest=LABELS_FOLDER,\n    )\n</pre> LABELS_FOLDER = \"./test_wildfire_vector\" if (     Path(LABELS_FOLDER).is_dir() is False     or set(EXAMPLE_LABEL_FILES).issubset(glob(f\"{LABELS_FOLDER}/*.json\")) is False ):     rapid_mapping_geojson_downloader(         event_id=\"748\",         aoi=\"01\",         monitoring_number=\"05\",         version=\"v1\",         dest=LABELS_FOLDER,     )     rapid_mapping_geojson_downloader(         event_id=\"801\",         aoi=\"01\",         monitoring_number=\"02\",         version=\"v1\",         dest=LABELS_FOLDER,     ) <p>Assuming this notebook is run from <code>./docs/examples</code>, let's jump back to the project root so that the same <code>config.yaml</code> can be used if running the TerraKit CLI from this project's root directory.</p> In\u00a0[\u00a0]: Copied! <pre>root_dir = os.getcwd().split(\"/docs/examples\")[0]\nos.chdir(root_dir)\nprint(os.getcwd())\n</pre> root_dir = os.getcwd().split(\"/docs/examples\")[0] os.chdir(root_dir) print(os.getcwd()) <p>Use the <code>config.yaml</code> file to define a working directory that will be used for any outputs from TerraKit including downloading tiles, chipping the data and storing in a chosen format. By default, the working directory is set to <code>./tmp</code> and the dataset name is set to <code>terrakit_curated_dataset</code>. The working directory does not need to exist beforehand. It is best to start with an empty working directory as TerraKit will look up and also delete certain files from this directory.</p> In\u00a0[\u00a0]: Copied! <pre># config.yaml\n\"\"\"\ndataset_name: \"terrakit_curated_dataset\"\nworking_dir: \"./tmp/terrakit_curated_dataset\"\n\"\"\"\n</pre> # config.yaml \"\"\" dataset_name: \"terrakit_curated_dataset\" working_dir: \"./tmp/terrakit_curated_dataset\" \"\"\" In\u00a0[\u00a0]: Copied! <pre># config.yaml\n\"\"\"\nlabels:\n  labels_folder: \"./docs/examples/test_wildfire_vector\"\n  label_type: vector\n  datetime_info: filename\n  active: True\n\"\"\"\n</pre> # config.yaml \"\"\" labels:   labels_folder: \"./docs/examples/test_wildfire_vector\"   label_type: vector   datetime_info: filename   active: True \"\"\" <p>Run the <code>terrakit</code> CLI using the labels option to process the labels inside the <code>labels_folder</code>. This will output a pair of shapefiles in the working directory used by the download step coming up next.</p> In\u00a0[\u00a0]: Copied! <pre>!terrakit --config docs/examples/config.yaml labels\n</pre> !terrakit --config docs/examples/config.yaml labels In\u00a0[\u00a0]: Copied! <pre># config.yaml\n\"\"\"\ndownload:\n  active: True\n  max_cloud_cover: 80\n  keep_files: False\n  data_source:\n  - data_connector: \"sentinel_aws\"\n    collection_name: \"sentinel-2-l2a\"\n    bands: [\"blue\", \"green\", \"red\"]\n    save_file: \"sentinelaws_s2_l2a_cli_test\" # Default: ./{working_dir}/{collection_name}/{tile_id}.tif\n  date_allowance: \n    pre_days: 0\n    post_days: 21\n  transform:\n    scale_data_xarray: True\n    impute_nans: true\n    reproject: True\n\"\"\"\n!terrakit --config docs/examples/config.yaml download\n</pre> # config.yaml \"\"\" download:   active: True   max_cloud_cover: 80   keep_files: False   data_source:   - data_connector: \"sentinel_aws\"     collection_name: \"sentinel-2-l2a\"     bands: [\"blue\", \"green\", \"red\"]     save_file: \"sentinelaws_s2_l2a_cli_test\" # Default: ./{working_dir}/{collection_name}/{tile_id}.tif   date_allowance:      pre_days: 0     post_days: 21   transform:     scale_data_xarray: True     impute_nans: true     reproject: True \"\"\" !terrakit --config docs/examples/config.yaml download In\u00a0[\u00a0]: Copied! <pre># config.yaml\n\"\"\"\nchip:\n  sample_dim: 256\n\"\"\"\n!terrakit --config docs/examples/config.yaml chip\n</pre> # config.yaml \"\"\" chip:   sample_dim: 256 \"\"\" !terrakit --config docs/examples/config.yaml chip In\u00a0[\u00a0]: Copied! <pre># config.yaml\n\"\"\"\nstore:\n  active: True\n  tortilla_name: \"terrakit_curated_tortilla\"\n\"\"\"\n!terrakit --config docs/examples/config.yaml store\n</pre> # config.yaml \"\"\" store:   active: True   tortilla_name: \"terrakit_curated_tortilla\" \"\"\" !terrakit --config docs/examples/config.yaml store"},{"location":"examples/terrakit_cli/#terrakit-cli","title":"TerraKit: CLI\u00b6","text":""},{"location":"examples/terrakit_cli/#0-download-example-labels-define-a-working-directory-and-a-dataset-name","title":"0. Download example labels, define a working directory and a dataset name\u00b6","text":""},{"location":"examples/terrakit_cli/#1-process-labels-using-terrakit-cli","title":"1. Process labels using TerraKit CLI\u00b6","text":"<p>Specify a labels folder in the <code>config.yaml</code> file using the <code>labels_folder</code> parameter to get started processing labels. All of the other arguments are optional. Here we also set the <code>label_type</code> to <code>vector</code> and specify that the label datetime information can be found in the filename by setting <code>datetime_info</code> to <code>filename</code>.</p>"},{"location":"examples/terrakit_cli/#2-download-data-using-terrakit-cli","title":"2. Download data using terrakit CLI\u00b6","text":"<p>Here we specify the download parameters.</p> <p>To specify more than one data connector, simply extend the <code>data_source</code> mapping. For example:</p> <pre>data_source:\n  - data_connector: \"sentinel_aws\"\n    collection_name: \"sentinel-2-l2a\"\n    bands: [\"blue\", \"green\", \"red\"]\n  - data_connector: \"sentinelhub\"\n    collection_name: s2_l2a\n    bands: [\"B04\", \"B03\", \"B02\"]\n</pre> <p>To find out about the different data connectors available, take a look at the Data Connector section of the docs.</p>"},{"location":"examples/terrakit_cli/#3-chip-the-downloaded-data-using-terrakit-cli","title":"3 Chip the downloaded data using TerraKit CLI\u00b6","text":""},{"location":"examples/terrakit_cli/#4-store-the-downloaded-data-in-a-taco-using-terrakit-cli","title":"4 Store the downloaded data in a taco using TerraKit CLI\u00b6","text":""},{"location":"examples/terrakit_download/","title":"TerraKit: Easy geospatial data search and query","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport xarray as xr\nimport pandas as pd\n\nfrom pathlib import Path\n\nfrom terrakit import DataConnector\nfrom terrakit.download.transformations.impute_nans_xarray import impute_nans_xarray\nfrom terrakit.download.transformations.scale_data_xarray import scale_data_xarray\nfrom terrakit.download.geodata_utils import save_data_array_to_file\n\n# To be able to query using a data connector, we will import api credentials from .env\nimport dotenv\n\ndotenv.load_dotenv()\n</pre> import numpy as np import xarray as xr import pandas as pd  from pathlib import Path  from terrakit import DataConnector from terrakit.download.transformations.impute_nans_xarray import impute_nans_xarray from terrakit.download.transformations.scale_data_xarray import scale_data_xarray from terrakit.download.geodata_utils import save_data_array_to_file  # To be able to query using a data connector, we will import api credentials from .env import dotenv  dotenv.load_dotenv() <p>First, we create a DataConnector instance for the connector we wish to use.  You can list the available collections for that connector.</p> <p>Currently, you can select from:</p> <ul> <li><code>sentinelhub</code></li> <li><code>sentinel_aws</code></li> <li><code>nasa_earthdata</code></li> <li><code>IBMResearchSTAC</code></li> <li><code>TheWeatherCompany</code></li> </ul> In\u00a0[2]: Copied! <pre>data_connector = \"sentinel_aws\"\ndc = DataConnector(connector_type=data_connector)\ndc.connector.list_collections()\n</pre> data_connector = \"sentinel_aws\" dc = DataConnector(connector_type=data_connector) dc.connector.list_collections() <p>Now we can search for data.  To do so we select the collection and specify our bounding box and time range.</p> <p>The <code>find_data()</code> function will search for data and return both a list of unique dates where data is available, but also the raw results from the search.  In the case of a STAC catalogue, the results will be the STAC entries, for another connector, it will be the item details which are returned.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = [34.671440, -0.090887, 34.706448, -0.087678]\n</pre> bbox = [34.671440, -0.090887, 34.706448, -0.087678] In\u00a0[\u00a0]: Copied! <pre>collection_name = \"sentinel-2-l2a\"\n\nbands = [\"blue\", \"green\", \"red\"]\n\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bands=bands,\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"sentinel-2-l2a\"  bands = [\"blue\", \"green\", \"red\"]  unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bands=bands,     bbox=bbox, )  print(unique_dates) <p>Now to query the data, we specify the bands we want to return, plus an (optional) save filename.  The <code>get_data()</code> function will query the data from the data source and return an xarray object containing the data, plus if you provide <code>save_file=</code> as an argument, it will also save a geotiff file.</p> <p>If the bands are not found for the collection chosen, it will try to match the alt names stored in the internal collections catalogue, otherwise it will fail and tell you the available bands.</p> In\u00a0[\u00a0]: Copied! <pre>save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n    bands=bands,\n    save_file=f\"{save_filestem}.tif\",\n)\n\ndai = scale_data_xarray(da, list(np.ones(len(bands))))\ndai = impute_nans_xarray(dai)\nsave_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True)\n</pre> save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox,     bands=bands,     save_file=f\"{save_filestem}.tif\", )  dai = scale_data_xarray(da, list(np.ones(len(bands)))) dai = impute_nans_xarray(dai) save_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True) <p>Before connecting to the Sentinel hub data connector, ensure credentials have been added to your local environment. The easiest way to do this is to add them to your <code>.env</code>. Login to sentinel-hub.com to generate a Oauth client ID and client secret, then add them to the <code>.env</code> file:</p> <pre># .env\nSH_CLIENT_ID=&lt;your_token_here&gt;\nSH_CLIENT_SECRET=&lt;your_token_here&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>data_connector = \"sentinelhub\"\ndc = DataConnector(connector_type=data_connector)\ndc.connector.list_collections()\n</pre> data_connector = \"sentinelhub\" dc = DataConnector(connector_type=data_connector) dc.connector.list_collections() In\u00a0[\u00a0]: Copied! <pre>collection_name = \"s2_l2a\"\nbands = [\"B04\", \"B03\", \"B02\"]\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"s2_l2a\" bands = [\"B04\", \"B03\", \"B02\"] unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox, )  print(unique_dates) In\u00a0[\u00a0]: Copied! <pre>save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n    bands=bands,\n    save_file=f\"{save_filestem}.tif\",\n)\ndai = scale_data_xarray(da, list(np.ones(len(bands))))\ndai = impute_nans_xarray(dai)\nsave_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True)\n</pre> save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox,     bands=bands,     save_file=f\"{save_filestem}.tif\", ) dai = scale_data_xarray(da, list(np.ones(len(bands)))) dai = impute_nans_xarray(dai) save_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True) In\u00a0[\u00a0]: Copied! <pre>collection_name = \"s1_grd\"\n\nbands = [\"VV\", \"VH\"]\n\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"s1_grd\"  bands = [\"VV\", \"VH\"]  unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox, )  print(unique_dates) In\u00a0[\u00a0]: Copied! <pre>date = unique_dates[0]\nsave_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=date,\n    date_end=date,\n    bbox=bbox,\n    bands=bands,\n    save_file=f\"{save_filestem}.tif\",\n)\n\ndai = scale_data_xarray(da, list(np.ones(len(bands))))\ndai = impute_nans_xarray(dai)\nsave_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True)\n</pre> date = unique_dates[0] save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=date,     date_end=date,     bbox=bbox,     bands=bands,     save_file=f\"{save_filestem}.tif\", )  dai = scale_data_xarray(da, list(np.ones(len(bands)))) dai = impute_nans_xarray(dai) save_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True) <p>Before connecting to the NASA Earth data connector, ensure credentials have been added to your local environment. The easiest way to do this is to add them to your <code>.env</code>. Login to urs.earthdata.nasa.gov to generate a token, then add it to the <code>.env</code> file:</p> <pre># .env\nNASA_EARTH_BEARER_TOKEN=&lt;your_token_here&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>data_connector = \"nasa_earthdata\"\ndc = DataConnector(connector_type=data_connector)\ndc.connector.list_collections()\n</pre> data_connector = \"nasa_earthdata\" dc = DataConnector(connector_type=data_connector) dc.connector.list_collections() In\u00a0[\u00a0]: Copied! <pre>collection_name = \"HLSL30_2.0\"\n\nbands = [\"B04\", \"B03\", \"B02\"]\n\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"HLSL30_2.0\"  bands = [\"B04\", \"B03\", \"B02\"]  unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox, )  print(unique_dates) In\u00a0[\u00a0]: Copied! <pre>date = unique_dates[0]\nsave_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=\"2024-01-01\",\n    date_end=\"2024-01-31\",\n    bbox=bbox,\n    bands=bands,\n    save_file=f\"{save_filestem}.tif\",\n)\n\ndai = scale_data_xarray(da, list(np.ones(len(bands))))\ndai = impute_nans_xarray(dai)\nsave_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True)\n</pre> date = unique_dates[0] save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=\"2024-01-01\",     date_end=\"2024-01-31\",     bbox=bbox,     bands=bands,     save_file=f\"{save_filestem}.tif\", )  dai = scale_data_xarray(da, list(np.ones(len(bands)))) dai = impute_nans_xarray(dai) save_data_array_to_file(dai, save_file=f\"{save_filestem}.tif\", imputed=True) <p>Before connecting to the IBM Research STAC data connector, ensure credentials have been added to your local environment. The easiest way to do this is to add them to your <code>.env</code>.</p> <pre># .env\nAPPID_ISSUER=&lt;issuer&gt;\nAPPID_USERNAME=&lt;user-email&gt;\nAPPID_PASSWORD=&lt;user-password&gt;\nCLIENT_ID=&lt;client-id&gt;\nCLIENT_SECRET=&lt;client-secret&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>data_connector = \"IBMResearchSTAC\"\ndc = DataConnector(connector_type=data_connector)\ndc.connector.list_collections()\n</pre> data_connector = \"IBMResearchSTAC\" dc = DataConnector(connector_type=data_connector) dc.connector.list_collections() In\u00a0[\u00a0]: Copied! <pre>collection_name = \"sentinel-5p-l3grd-ch4-wfmd\"\ndate_start = \"2024-01-19\"\ndate_end = \"2024-01-21\"\nbands = [\"CH4_column_volume_mixing_ratio\"]\nbbox = [-102.3, 31.5, -101.7, 32.1]\n\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=date_start,\n    date_end=date_end,\n    bands=bands,\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"sentinel-5p-l3grd-ch4-wfmd\" date_start = \"2024-01-19\" date_end = \"2024-01-21\" bands = [\"CH4_column_volume_mixing_ratio\"] bbox = [-102.3, 31.5, -101.7, 32.1]  unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=date_start,     date_end=date_end,     bands=bands,     bbox=bbox, )  print(unique_dates) In\u00a0[\u00a0]: Copied! <pre>file_path = f\"./tmp_download/{data_connector}_{collection_name}.nc\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=date_start,\n    date_end=date_end,\n    bbox=bbox,\n    bands=bands,\n    save_file=file_path,\n)\nprint(file_path)\n</pre> file_path = f\"./tmp_download/{data_connector}_{collection_name}.nc\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=date_start,     date_end=date_end,     bbox=bbox,     bands=bands,     save_file=file_path, ) print(file_path) In\u00a0[\u00a0]: Copied! <pre>ds = xr.open_dataset(file_path)\nds\n</pre> ds = xr.open_dataset(file_path) ds In\u00a0[\u00a0]: Copied! <pre>variable = list(ds)[0]\nds[variable].isel(bands=0, time=2).plot()\n</pre> variable = list(ds)[0] ds[variable].isel(bands=0, time=2).plot() In\u00a0[\u00a0]: Copied! <pre>data_connector = \"TheWeatherCompany\"\ndc = DataConnector(connector_type=data_connector)\ndc.connector.list_collections()\n</pre> data_connector = \"TheWeatherCompany\" dc = DataConnector(connector_type=data_connector) dc.connector.list_collections() In\u00a0[\u00a0]: Copied! <pre>collection_name = \"weathercompany-daily-forecast\"\nstart_timestamp = pd.Timestamp.today().date()\ndate_start = start_timestamp.isoformat()\nend_timestamp = start_timestamp + pd.Timedelta(15, unit=\"D\")\ndate_end = end_timestamp.isoformat()\nbands = [\"temperatureMax\"]\nbbox = (-102.3, 31.5, -101.7, 32.1)\n\nunique_dates, results = dc.connector.find_data(\n    data_collection_name=collection_name,\n    date_start=date_start,\n    date_end=date_end,\n    bands=bands,\n    bbox=bbox,\n)\n\nprint(unique_dates)\n</pre> collection_name = \"weathercompany-daily-forecast\" start_timestamp = pd.Timestamp.today().date() date_start = start_timestamp.isoformat() end_timestamp = start_timestamp + pd.Timedelta(15, unit=\"D\") date_end = end_timestamp.isoformat() bands = [\"temperatureMax\"] bbox = (-102.3, 31.5, -101.7, 32.1)  unique_dates, results = dc.connector.find_data(     data_collection_name=collection_name,     date_start=date_start,     date_end=date_end,     bands=bands,     bbox=bbox, )  print(unique_dates) In\u00a0[\u00a0]: hide-output Copied! <pre>data_dir = Path(\".\") / \"tmp_download\"\nif not data_dir.exists():\n    data_dir.mkdir()\nsave_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"\n\nda = dc.connector.get_data(\n    data_collection_name=collection_name,\n    date_start=date_start,\n    date_end=date_end,\n    bbox=bbox,\n    bands=bands,\n    save_file=f\"{save_filestem}.nc\",\n)\n</pre> data_dir = Path(\".\") / \"tmp_download\" if not data_dir.exists():     data_dir.mkdir() save_filestem = f\"./tmp_download/{data_connector}_{collection_name}\"  da = dc.connector.get_data(     data_collection_name=collection_name,     date_start=date_start,     date_end=date_end,     bbox=bbox,     bands=bands,     save_file=f\"{save_filestem}.nc\", ) In\u00a0[\u00a0]: Copied! <pre>for file_path in data_dir.rglob(\"*.nc\"):\n    if file_path.is_file():\n        print(file_path)\n</pre> for file_path in data_dir.rglob(\"*.nc\"):     if file_path.is_file():         print(file_path) In\u00a0[\u00a0]: Copied! <pre>ds = xr.open_dataset(\"tmp_download/TheWeatherCompany_weathercompany-daily-forecast.nc\")\nds\n</pre> ds = xr.open_dataset(\"tmp_download/TheWeatherCompany_weathercompany-daily-forecast.nc\") ds In\u00a0[\u00a0]: Copied! <pre>ds[\"__xarray_dataarray_variable__\"].isel(bands=0, time=1).plot()\n</pre> ds[\"__xarray_dataarray_variable__\"].isel(bands=0, time=1).plot() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/terrakit_download/#terrakit-easy-geospatial-data-search-and-query","title":"TerraKit: Easy geospatial data search and query\u00b6","text":"<p>TerraKit Data Connectors can be used outside the TerraKit Pipeline. This notebook will guide you through using TerraKit Data Connectors to download data from different data collections.</p> Install TerraKit For instructions on how to install TerraKit, take a look at the Welcome page."},{"location":"examples/terrakit_download/#sentinel-2-l2a-from-sentinel-aws","title":"Sentinel-2-l2a from Sentinel AWS\u00b6","text":""},{"location":"examples/terrakit_download/#sentinel-2-l2a-from-sentinel-hub","title":"Sentinel-2-l2a from Sentinel Hub\u00b6","text":""},{"location":"examples/terrakit_download/#sentinel-1_grd-from-sentinel-hub","title":"Sentinel-1_grd from Sentinel Hub\u00b6","text":""},{"location":"examples/terrakit_download/#hls-l30-from-nasa-earthdata","title":"HLS-L30 from NASA Earthdata\u00b6","text":""},{"location":"examples/terrakit_download/#sentinel-5p-l3grd-ch4-wfmd-from-ibm-research-stac","title":"Sentinel-5p-l3grd-ch4-wfmd from IBM Research STAC\u00b6","text":""},{"location":"examples/terrakit_download/#example-for-forecast-temperature-from-the-weather-company","title":"Example for forecast temperature from The Weather Company\u00b6","text":"<p>Before connecting to the The Weather Company data connector, ensure credentials have been added to your local environment. The easiest way to do this is to add them to your .env.</p> <pre><code># .env\nTHE_WEATHER_COMPANY_API_KEY=&lt;your_api_key_here&gt;\n</code></pre>"}]}